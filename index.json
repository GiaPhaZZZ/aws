[{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.1-event1/","title":"Vietnam Cloud Day 2025","tags":[],"description":"","content":"Summary Report “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Purpose of the Event Raise awareness of the importance of Artificial Intelligence (AI). Highlight the applications of Generative AI (GenAI) across multiple domains of life. Introduce new services from Amazon Web Services (AWS). List of Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Key Highlights Vietnam adopting digital technology and cloud computing as the foundation for Industry 4.0 Expansion of cloud and digital infrastructure. Increased licensing opportunities for companies and foreign investors. Strong emphasis on security and information protection. U2U Network – Technology infrastructure integrated with Generative AI to simplify blockchain adoption By combining its existing technologies with Generative AI, U2U Network makes blockchain more approachable for new users and businesses. Some examples of AI’s role in shaping Vietnam’s technology landscape include:\n60% of Vietnamese students now use edtech applications. Over 765 AI startups exist in Vietnam, ranking second in Southeast Asia. Healthcare services have become more accessible and cost-efficient—e.g., at Hospital 10A, AI-assisted consultations reduced examination time to just 5 minutes per case. Generative AI helps users make better everyday decisions. AWS’s investment in education AWS has trained more than 100,000 builders in Vietnam. Cloud services have become more accessible locally. The AWS Cloud Journey program, lasting 6 months, ensures practical job opportunities. AWS culture is a key differentiator that drives innovation. The importance of AI technology transfer Today represents a unique moment in history where AI is transforming everything. Beyond technology, AI adoption involves people, skills, culture, and society—fostering a culture of continuous learning. Transition from “training mode” to “inference mode” with safe and responsible model usage. AWS enables access to cutting-edge models. For instance, Nearmap helps customers focus on decision-making, creativity, and strategy while delegating repetitive tasks to machines. AWS SageMaker SageMaker enables developers and data scientists to build, train, and deploy machine learning models at scale on AWS, without the burden of infrastructure management. It provides:\nData Preparation: Tools for cleaning, organizing, and transforming datasets. Model Building and Training: Pre-configured Jupyter notebooks with support for frameworks like TensorFlow and PyTorch. Deployment: Seamless cloud deployment with integration into other AWS services. AI-Driven Development Lifecycle (AI-DLC) Naïve AI-assisted coding—sometimes called “vibe coding”—lets AI perform all tasks. However, debugging becomes difficult, and models lack transparency. A more balanced approach, the AI-Driven Development Lifecycle (AI-DLC), delegates many tasks to AI while humans retain control of core decisions. The process involves three stages:\nInception: Brainstorming ideas, defining requirements, and task planning. Construction: Domain modeling, AI-assisted code generation, and testing. Operation: Production deployment, incident management, and long-term maintenance. Securing AI Applications AI models often exhibit vulnerabilities, including hallucination (false but confident outputs), overconfidence, and susceptibility to data manipulation. To mitigate these risks, stronger policies are needed to ensure reliability and trust:\nAI governance policies Root-cause analysis of hallucinations Data poisoning prevention Prompt security Access control and authorization Lessons Learned Work Mindset Problem Understanding: Identify the customer’s needs and problems first. Planning and Strategy: Develop a roadmap and select appropriate tools and services. Data Preparation: Dedicate time to cleaning and preparing datasets for consistency and quality, minimizing rework later. Applying New Techniques Amazon Web Services: Leverage cloud technologies to deliver higher-quality solutions. Emerging Technologies: Stay updated with the latest research and techniques to remain competitive. AI Strategies AI in Work: Strike a balance—AI should neither handle everything nor be sidelined. Combining human expertise with AI maximizes efficiency and outcomes. Increase Security: Enhance security, mitigate risks, and strengthen AI capabilities to prevent data leakage and hallucinations during training. Application to Work AWS SageMaker: To be applied for building machine learning models in upcoming projects, leveraging cloud integration for better results. Amazon Q: An AI-powered assistant for dashboards and conversational analytics, enabling more intuitive and user-friendly data exploration. AI-Driven Development Lifecycle: Applying the three stages (Inception – Construction – Operation) for structured, AI-assisted development. Generative AI (GenAI): Used in research and ideation, both before and during project execution. Experience at the Event Attending “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” was one of the most memorable experiences of my career. AWS organizes Cloud Day only once a year, and I was fortunate to participate. Beyond listening to speakers present new technologies, I also gained deeper awareness of the role of AI and cloud computing in building innovative, user-centric products.\nKnowledge from leading experts Speakers and representatives of major tech corporations emphasized the role of Generative AI in shaping the future of technology. Through practical examples, I better understood how to apply the AI-Driven Development Lifecycle (AI-DLC) and Securing Generative AI Applications with AWS in large-scale projects. Hands-on technical experience Sessions on U2U Network illustrated how Generative AI enhances blockchain accessibility and decision-making. Learned strategies to strengthen security and reliability in machine learning models, preventing leaks and hallucinations. Gained practical insights on maximizing AI’s potential in project development. Exploring modern tools Directly experimented with Amazon SageMaker, supporting end-to-end machine learning workflows. Learned to use Amazon Q for creating dashboards and visualizing data more intuitively. Networking and discussions Panel discussions brought together experts and CEOs from major corporations to address GenAI strategies and leadership approaches, providing a broad perspective on technology trends. I gained insight into how to effectively use AI—not by giving it full control, nor by restricting it to side tasks, but by maximizing its supportive role in work. Key Takeaways Applying AI-DLC significantly improves work efficiency, while saving time and costs. Using Generative AI (GenAI) in research and knowledge acquisition streamlines learning and decision-making. Tools such as Amazon Q and SageMaker can substantially boost productivity when integrated into projects. Event Photos Figure 1 Figure 2 Figure 3 Overall, the event not only provided technical knowledge but also transformed my mindset on application design, system modernization, and effective team collaboration.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with other FJC members. Learn about AWS and its available services. Open an AWS account and practice with basic features. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn in-depth about AWS and its services: + Create first AWS account + Set up MFA + Practice using AWS Management Console + Explore AWS support plans 08/09/2025 08/09/2025 https://000001.awsstudygroup.com/ 3 - Read and study FJC AWS rules and regulations. + Join the FJC Facebook group and review workshop materials + Attend first session at AWS office + Meet and form groups + First online meeting, networking, elect team leader 09/09/2025 09/09/2025 https://policies.fcjuni.com/ 4 - Learn about cost management and additional knowledge: + Understand and use Billing and Cost Management + Practice creating groups, users, policies, and roles with IAM 10/09/2025 10/09/2025 https://000007.awsstudygroup.com/ 5 - Explore Amazon Virtual Private Cloud (VPC) and conduct hands-on testing 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/ 6 - Translate log \u0026ldquo;Exploring Quantum Measurements, Observables and Operators: Practical insights with Amazon Braket\u0026rdquo; from English to Vietnamese 12/09/2025 12/09/2025 https://aws.amazon.com/blogs/quantum-computing/exploring-quantum-measurements-observables-and-operators-practical-insights-with-amazon-braket/ Week 1 Achievements: A - AWS Account Setup\nSuccessfully created first AWS account\nLinked email, verified account, added Visa as payment method Managed IAM login Tested creating, deleting, editing Account Alias Completed MFA setup:\nUsed local device password for MFA Enabled U2F security key Created Admin Group and Admin User with predefined policies\nOpened a test support case\nPracticed with AWS Management Console:\nSwitched to Singapore Region Searched for available AWS services (EC2, Cloud9, Security Lake, …) and related docs/blogs Added/removed services from favorites Customized console home with widgets B - Cost Management \u0026amp; Knowledge\nUsed Billing and Cost Management to create a Budget (monthly cost budget template)\nUnderstood types of costs (cost, usage, R, …)\nStudied AWS Support Plans:\nBasic Developer Business Enterprise IAM (Identity and Access Management)\nLearned IAM concepts and operations: Groups, Users, Policies, Roles Created Groups and Users Tested role switching for created users C - Amazon Virtual Private Cloud (VPC)\nCompleted study of VPC concepts:\nSubnets Route Tables Internet Gateway NAT Gateway Created a test VPC\nAdded subnets, gateways, route tables, and security groups to finalize the test setup\n"},{"uri":"https://giaphazzz.github.io/aws/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Exploring Quantum Measurements, Observables and Operators: Practical insights with Amazon Braket by Sudarsan Srinivasan and Charunethran Panchalam Govindarajan on 12 SEP 2025 in Amazon Braket, Quantum Technologies\nIntroduction In this post, we explore the mathematical foundations and practical implementations of quantum measurement techniques using Amazon Braket. We examine this subject through multiple perspectives: action of gates as basis transformation, projection as inner product, projector formalism, and observable formalism. A key insight that we’ll develop is understanding basis transformations as tools for converting phase information to amplitude information – a concept central to many quantum algorithms.\nThis post aims to provide both intuitive understanding and implementation ideas for those looking to deepen their understanding of quantum measurement techniques.\nSuperposition and Measurement in Arbitrary Basis A general one-qubit state can be written as a superposition in the computational basis:\n$$ |\\psi\\rangle = \\alpha_0|0\\rangle + \\alpha_1|1\\rangle $$\nwhere alpha_0 and alpha_1 are complex probability amplitudes satisfying |alpha_0|^2 + |alpha_1|^2 = 1. These amplitudes quantify the projection (or overlap) of the state onto the basis vectors.\nQuantum measurement determines the degree of projection of the state onto the measurement basis. Now consider measuring this state in a different orthonormal basis. To represent the state in this arbitrary basis as\n$$ |\\psi\\rangle = \\beta_0|u_0\\rangle + \\beta_1|u_1\\rangle $$\nwe need to find coefficients beta_0 and beta_1.\nConsider the unitary matrix U with |u_0\u0026gt; and |u_1\u0026gt; as columns:\n$$ U = [|u_0\\rangle \\ |u_1\\rangle] $$\nTo find beta_0 and beta_1, we express the new basis vectors in terms of the computational basis:\n$$ U|0\\rangle = |u_0\\rangle, \\quad U|1\\rangle = |u_1\\rangle $$\n$$ |\\psi\\rangle = \\beta_0 U|0\\rangle + \\beta_1 U|1\\rangle = U (\\beta_0|0\\rangle + \\beta_1|1\\rangle) $$\nSince U is unitary, applying U^\\dagger to both sides yields:\n$$ U^\\dagger |\\psi\\rangle = \\beta_0|0\\rangle + \\beta_1|1\\rangle $$\nThe coefficients beta_0 and beta_1 are the projections of U^\\dagger |\\psi\\rangle onto the computational basis states. Therefore, measuring U^\\dagger |\\psi\\rangle in the computational basis gives identical results to measuring the original state in the |u_0\u0026gt;, |u_1\u0026gt; basis.\nThis idea naturally generalizes to an n-qubit system.\nAn n-qubit system lives in N = 2^n dimensional Hilbert space (with 2^n basis vectors). In the computational basis, these 2^n basis are denoted by the binary strings of numerals from 0 to 2^n -1: |00…0\u0026gt;, |00…1\u0026gt; … |11…1\u0026gt;.\nA general state |\\psi\u0026gt; can be expressed in the computational basis as\n$$ |\\psi\\rangle = \\alpha_0 |00…0\\rangle + \\alpha_1 |00…1\\rangle + \\dots + \\alpha_{N-1} |11…1\\rangle = \\sum_k \\alpha_k |k\\rangle $$\nwhere 0 ≤ k ≤ 2^n -1 and the sum of |alpha_k|^2 equals 1 (normalization condition).\nSuppose we now want to measure this state in any other arbitrary orthonormal basis. When we measure |\\psi\u0026gt; in this basis, we are asking: “What is the probability of finding the system in state |u_k\u0026gt;?”\nThe state |\\psi\u0026gt; can then be expressed in this basis as\n$$ |\\psi\\rangle = \\beta_0 |u_0\\rangle + \\beta_1 |u_1\\rangle + \\dots + \\beta_{N-1} |u_{N-1}\\rangle $$\nwhere beta_k = \u0026lt;u_k|\\psi\u0026gt; is the amplitude of the state projected onto basis |u_k\u0026gt;.\nAs before, if U is a unitary matrix whose columns are |u_0\u0026gt;, |u_1\u0026gt;, … |u_{N-1}\u0026gt;,\n$$ U^\\dagger|\\psi\\rangle = \\sum_k \\beta_k |k\\rangle $$\nMeasuring |\\psi\u0026gt; in the basis |u_k\u0026gt; is equivalent to applying U^\\dagger to the state and measuring in the computational basis.\nConversely, let V be any unitary operator (quantum gate). Then applying gate V to state |\\psi\u0026gt; and measuring the result V|\\psi\u0026gt; in the computational basis is equivalent to measuring the original state |\\psi\u0026gt; in the basis formed by the columns of V^\\dagger.\nThis is an important principle of quantum measurement. Physical devices permit measurement in Z basis (the computational basis). Measurement in a different basis is then achieved by an equivalent unitary transformation.\nLet us now view this through an example. In the following diagram (figure 1), a single qubit quantum state |\\psi\u0026gt; is represented in Real 2D vector space (with probability amplitudes as real numbers)\nFigure 1: Representation of a state in different basis on real 2D plane. |0⟩ and |1⟩ represent the computational basis while |u₀⟩ and |u₁⟩ represent any arbitrary basis. |ψ⟩ represents a single qubit state. The projections of the state on each basis vectors are inner products of the state with the respective basis vectors.\nConsider a state vector |ψ⟩ oriented at π/6 (30°) from |0\u0026gt;.\nThen, the state can be represented in the computational basis as:\n$$ |\\psi\\rangle = \\cos(\\pi/6)|0\\rangle + \\sin(\\pi/6)|1\\rangle = \\frac{\\sqrt{3}}{2}|0\\rangle + \\frac{1}{2}|1\\rangle $$\nIn the computational basis:\nProbability of measuring |0\u0026gt;: |cos(π/6)|^2 = 3/4 = 75% Probability of measuring |1\u0026gt;: |sin(π/6)|^2 = 1/4 = 25% Now consider measuring this same state in an arbitrary orthonormal basis that is at an angle of π/8 (22.5°) from the computational basis:\n$$ |u_0\\rangle = \\cos(\\pi/8)|0\\rangle + \\sin(\\pi/8)|1\\rangle $$\n$$ |u_1\\rangle = -\\sin(\\pi/8)|0\\rangle + \\cos(\\pi/8)|1\\rangle $$\nThe measurement outcomes will depend on the projection of state |\\psi\u0026gt; onto these basis vectors.\nAnalytically, we can see that:\n$$ |\\psi\\rangle = \\cos(\\pi/24)|u_0\\rangle + \\sin(\\pi/24)|u_1\\rangle $$\nIn the arbitrary orthonormal basis:\nProbability of measuring |u_0\u0026gt;: |cos(π/24)|^2 ≈ 0.983 (≈ 98.3%)\nProbability of measuring |u_1\u0026gt;: |sin(π/24)|^2 ≈ 0.017 (≈ 1.7%)\nNote on Bloch Sphere Representation On the Bloch sphere, angular parameters are twice the physical angles in the state space. A rotation by an angle θ in state space corresponds to a rotation by 2θ on the Bloch sphere. This relates to the SU(2) group being a double cover of SO(3).\nLet us now see this using Amazon Braket.\nAs shown in the code below, first, we create an instance of LocalSimulator and a Circuit. Amazon Braket initializes qubits to |0⟩ (oriented along the Z axis of Bloch Sphere). We then rotate the state by an angle 30 degrees (π/6) along the Y axis. Note that a physical rotation of π/6 corresponds to a rotation of π/3 on Bloch Sphere. The qubit state stays in the XZ plane of Bloch Sphere. This corresponds to the representation in Figure 1.\nWe create two vectors u_0 and u_1 that denote the measurement basis of our interest. These are orthogonal. We then create a matrix with these two vectors as columns. For higher dimensions, the same pattern applies. We then take the adjoint of this matrix – since our scenario is with real values, transpose of this matrix is taken (same as adjoint). The resultant matrix is unitary. This unitary operator is applied to the state (Circuit object) using “Circuit.unitary” method. We then measure the state in the computational basis. Note that measurement in Amazon Braket is by default on the computational basis.\nCode: Measuring in an arbitrary basis import numpy as np from braket.circuits import Circuit from braket.devices import LocalSimulator device = LocalSimulator() circuit = Circuit() state_angle = np.pi / 3 circuit.ry(0, state_angle) u_0 = np.array([[np.cos(np.pi/8)], [np.sin(np.pi/8)]]) u_1 = np.array([[-np.sin(np.pi/8)], [np.cos(np.pi/8)]]) matrix_formed_from_basis_vectors = np.hstack((u_0, u_1)) basis_rotation_gate = matrix_formed_from_basis_vectors.T circuit.unitary(matrix=basis_rotation_gate, targets=[0]) circuit.probability(target=0) task = device.run(circuit, shots=1000) taskResult = task.result() measurement_probabilities = taskResult.values[0] counts = taskResult.measurement_counts print(\u0026#34;Probability : \u0026#34;, measurement_probabilities) print(\u0026#34;Counts : \u0026#34;, counts) Running this code will yield a result similar to this(note that you may get a different number, but it will be close to and tend towards this):\nProbability : [0.983 0.017] Counts : Counter({\u0026#39;0\u0026#39;: 983, \u0026#39;1\u0026#39;: 17}) The result matches the analytically derived values.\nWhy measure in a different basis? Different measurement bases reveal complementary aspects of quantum states. This complementarity arises from the probabilistic nature of quantum measurement: we observe outcomes with specific probabilities via the Born rule, not the underlying complex amplitudes directly.\nConsider a general state |ψ⟩ = α|0⟩ + β|1⟩ where α and β are complex numbers. Measurement in the computational basis reveals |α|² and |β|² (the probabilities of outcomes |0⟩ and |1⟩), but not the relative phase between α and β. This phase information remains encoded in the quantum state and is inaccessible through measurements in that particular basis. States differing only in relative phase yield identical measurement statistics in that basis, despite being distinct quantum states.\nExample: Accessing Hidden Phase Information\nConsider |+⟩ = (|0⟩ + |1⟩)/ √2 and |−⟩ = (|0⟩ − |1⟩)/ √2 . These states differ only in the relative phase between their |0⟩ and |1⟩ components. Notably, despite both being equal superpositions, |+⟩ and |−⟩ are orthogonal states — as maximally distinguishable from each other as |0⟩ and |1⟩ are.\nIn computational basis measurements, both states yield 50% |0⟩ and 50% |1⟩. The ± phase relationship is not observable. However, in the {|+⟩, |−⟩} basis, |+⟩ always yields |+⟩ and |−⟩ always yields |−⟩ (both with probability = 1). The phase information becomes directly observable.\nThis illustrates quantum measurement complementarity: complete characterization of a quantum state requires measurements in multiple incompatible bases—bases whose measurement operators do not commute, preventing simultaneous precise measurement. Each measurement basis accesses specific quantum information while rendering other aspects unobservable.\nHow does a change of basis uncover aspects of quantum information? Let us go back to the measurement postulate. The probabilities of outcome depend on the projection of the quantum state on the basis states (vectors or subspaces). This can be better visualized in 1 qubit case on a Bloch sphere. An arbitrary state is a point on the Bloch sphere and the projection of that state on the Z axis determines the probability of that state yielding |0⟩ or |1⟩ state upon measurement. Probabilities of states that all lie on the same latitude have the same value. Different points on the Bloch sphere at the same latitude differ from each other only in relative phase.\nWhen we measure the state from a different basis, we are enquiring about the projection of the same state vector on a vector or direction that is different from the computational basis. The states that differ only in phase with respect to the computational basis (and have the same projections on Z direction) cast different projections on measurement basis / axis that is different from the computational basis.\nAs an analogy, take two cities in a globe that are at the same latitude. They will have the same projection along the north-south axis, but if we reorient and hinge the globe on to equatorial axis, these same cities will cast different projections along this new axis.\nThe crucial insight is that what appears as “phase” in the computational basis, manifests itself as “amplitude” in a different basis. Since amplitudes determine the measurement outcomes, basis transformation can be viewed as the mechanism to enable this phase to amplitude change.\nThis is the reason Hadamard transform plays such a critical role in quantum algorithms. As we saw from the previous section, any Unitary transform (such as Hadamard) can be viewed and interpreted as a basis transformation (coordinate rotation). Hadamard transform changes the computational basis {|0⟩, |1⟩} perspective to |+⟩ and |-⟩ perspective. Information that is encoded in phase (in algorithms such as Simon’s, Shor’s and any phase kickback dependent procedure) use the basis transformation approach to uncover the pattern in the quantum information.\nWe can draw a couple of key points from this analysis:\n1.The choice of basis of measurement gives us unique perspectives that may not be available from another basis; and we cannot get all perspectives simultaneously. A greater certainty of measurement in one basis set may a greater uncertainty in the corresponding basis from another set.\n2.We can interpret the operation of a Unitary matrix on a state in two ways – either as Unitary matrix changing the state of the system, or alternatively, as basis transformation which enables measurement (or viewing angle) on the qubit from a different coordinate system\nProjector formulation for measuring a state in an arbitrary basis In quantum mechanics, physical measurements are described by Observables. This measurement is about determining how much of a quantum state “projects onto” each basis vector. This geometric intuition of projection is captured using the notion of projectors, which serve as the elementary building blocks for understanding measurements using Observables.\nWe saw that the probability that a state |ψ⟩, when measured in an arbitrary basis {|u0⟩, |u₁⟩…|uk⟩, …, |uN-1⟩}, is seen in state |uk⟩ is:\n$$ |\\langle u_k|\\psi\\rangle|^2 = \\langle u_k|\\psi\\rangle \\langle u_k|\\psi\\rangle^* = \\langle u_k|\\psi\\rangle \\langle \\psi|u_k\\rangle = \\langle \\psi|u_k\\rangle \\langle u_k|\\psi\\rangle $$\nThe outer product expression |uk⟩⟨uk| is called the projector for the basis state |uk⟩. Let us denote it by Pk. Therefore, the probability of obtaining an outcome state |uk⟩ when |ψ⟩ is measured in the basis {|u0⟩, |u₁⟩…|uk⟩, …, |uN-1⟩} is:\n$$ \\langle ψ|Pk|ψ \\rangle $$\nThe projector Pk can be understood as an operation that extracts the component of |ψ⟩ that lies along direction |uk⟩:\n$$ Pk|ψ\\rangle = |uk⟩\\langle uk|ψ\\rangle $$\n⟨uk|ψ⟩ is the complex amplitude quantifying the “overlap” between |ψ⟩ and |uk⟩, and Pk|ψ⟩ scales |uk⟩ by this amplitude, yielding the component of |ψ⟩ that lies along |uk⟩.\nProjectors can be viewed as the fundamental building blocks of quantum observables. Action of a projector represents the most basic type of measurement in quantum mechanics. A Projector P asks the elementary (“yes”/”no”) question: “Is the system in a state defined by the projector?” We get the answer “Yes” with a probability ⟨ψ|P|ψ⟩.\nProjectors are Hermitian operators with eigenvalues 0 and 1. They preserve components in their target subspace (eigenvalue 1) while removing orthogonal components (eigenvalue 0).\nFrom Projectors to Observables Projectors answer binary “yes/no” questions about quantum states within a specific direction or subspace. An *Observable extends this concept by assigning numerical values to different measurement outcomes.\nAn Observable is essentially a construct that measures a quantum state in the basis defined by its eigenvectors. Each observable implicitly defines a measurement basis – its eigenbasis – determining the “reference frame” in which the quantum state is observed.\nMathematically, an observable is a weighted sum of orthogonal projectors:\n$$ A = λ_0 P_0 + λ_1 P_1 + … + λ_{n-1} P_{n-1} \\quad \\text{(Spectral Decomposition of an observable)} $$\nwhere each λᵢ is a real eigenvalue and Pᵢ is the projector onto the corresponding eigenspace.\nWhen we measure an observable A in a state |ψ⟩, each measurement yields exactly one of the eigenvalues λᵢ. The probability of obtaining λᵢ is ⟨ψ|Pᵢ|ψ⟩. Upon measurement, the state “collapses” to a new state:\n$$ \\frac{P_i |\\psi\\rangle}{\\sqrt{\\langle \\psi|P_i|\\psi \\rangle}} $$\nThe denominator is the normalization factor to ensure the state remains unit-normalized.\nPut differently, an observable provides a measurement model that uses its eigenvectors as the measurement basis and produces eigenvalues as measurement outcomes. For example, measuring with **Pauli X observable is equivalent to measuring in the |+⟩ and |-⟩ basis. For example, measuring with Pauli X observable is equivalent to measuring in |+⟩ and |-⟩ basis and measuring with Pauli Z is nothing but measuring in |0⟩ and |1⟩ basis (|+⟩ and |-⟩ are the eigenvectors of X and |0⟩ and |1⟩ are the eigenvectors of Z). The measurement outcome follows a probability distribution determined by the quantum state’s projection onto each eigenspace.\nEach experimental “shot” on a device (simulator or real quantum hardware) represents a single measurement instance, causing the quantum state to collapse to one of the eigenspaces while yielding the corresponding eigenvalue as the measurement result. The expectation value of a measurement is then the weighted sum of all the measurement shots. It is important to note that each measurement shot always yields one of the eigenvalues; the expectation value is a statistical average computed from multiple measurement results.\nA projector operator may be viewed as the most elementary observable, having just one eigenvalue of 1. In other words, it is the Observable that has only one component in its spectral decomposition.\nIn general, an Observable gives the expectation value – the statistical average of the eigenvalues assigned to each measurement basis (the eigenstates of the observable). A measurement using an elementary projector (as Observable) gives the probability directly, since the eigenvalue of the projector is 1 and does not affect the statistics of measurement.\nObservable as a sum of Pauli Terms In principle, any Hermitian operator A with orthonormal eigenvectors can serve as a quantum observable, where measurements project onto these eigenstates with corresponding eigenvalues λᵢ. The observable\n$$ A = \\sum_i λ_i |u_i\\rangle \\langle u_i| $$\nrepresents the complete measurement in this custom basis. Most quantum hardware can only perform measurements in a single, fixed computational basis – the Z-basis (|0⟩, |1⟩).\nQuantum computing frameworks handle this through systematic decomposition:\nAny single-qubit Hermitian operator A can be uniquely decomposed as:\n$$ A = a_0 I + a_1 X + a_2 Y + a_3 Z $$\nHere, aᵢ are real coefficients and X, Y, Z are the three Pauli matrices, along with the identity matrix I.\nPauli measurements use Pauli matrices as observables. When a custom Hermitian is used as an Observable, Amazon Braket decomposes it into this linear combination of Pauli observables. Each Pauli operator can then be implemented in terms of Pauli-Z computational basis measurements through appropriate basis rotations, translating arbitrary measurements into physically implementable operations.\nPauli X measurement is implemented by applying Hadamard gate (H) before a Z basis measurement; Pauli Y measurement is implemented by first applying S† (phase gate) and then Hadamard gate followed by the Z basis measurement.\nMeasuring in an arbitrary basis using Observable in Amazon Braket The following code illustrates using a projector to measure the state of a system in the basis {|u0⟩, |u₁⟩) as in previous example.\nFirst, we define a method to create a projector, given a basis vector. A projector in its most basic form is an outer product of the basis vector with itself. This projector gives the probability that the state is seen in the basis defined by the vector used for creating it.\nWe define the two basis vectors as previously and build projector operators for them. As before, we initialize and rotate the state vector using RY gate. We then use the “expectation” method on Circuit to specify the observables of interest – in this case, the two projectors.\nA point to be noted is that in Braket, we can apply only one unique non-identity observable to each qubit (unless shots is set to 0). To illustrate the action of both projectors (each being an observable) corresponding to the two basis vectors, we set shots=0 here, though we can alternatively set each projector individually with non-zero shots to see those results.\nCode: Using a Projector as an Observable for measuring in an arbitrary basis import numpy as np from braket.circuits import Circuit from braket.devices import LocalSimulator from braket.circuits import observables def get_projector(basis_vector): projector = np.linalg.outer(basis_vector, basis_vector) return observables.Hermitian(projector) basis_u0 = np.array([np.cos(np.pi/8), np.sin(np.pi/8)]) basis_u1 = np.array([-np.sin(np.pi/8), np.cos(np.pi/8)]) projector_u0 = get_projector(basis_u0) projector_u1 = get_projector(basis_u1) circ = Circuit() device = LocalSimulator() state_angle = np.pi / 3 circ.ry(0, state_angle) circ.expectation(projector_u0) circ.expectation(projector_u1) task = device.run(circ, shots=0) taskResult = task.result() probability_u0 = taskResult.values[0] probability_u1 = taskResult.values[1] print(\u0026#34;Probability that state is in basis u_0 : \u0026#34;, np.round(probability_u0, 4)) print(\u0026#34;Probability that state is in basis u_1 : \u0026#34;, np.round(probability_u1, 4)) Output:\nProbability that state is in basis u_0 : [0.983] Probability that state is in basis u_1 : [0.017] Probabilities from projector formulation match the coordinate rotation method. They are mathematically equivalent.\nConclusion Quantum measurement involves projecting states onto different bases, yielding probabilistic outcomes. Measuring in an arbitrary basis is equivalent to applying a unitary rotation followed by a computational basis measurement. Basis transformation converts unobservable phase into measurable amplitude. Projectors serve as elementary observables, and general observables decompose into Pauli operators for practical measurement in Amazon Braket.\nReferences “Bloch Sphere.” Wikipedia, https://en.wikipedia.org/wiki/Bloch_sphere. Accessed 20 Aug 2025. “Pauli matrices.” Wikipedia, https://en.wikipedia.org/wiki/Pauli_matrices. Accessed 20 July 2025. Townsend, J. S. (2012). A Modern Approach to Quantum Mechanics (2nd ed.). University Science Books, Chapter 2: Rotation of Basis States and Matrix Mechanics TAGS: Amazon Braket, quantum algorithms, quantum computing, Quantum Technologies\nAuthor Sudarsan Srinivasan – Senior Solutions Architect at AWS, interested in Physics, Mathematics, and Philosophy.\nCharunethran Panchalam Govindarajan – Sr. Product Marketing Manager at AWS, focused on High-Performance Computing and Quantum Technologies.\n"},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY In this workshop, we will present how we created an online platform that allows internet users to freely check, track, and even predict the path of ongoing storms in the West Pacific region. This platform helps users better prepare for upcoming natural disasters and reduces the potential damage they may cause.\nThe platform provides two main functionalities:\nShowing Recent Storms – Allows users to view the path, intensity, wind speed, and other characteristics of recent storms in the West Pacific region. Predicting Hurricane Trajectories – Allows users to input past storm locations (latitude and longitude; at least 9 data points) to obtain predictions about the storm’s near-future movement, intensity changes, and potential path. Following the flow of this workshop, we will discuss the datasets, pre-processing steps, model-training pipeline, and the process of building the online platform using AWS services. We will also demonstrate our proposed augmentation techniques—Stepwise Temporal Fading Augmentation (STFA) and Plausible Geodesic Bearing Augmentation (PGBA)—along with the use of physics-informed machine learning. These approaches enhance the realism of the training data and significantly improve prediction accuracy for storm trajectories, lifetime estimates, and total travel distance.\nFigure 1 : Model pipeline Once the model-training process is completed, we move to building the online platform using a serverless architecture. This architecture is cost-efficient, scalable, and easy to maintain/deploy—making it an ideal choice for our project. Below are the main AWS services used:\nAWS Lambda – Executes the ML models and handles backend logic Amazon S3 – Stores static files, trained models, and storm data Amazon API Gateway – Routes user requests to the appropriate Lambda functions depending on whether they are viewing recent storms or running predictions Amazon CloudFront – Speeds up content delivery through edge locations AWS Secrets Manager – Stores API keys and other sensitive information … – Additional supporting services as needed Figure 2 : Platform Architecture "},{"uri":"https://giaphazzz.github.io/aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Worklog Internship at FJC – 12-Week Overview\nOn this page, I will introduce my worklog. I completed a 12-week internship at FJC, going through training and hands-on practice with AWS, from basic familiarity to deploying serverless and ML workflows on the cloud. Below is an overview of the weekly tasks:\nWeek 1: Getting familiar with AWS and basic AWS services\nInteracted and got to know other members participating in FJC. Learned an overview of AWS and the services it provides. Opened an AWS account and practiced basic features. Week 2: Learning basic knowledge of EC2, Cloud9, S3\nMastered how to use EC2, Cloud9, and S3 to support AWS projects. Continued learning basic operations to build workflows. Week 3: Exploring Amazon Lightsail and scaling EC2\nLearned the concept of Lightsail. Practiced scaling EC2 instances. Week 4: Grasping fundamental Cloud knowledge\nLearned basic cloud concepts. Explored AWS CloudWatch, CloudFront, CLI, and management tools. Week 5: Migrate to AWS \u0026amp; optimize Lambda\nUnderstood modules in “Migrate to AWS”. Learned how to optimize costs when using AWS Lambda. Week 6: AWS system management and lab practice\nLearned about Grafana tags and managing AWS System Manager. Practiced lab exercises provided by FJC. Week 7: AWS CloudFormation \u0026amp; CDK\nStudied AWS CloudFormation and CDK in detail. Learned to automate infrastructure and application deployment. Week 8: IaC, VPC Flow Logs, and Billing\nExplored Infrastructure as Code (IaC) and VPC flow logs. Completed lab exercises related to billing and cost management. Week 9: Security and AWS SSO\nStudied modules related to Security. Got familiar with AWS Single Sign-On and Security Hub. Week 10: Containerization \u0026amp; Deployment\nUnderstood the process of containerizing applications with Docker. Deployed on AWS Cloud and got familiar with ECS, CDK, and CodePipeline. Week 11: AWS Serverless\nGot familiar with AWS serverless services. Practiced with AWS SAM, Cognito, Lambda, and related services. Week 12: Data Lake \u0026amp; ML workflow\nUnderstood the concept of Data Lake and its core components: S3, Glue, Athena, QuickSight. Practiced ingesting, processing, querying, and visualizing data on AWS. Got familiar with serverless analytics and machine learning workflows using SageMaker. "},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.2-event2/","title":"Data Science on AWS","tags":[],"description":"","content":"Summary Report: “Data Science on AWS” Event Objectives To introduce AWS’s specialized services for the AI field. To explain key concepts in machine learning. To demonstrate the use of several AWS services. Speakers Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương – Cloud Development Engineer, AWS Community Builder Key Highlights Cloud Service Utilities for Data Science Provide APIs for models, reducing the need for heavy coding. Support efficient data cleaning and transformation. Enable large-scale and parallel processing. Offer multiple platforms for model training on AWS. Assist in model deployment, usage, and packaging. Fundamental Concepts in Data Science AI (Artificial Intelligence): Techniques that allow computers to mimic human intelligence using logic, if-then rules, and machine learning. Machine Learning: A subset of AI that uses algorithms to detect patterns in data and build logical models. Deep Learning: A subset of machine learning that uses multi-layer neural networks to perform complex tasks. Generative AI: A type of neural network trained on massive datasets from major tech companies, capable of creativity and generating new content. Amazon Comprehend AWS service for text classification and analysis. Supports personal information identification (PII). Understands multiple languages and detects sentiment. Extracts events and key phrases from text. Amazon Translate Specialized in translation and summarization; can generate data for chatbots. Highly accurate and easy to integrate into applications. A fully managed service, offering complete user control. Amazon Textract Extracts text and data from virtually any document. Enables text conversion and multilingual processing. Amazon Polly and Amazon Transcribe Polly: Text-to-speech service. Transcribe: Speech-to-text service. Both support multiple languages and conversion types. Amazon Lex Enables creation of conversational interfaces for any application. Provides fast, visually appealing, and time-saving interface design. Easy to use with built-in integrations and high-quality text output. Amazon Rekognition Allows users to analyze images and videos with high accuracy. Example use case: filtering or moderating sensitive content in media. Amazon Personalize Personalizes user experiences without requiring ML model integration. Utilizes user metadata, item metadata, and behavioral data for recommendations. Feature Engineering The process of making data understandable to machines. Focuses on encoding and transforming data for optimal machine performance. Example: fixing missing or corrupted data — data cleaning is part of feature engineering. Machine Learning Process Goes beyond just building models. Includes analyzing business problems and evaluating model performance. Also involves deploying models for end-users and ensuring long-term improvement. Key Takeaways Deeper Understanding of Data Science Learned technical definitions of core concepts: Artificial Intelligence, Machine Learning, Deep Learning, Generative AI. Understood the ML process: not only coding and model creation but also evaluation and deployment steps. Recognized the advantages of AWS services: save time, improve results, and optimize costs. Application to Work Apply Amazon Translate: Integrate into future projects for multilingual support. Use Amazon Textract: Automate text and information extraction to save time. Focus on Feature Engineering: As a foundational and crucial step, it directly affects all subsequent stages of data science work. Event Experience Attending the “Data Science on AWS” workshop was an inspiring journey into the world of cloud-powered data science — full of valuable knowledge and practical tools that will greatly support both work and study.\nLearning from Expert Speakers The two AWS speakers delivered insightful and engaging presentations, sparking my interest in cloud platforms and their powerful services. The audience raised many thoughtful questions, helping me deepen my understanding of data science and its modern applications. Hands-on Demo Experience Mr. Kha demonstrated how to use AWS services to build a complete machine learning model. The demo was clear, concise, and effectively showcased the convenience and flexibility of AWS tools. Lessons Learned Using AWS services for building and deploying ML models is essential — saving time while delivering high accuracy and efficiency. The machine learning process goes beyond modeling; it also involves solving business problems, deploying products, and planning for continuous improvement. Services like Amazon Rekognition offer convenient APIs that can easily be integrated into projects for practical use. Some event photos Figure 1 Figure 2 Figure 3 Figure 4 Overall, the event not only provided technical knowledge but also helped me reshape my thinking about machine learning process and cloud engineer.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Continue building knowledge for project model development. Gain solid understanding of EC2, Cloud9, S3, and related services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study algorithms needed for the project: + Spherical geometry + Linear algebra \u0026amp; probability/statistics + Machine learning algorithms + Predict new points from old points + direction + distance 15/09/2025 15/09/2025 3 - Learn about EC2: + Main functions of the service + Deployment practice + Cost of usage 16/09/2025 16/09/2025 https://000004.awsstudygroup.com/ 4 - Explore Cloud9 programming IDE: + Deploy and configure basic settings + Hands-on usage 17/09/2025 17/09/2025 https://000049.awsstudygroup.com/ 5 - Study Amazon S3: + Create S3 bucket and upload data + Try out main features 18/09/2025 18/09/2025 https://000057.awsstudygroup.com/ 6 - Translate log \u0026ldquo;Implement network connectivity patterns for Oracle Database@AWS\u0026rdquo; from English to Vietnamese 19/09/2025 19/09/2025 https://aws.amazon.com/blogs/database/implement-network-connectivity-patterns-for-oracle-databaseaws/ Week 2 Achievements: A – EC2\nLearned the basics of Amazon Elastic Compute Cloud (EC2).\nProvides compute power and ability to launch virtual servers without upfront hardware. AMI (Amazon Machine Image) used as templates for EC2 instances (OS, app server, block device mapping). Storage options: Amazon EBS volumes \u0026amp; Instance Store volumes. Explored key EC2 features:\nElastic Infrastructure Networking Capabilities Availability Zone selection Resource management Performance monitoring \u0026amp; notifications Deployment preparation:\nCreated VPC for Windows instance Created Security Group for Windows instance Successfully launched Microsoft Windows EC2 instance and connected from local computer.\nAdvanced EC2 practices:\nModified instance type Created \u0026amp; managed EBS Snapshots Created custom AMI Deployed instances from custom AMI Tested recovery access for Windows instances Deployed Node.js application on EC2 Windows Learned about EC2 cost management:\nRestricting service usage Limiting EC2 usage Managing EBS storage types Restricting resource deletion by IP and time window Cleaned up resources:\nDeleted EC2 instances, AMIs, Snapshots Deleted security groups, key pairs, VPC, etc. B – AWS Cloud9\nLearned the basics of AWS Cloud9:\nCloud9 is a browser-based IDE Provides build, run, edit, and cloud deployment support Customizable IDE (themes, keyboard shortcuts, etc.) Created Cloud9 environment:\nNamed and configured settings Completed setup and waited for environment readiness Tested basic features:\nUsed Cloud9 terminal Experimented with text files Tried out other available functions C – Amazon S3\nLearned the basics of Amazon Simple Storage Service (S3):\nProvides secure, scalable storage Designed to handle all data sizes \u0026amp; types Use cases: data warehouses, websites, mobile apps, etc. Created S3 bucket and uploaded data\nExpanded S3 usage:\nEnabled static website hosting Configured public access \u0026amp; object permissions Tested static website Accelerated website using CloudFront Tried additional features:\nBucket versioning Object movement Cross-region replication Cleaned up resources after experiments\n"},{"uri":"https://giaphazzz.github.io/aws/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Implement network connectivity patterns for Oracle Database@AWS by Sameer Malik, Anvesh Koganti, and Shubham Singh | 11 SEP 2025 | AWS Cloud WAN, AWS Transit Gateway, Expert (400), Networking \u0026amp; Content Delivery, Oracle Database@AWS, Technical How-to\nOracle Database@AWS (ODB@AWS) is an offering you can use to access Oracle Exadata infrastructure managed by Oracle Cloud Infrastructure (OCI) within Amazon Web Services (AWS) data centers. You can use ODB@AWS to migrate your Oracle Exadata workloads to AWS while maintaining the same performance and features as your on-premises Oracle Exadata deployments. You benefit from reduced application latency by establishing low-latency connectivity between Oracle Exadata and your applications running on AWS. Additionally, you can integrate Oracle Database@AWS with other AWS services to improve the availability and scalability of your Oracle database applications.\nIn this post, we show how to implement different IP routing-based network connectivity patterns for connectivity between your ODB network and other resources deployed in AWS and your on-premises network. The patterns work well when you have non-overlapping IP address spaces and need traditional Layer 3 connectivity.\nConnectivity patterns covered in this post include:\nOne-to-one connectivity between the application virtual private cloud (VPC) and ODB network\nScalable single-Region connectivity between multi-VPC or on-premises infrastructure and ODB network using AWS Transit Gateway\nScalable global connectivity between multi-VPC or on-premises infrastructure and ODB network using AWS Cloud WAN\nBefore proceeding further, we recommend that you be familiar with AWS services such as Amazon Virtual Private Cloud (Amazon VPC), AWS Direct Connect, Amazon Route 53 Resolver endpoints, Transit Gateway, and AWS Cloud WAN.\nUnderstanding the basics The following is a high-level overview of the core components in ODB@AWS essential for network connectivity. For more information, see the Oracle Database@AWS User Guide.\nODB network – An ODB network is a private, isolated network that hosts Oracle infrastructure within an AWS Availability Zone. Unlike a standard VPC, ODB network lacks internet connectivity and supports only ODB@AWS resources.\nODB peering – ODB peering establishes private network connectivity between your ODB network and an Amazon VPC, enabling applications to communicate with Oracle databases as if they were on the same network. ODB peering bridges AWS and Oracle environments and offers a support for routing capability that enables traffic using specific AWS networking services connected to your peered VPC to reach the ODB network. The following figure illustrates this peering relationship between a peered VPC and ODB network.\nAccess control with peered CIDRs – Peered CIDRs is a configuration that acts as a network-level access control list (ACL). You can specify and manage IP CIDR ranges that are allowed to reach the Oracle database resources in your ODB network. Setting up the peered CIDRs triggers automation on the Oracle OCI side to configure route rules and security rules linked to the Oracle Virtual Cloud Network (Amazon VPC equivalent) where the resources reside to allow traffic from/to specific CIDRs on specific protocols and ports. You can include specific subnet CIDRs instead of entire VPC ranges, providing granular security control. Refer to the considerations section later in this post for more details on peered CIDRs and OCI automation specifics.\nPeered VPC usage patterns – The peered VPC can serve multiple purposes. It can function as a direct application VPC, providing the simplest network path for dedicated Oracle workloads. Alternatively, it can act as a transit VPC that uses transitive routing to make the ODB network accessible from multiple VPCs and on-premises networks through Transit Gateway or AWS Cloud WAN based hub-and-spoke architectures. A peered VPC can also simultaneously fulfill both roles, serving as both an application VPC hosting workloads and a transit VPC connecting other networks to the ODB network.\nDNS configuration ODB@AWS automatically manages DNS within the ODB network, creating Fully Qualified Domain Names (FQDNs) for virtual machine (VM) clusters. By default, these use the *.oraclevcn.com domain pattern (where you can specify a prefix that gets appended to oraclevcn.com, such as myhost.oraclevcn.com). Alternatively, you can configure custom domain names using your own complete domain (for example, myhost.myodb.com). Regardless of the domain configuration chosen, DNS queries from your VPCs must be forwarded to the ODB network’s DNS infrastructure to resolve these domain names.\nThe DNS architecture consists of the following components:\nRoute 53 Resolver outbound endpoint – Forwards DNS queries from your VPC to the ODB network’s DNS infrastructure. After the ODB network is created, it provides a DNS listener IP address that the outbound endpoint must have network connectivity to.\nForwarding rules – Direct queries for Oracle domains (for example, client.xxxxx.oraclevcn.com) to the ODB network’s DNS listener IP using the outbound endpoint. The rule matches both the exact domain and its subdomains (for example, if the rule is for example.com, it matches both example.com and sub.example.com). You can share these rules across AWS accounts using AWS Resource Access Manager (AWS RAM) and associate them with VPCs that need to resolve Oracle domain names. For more details on domain matching, see How Resolver determines whether the domain name in a query matches any rules.\nIn the following figure, the outbound endpoint is deployed within the application VPC for simplified configuration and direct connectivity to the peered ODB network. Alternative deployment options and centralized DNS management strategies are discussed later in this post. For detailed DNS configuration steps, see Configuring DNS for Oracle Database@AWS.\nNetwork connectivity patterns In this section, we discuss three common network connectivity patterns for ODB@AWS, ranging from simple direct connectivity to scalable multi-Region architectures. Each pattern addresses different business needs and scaling requirements. For simplicity, the diagrams don’t show account-level resource placement. Account placement considerations for each pattern are detailed in the next section.\nConnectivity pattern 1: Direct peering between the application VPC and the ODB network This pattern uses the peered VPC as an application VPC, hosting database applications directly within the VPC peered to the ODB network. This approach provides the simplest network path between applications and databases.\nThis pattern works well when Oracle databases primarily serve applications within a single VPC. As shown in the following figure, the ODB network is bound to a single Availability Zone (defined when the ODB network is created), whereas the application VPC and its resources can span multiple Availability Zones. For multi-VPC or hybrid architectures, consider the ODB transit VPC patterns in subsequent sections.\nThe workflow consists of the following components:\nA. Amazon EC2 instances in the application VPC resolve the database hostname and initiate database connections using the IP. B. VPC route tables direct traffic toward the ODB network through the ODB peering connection. C. Because the VPC CIDR is included in the peered CIDRs list, configurations on the Oracle OCI side allow this traffic. Connectivity pattern 2: Single-Region scalable connectivity using Transit Gateway In this pattern, we explore network connectivity to the ODB network at scale in the single AWS Region. At the time of writing, ODB network supports direct peering with only one VPC; one-to-many peering connection between an ODB network and multiple VPCs is not supported. A transit gateway is a centralized networking component that serves as an interconnection point for multiple VPCs and on-premises networks. At the time of writing, a built-in transit gateway attachment for ODB network is not supported. However, you can peer your ODB network to a single transit VPC and then connect that transit VPC to the transit gateway, as shown in the following figure.\nIn this pattern, the peered VPC is acting only as the ODB transit VPC (as described earlier in this post). However, as highlighted in the peered VPC usage pattern, the peered VPC can simultaneously host workloads if desired.\nThe transit gateway attachment to the transit VPC must be created in a single Availability Zone only, specifically the same Availability Zone where the ODB network is created. To highlight this, in the preceding figure, we have shown the workloads spanning across multiple Availability Zones in the application VPC, but the transit VPC only has a transit gateway attachment in Availability Zone 1. For detailed configuration steps, see Configuring Amazon VPC Transit Gateways for Oracle Database@AWS.\nWorkflow components:\nA. EC2 instances in the application VPC resolve the database hostname to obtain the IP address and then initiate database connections using this IP. VPC subnet route table lookup happens, and the packet is forwarded to the transit gateway using the transit gateway attachment.\nB. The associated Transit Gateway route table has a static route for ODB network CIDR with the destination as the transit VPC attachment. Traffic from the transit VPC uses the ODB peering route in the VPC subnet route table and traffic is forwarded to the ODB network.\nC. Because the VPC CIDR is included in the peered CIDRs list, configurations on the Oracle OCI side are present to allow this traffic. Response traffic follows the reverse path back to the originating EC2 instances.\nConnectivity pattern 3: Multi-Region scalable connectivity using AWS Cloud WAN In this pattern, we explore network connectivity to ODB from multiple Regions and on-premises networks. AWS Cloud WAN simplifies multi-Region connectivity through dynamic route propagation. You can use this to connect your data centers, branch offices, and VPCs across multiple Regions into one unified network, all controlled through a single global network policy. AWS Cloud WAN supports built-in segmentation, which means you can easily manage network isolation across your Regions and on-premises locations. Using network segments, you can divide your global network into separate isolated networks. For more information, see the AWS Cloud WAN User Guide.\nAt the time of writing, a built-in attachment for ODB network is not supported with AWS Cloud WAN. By peering your ODB network to a transit VPC and then attaching that transit VPC to the AWS Cloud WAN core network, you can use AWS Cloud WAN to route network traffic between your ODB network and multiple other VPCs across Regions and on-premises locations. In this pattern, the peered VPC acts as the transit VPC based on the peered VPC usage pattern. You need to make sure the transit VPC attachment to the AWS Cloud WAN core network is created in one Availability Zone only—the same one as the ODB network.\nFor simplicity, we have shown all application VPCs that need access to the ODB network to be associated with the ODB application VPC segment while the Direct Connect gateway is associated with a hybrid segment and segment sharing is enabled between both segments, as shown in the following figure. For detailed configuration steps, see Configuring AWS Cloud WAN for Oracle Database@AWS.\nWorkflow components:\nA. EC2 instances in the application VPC resolve the database hostname to obtain the IP address and then initiate database connections using this IP. VPC subnet route table lookup happens, and the packet is forwarded to the AWS Cloud WAN core network using the AWS Cloud WAN VPC attachment.\nB. Application VPC 1 is associated with the ODB application VPC segment, which has the associated attachment CIDRs dynamically propagated. Because built-in attachment to ODB network is currently not supported on AWS Cloud WAN, we create a static route pointing to the transit VPC attachment.\nC. Traffic from the transit VPC uses the ODB peering route in the subnet route table and traffic is forwarded to the ODB network.\nD. As highlighted in the preceding section, necessary CIDRs for application VPCs and the on-premises network are added in the peered CIDR list on the ODB network. Response traffic follows the reverse path back to originating EC2 instances.\nConsiderations When implementing ODB@AWS network connectivity, keep in mind these considerations:\nWhen creating an ODB network, consider IP address space requirements, including non-overlapping CIDRs with connected networks and reserved IP ranges that can’t be used. These CIDR ranges can’t be modified after ODB network creation.\nSetting up ODB peering doesn’t automatically add routes for ODB network CIDRs (client CIDR or backup CIDR) to the peered VPC route tables. You must manually run the aws ec2 create-route command to add these routes to the VPC route tables as needed. See configuring VPC route tables for ODB peering for specific AWS Command Line Interface (AWS CLI) commands.\nWhen you set up ODB peering with a VPC, the primary VPC CIDR gets automatically added to the peered CIDR list. Secondary CIDRs of the peered VPC must be added manually. At the time of writing, you can’t edit the automatically added primary CIDR to limit it to specific subnet CIDRs of this VPC. Include all required source CIDR ranges that need to communicate with the databases in the peered CIDR list, which serves as an ACL.\nWhen an entry is added to the peered CIDR list, OCI side automation takes place to facilitate connectivity. A new static route is added in the route rules (VPC route table equivalent) associated to the Virtual Cloud Network (Amazon VPC equivalent) to facilitate traffic to the CIDR range added as an entry. Additionally, entries are added in the security rules (AWS security groups and network ACL equivalent) associated to the Virtual Cloud Network to allow ICMP traffic (for MTU path discovery and ping), TCP traffic on port 22 (for SSH), and other TCP ports needed for database traffic.\nRoute 53 Resolver outbound endpoints can be deployed either in the peered VPC or in a separate VPC your networking team manages. If you already have outbound endpoints set up to facilitate hybrid DNS resolution, you can use them as well. In either case, make sure the VPC CIDR containing the endpoint is added to ODB’s peered CIDRs list and network connectivity exists between the endpoint VPC and the peered VPC (through Transit Gateway or AWS Cloud WAN).\nRoute 53 profiles can be used to share resolver rules across AWS accounts instead of directly sharing individual rules using AWS RAM. As of this writing, the Transit Gateway attachment and AWS Cloud WAN attachment for transit VPC need to be created in one Availability Zone only—the same one as the ODB network for connectivity.\nMulti-Region connectivity can also be achieved using Transit Gateway inter-Region peering instead of Cloud WAN. This approach requires static routing configuration between peered transit gateways. A transit VPC can’t be simultaneously attached to both Transit Gateway and AWS Cloud WAN, or even multiple transit gateways or Cloud WAN core networks. You must choose a single transit service for connectivity.\nFor a pattern 1 account placement, the ODB network is provisioned in the buyer account and can be shared using AWS RAM with multiple accounts. However, as of this writing, only one peering connection can be established to the ODB network. This means you can share the ODB network using AWS RAM to another account and create the ODB peering connection with the application VPC in that shared account, providing flexibility for cross-account application deployments. See Resource sharing in Oracle Database@AWS for more details.\nFor patterns 2 and 3 account placements, when implementing Transit Gateway and AWS Cloud WAN connectivity, both the transit VPC and the ODB network must reside in the same AWS account—specifically the buyer account where the ODB network is provisioned. However, Transit Gateway and AWS Cloud WAN can reside in a different account (such as a central networking account) and be shared with the buyer account using AWS RAM to establish the necessary connectivity.\nConclusion In this post, we demonstrated the network integration capabilities of Oracle Database@AWS, which offers a robust and versatile solution for enterprises to seamlessly extend the power of Oracle Exadata databases in the AWS Cloud.\nOracle Database@AWS provides organizations with flexible options to integrate their Oracle database deployments with their broader AWS infrastructure and hybrid environments. By having the choice to peer an ODB network directly with a single VPC, use Transit Gateway to connect multiple VPCs, or use the centralized connectivity of AWS Cloud WAN, you can choose the network configuration that best suits your specific requirements.\nFor organizations needing IP-agnostic connectivity, overlapping IP address space support, or specialized integrations like zero-ETL and Amazon Simple Storage Service (Amazon S3) access, refer to Oracle Database@AWS network connectivity Using Amazon VPC Lattice.\nAbout the authors Sameer Malik\nSameer has been working on relational Databases for over 23 years now and is currently working as a Principal Database Solution Architect at AWS focused on RDS and Aurora. He has helped several customers with the migration and modernization of their Database workloads to AWS.\nAnvesh Koganti\nAnvesh is a Solutions Architect at AWS specializing in Networking. He focuses on helping customers build networking architectures for highly scalable and resilient AWS environments. Outside of work, Anvesh is passionate about consumer technology and enjoys listening to podcasts on tech and business. When disconnecting from the digital world, Anvesh spends time outdoors hiking and biking.\nShubham Singh\nShubham is a Senior Network Specialist Solutions Architect at AWS. He helps customers design innovative, resilient, and cost-effective solutions using AWS services. He is passionate about technology and enjoys building solutions in the Networking and Security. He holds a MS in Telecommunication Systems Management from Northeastern University, specializing in Computer Networking.\n"},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.2-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Collection and Preparation Data is a critical component of our project. It not only powers the machine learning model but is also directly displayed to end users so they can monitor the most recent storms in the West Pacific region. Because of this dual purpose—model training and real-time visualization—we carefully investigated multiple reliable and authoritative sources before selecting a single dataset that met all of our requirements: Hurricane Data from NOAA.\nThe National Oceanic and Atmospheric Administration (NOAA) is a scientific agency under the U.S. Department of Commerce. NOAA provides highly accurate, research-grade environmental data, including global weather observations, satellite imagery, and tropical cyclone records. With decades of investment in advanced technologies such as geostationary satellites, ocean buoys, radar systems, and climate monitoring networks, NOAA is widely regarded as one of the most trustworthy providers of hurricane information in the world.\nFor this project, we specifically use data from the International Best Track Archive for Climate Stewardship (IBTrACS)—a NOAA-led project and the world’s most comprehensive tropical cyclone dataset. IBTrACS consolidates and unifies historical and modern storm-track data from multiple meteorological agencies (e.g., JTWC, JMA, CMA, NHC). By merging these sources into a single consistent format, it improves inter-agency comparability and ensures researchers worldwide have access to the best available storm-track information.\nThe version of the dataset we use contains 226,153 rows of hurricane observations. Each row includes a variety of valuable fields such as:\nsid – storm ID number – storm number basin / subbasin – regional classification nature – storm type (e.g., tropical storm, typhoon) iso_time – timestamp lat / lon – storm center coordinates … and many additional meteorological properties However, for our machine learning model, we focus only on four key columns: sid, iso_time, lat, and lon. These form the essential time-series trajectory used to predict storm movement.\nThe dataset spans storms recorded from 1870 up to 2025, filtered to include only those within the West Pacific region—our geographical focus. The raw dataset can be accessed publicly here: https://data.humdata.org/dataset/vnm-ibtracs-tropical-storm-tracks#\nCleaning and Physics-Informed Feature Engineering One advantage of IBTrACS is that it is already well-maintained and consistent. Only minimal preprocessing is needed, primarily the removal of missing values.\nAfter cleaning, we apply our first step of physics-informed machine learning—a technique that injects physical knowledge directly into the data pipeline. From the latitude–longitude coordinates, we compute two additional features using the Haversine formula:\nDistance between consecutive storm points Bearing (direction of movement) These features are physically meaningful: they represent real-world movement patterns instead of arbitrary transformations. They enrich the dataset by giving the model more context about the storm’s momentum and direction, thereby improving learning efficiency and prediction accuracy.\nFigure 1 : Dataset Description Data for Display The data used for display on the platform is different from the data used for training, even though both originate from NOAA. The training dataset is static and historical, but the display dataset must always reflect the current storm conditions.\nTo achieve this, we implement a scheduled AWS Lambda function that automatically retrieves the latest storm-track updates at the end of each day. This ensures that the platform always presents the most recent and accurate information to users.\nThe processed display data is stored as a JSON file in an Amazon S3 bucket. When a user accesses the website:\nThe frontend sends a request to API Gateway API Gateway triggers the appropriate Lambda function The Lambda function fetches the JSON from S3 The resulting data is returned to the user for visualization This pipeline guarantees real-time, serverless, cost-effective data delivery.\nThe live dataset used for display can be accessed here: https://ncics.org/ibtracs/\nFigure 2 : Web to crawl data "},{"uri":"https://giaphazzz.github.io/aws/2-proposal/","title":"Proposal","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Geodesic-Aware Deep Learning for Hurricane Trajectory Prediction: A Physics-Informed and Augmentation-Driven Approach Proposal Document View Proposal Document\n1. Executive Summary Time-series data serves as one of the most fundamental representations of information in modern scientific and industrial applications. It is essential for understanding dynamic processes such as economic trends, energy consumption patterns, and meteorological changes over time. In particular, weather forecasting heavily relies on time-series data to predict future atmospheric conditions, hurricane trajectories, and seasonal anomalies based on historical records.\nWith the rapid progress in deep learning and neural network research, our project aims to develop an advanced forecasting model capable of accurately predicting the future path, intensity, and total travel distance of moving storms within the next few days. The model’s predictions can support early warning systems, allowing authorities and residents in affected regions to take precautionary measures well before a hurricane reaches their area.\nTo move beyond the limitations of current technologies and existing research, this study introduces several novel techniques and algorithms, including two new augmentation methods for geodesic time-series data and a spatial encoding mechanism designed to enhance the predictive performance of convolutional computing. The final system will be integrated into a cloud-based, serverless architecture on AWS, ensuring scalability, high availability, and cost efficiency for real-time storm tracking and analysis.\nThe system architecture leverages several AWS services to form a fully managed data processing and deployment pipeline. AWS Lambda functions serve as the backbone for serverless computation, automatically triggered by Amazon EventBridge to crawl and process new storm data from open meteorological sources on a scheduled basis. Processed data is stored securely in Amazon S3, while AWS CodePipeline and CodeBuild automate the continuous integration and deployment of new model versions. The trained model is hosted and exposed via Amazon API Gateway, enabling lightweight, real-time inference requests from the online forecasting platform. All system activities are monitored through Amazon CloudWatch, providing operational visibility, fault detection, and performance metrics.\nThe first proposed method, Stepwise Temporal Fading Augmentation (STFA), is a new time-series augmentation framework that models the natural decline in the influence of past observations. Unlike traditional approaches based on random perturbation or noise injection, STFA applies fading weights to earlier time steps while preserving recent information. This process generates realistic and diverse synthetic sequences, improving model robustness and generalization. The technique will be evaluated on hurricane trajectory prediction tasks that rely on sequential latitude–longitude data.\nThe second proposed technique, Plausible Geodesic Bearing Augmentation (PGBA), introduces an augmentation strategy based on the feasible range of storm bearings and distances. By analyzing the geodesic bearing and distance between consecutive storm locations, PGBA defines a realistic motion boundary within which new synthetic trajectories are generated. This approach enhances the model’s capacity to capture natural spatial variability and directional uncertainty in storm movements.\nAdditionally, this study explores a spatial–temporal representation of time-series data, enabling the application of convolutional neural networks (CNNs) to capture both spatial and temporal dependencies. This representation leverages the strengths of convolutional computing to model local interactions across space and time. It will serve as a baseline for comparison with Temporal Convolutional Network (TCN) models trained using the proposed augmentation methods.\nTraditional neural networks, whether used for sequential or image-based modeling, primarily learn statistical patterns from data. However, in many real-world physical systems, such purely data-driven models may fail to adhere to natural constraints, such as gravitational or geodesic relationships. To address this limitation, we incorporate the principles of Physics-Informed Machine Learning (PIML) into our approach. Specifically, geodesic distance and bearing are derived from the latitude–longitude data and integrated into the model’s training process as physically meaningful features. Furthermore, we employ the Haversine formula—which computes the spherical distance between two points—as an auxiliary loss term, complementing standard error metrics such as MSE, RMSE, MAE, and MAPE.\nBy combining the proposed augmentation methods, physics-informed learning principles, and a serverless deep learning infrastructure powered by AWS services, this research aims to develop a scalable, robust, and accurate framework for hurricane trajectory forecasting. The resulting system not only advances the state of geodesic time-series modeling but also demonstrates the practicality of deploying AI-driven environmental prediction systems as resilient, cloud-native applications that enhance preparedness and safety in hurricane-prone regions.\n2. Problem Statement What’s the Problem? To develop a reliable platform for tracking and issuing alerts on future hurricane movements, it is essential to construct a machine learning model that is both accurate and capable of producing dependable predictions. The tracking component can be addressed by continuously collecting and updating data from publicly available meteorological sources. However, these datasets are often geographically constrained and contain redundant or incomplete information.\nIn contrast, the predictive component presents greater complexity. Achieving accurate time-series forecasting in this context typically faces two primary challenges: (1) the limited diversity and coverage of available data, and (2) the absence of physical grounding, which restricts the model’s ability to reflect the underlying geophysical dynamics of hurricane behavior.\nData scarcity: Many time-series forecasting tasks suffer from limited training data. While there are various augmentation methods, few approaches directly focus on the declining importance of past values over time.\nPhysics ignorance: Most neural networks only learn from raw data, without considering real-world physical constraints. In trajectory prediction tasks (e.g., hurricanes), this often leads to unrealistic predictions.\nWe aim to:\nDevelop a new time-series augmentation method (STFA and PGBA) to improve robustness and generalization. Incorporate physics-based constraints into model training, bridging the gap between data-driven learning and real-world dynamics. Testing the strength of convolution network (convol2D) in term of forecasting trajectory Creating an online platform that provide latest information about currents storm and precisely predictions on their trajectory The Solution A - Stepwise Temporal Fading Augumentation STFA generates synthetic time-series sequences by gradually reducing the influence of earlier values. Unlike random noise injection, it systematically applies stepwise fading multipliers across bands of older data.\nLet a univariate sequence be:\n$$ X = [x_0, x_1, \\ldots, x_{T-1}] $$\nwhere $T$ is the sequence length of $X$.\nParameters:\n$n$: number of most steps to remain unchanged. $S$: number of step-bands to apply fading, each band is assigned a constant multiplier. $L = T - n$: length of the fading region. $k = \\frac{L}{S}$: values per band. $I_b$: index set of the $b$-th band. \\[ I_b = {\\ {i \\mid L - b \\cdot k ;\\leq; i ;\\leq; L - (b-1)\\cdot k - 1} ,} \\]\nTransformation:\nWe denote the augmented series as:\n$$ X = [x_0, \\ldots, x_{T-1}] $$\nwith the transformation rules:\n$$ x_t = \\begin{cases} x_t, \u0026amp; t \\in {T-n, \\ldots, T-1}, \\\\ m_b , x_t, \u0026amp; t \\in I_b, \\\\ m_{S+1} , x_t, \u0026amp; t \u0026lt; \\min(I_S), \\end{cases} $$\nwhere multipliers $m_b \\in (0,1)$ decrease monotonically from recent to older bands.\nThis formulation preserves the fidelity of recent history while exerting stronger control on the long-range influence of the sequence. The augmentation forces the model to focus on robust patterns beyond the raw data, while increasing diversity according to the chosen parameters.\nB - Plausible Geodesic Bearing Augmentation The Plausible Geodesic Bearing Augmentation (PGBA) technique enhances the realism and control of synthetic trajectory generation in geospatial time-series forecasting tasks. Unlike conventional random perturbation methods, PGBA introduces stochasticity that remains plausible within the physical constraints of the underlying motion. The generated trajectories are derived from the geometric relationships of past observations rather than from purely random steps, resulting in smoother paths and meaningful variability in the training dataset. This technique apply on every four locations in a data sequence.\nPGBA serves as a complementary augmentation to STFA, enriching the diversity of training samples while preserving the original dynamical structure. Its objective is to create redundant yet physically consistent trajectories that capture potential variations in storm movements or similar geospatial phenomena.\nCore Mechanism\nConsider a storm trajectory represented by a sequence of $n$ ordered geographical points:\n$$ P = [P_1, P_2, \\ldots, P_n] $$\nWe split this sequence into small blocks of 4 points, with $P_i$ as the starting point of each block, defined by its latitude and longitude:\n$$ P_i = (\\phi_i, \\lambda_i) $$\nHere, $\\phi_i$ and $\\lambda_i$ denote latitude and longitude in radians, respectively.\nThe geodesic distance $d_i$ and bearing $\\theta_i$ between consecutive points $P_i$ and $P_{i+1}$ are defined as:\n$$ d_i = \\text{Distance}(P_i, P_{i+1}), \\qquad \\theta_i = \\text{Bearing}(P_i, P_{i+1}) $$\nTo introduce plausible variability, PGBA perturbs the bearing by adding a small, uniformly distributed random noise $\\epsilon_i$:\n$$ \\theta_i^{\\text{aug}} = \\theta_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\text{Uniform}(-\\delta, \\delta) $$\nwhere $\\delta$ is a tunable angular bound controlling the range of deviation.\nThe first two points always remain unchanged and are used to compute distance and bearing. The subsequent augmented point is then computed using the geodesic destination formula, keeping the distance constant while allowing the bearing to vary within the range of the random noise:\n$$ P_{i+2}^{\\text{aug}} = \\text{Destination}(P_i, d_i, \\theta_i^{\\text{aug}}) $$\nThis process preserves the inter-point distance $d_i$ while slightly perturbing the direction to produce physically plausible deviations.\nMulti-Step Smoothing and Correction\nTo enhance spatial smoothness and generate curvilinear trajectories, PGBA applies a secondary correction at every fourth point. Let $P_{i+3}^{\\text{aug}}$ denote the fourth point. It is recomputed such that its bearing $\\theta_{i+3}^{\\text{corr}}$ minimizes the deviation from the original point $P_{i+3}$:\n$$ \\theta_{i+3}^{\\text{corr}} = \\arg\\min_{\\theta}, \\text{Distance}\\Big( \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta),; P_{i+3} \\Big) $$\nThen, the corrected augmented point is obtained as:\n$$ P_{i+3}^{\\text{aug}} = \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta_{i+3}^{\\text{corr}}) $$\nThis step ensures smooth transitions across multiple points while maintaining physical plausibility.\nNote that the first two and last locations in every geodesic time-series sequence always remain unchanged.\nC - Physics-Informed Machine Learning Neural network models such as RNNs, CNNs, and Transformers do not require explicit formulas or task-specific rules to perform well, provided that they are trained with sufficient data. For example, in machine translation tasks such as German-to-English translation using an RNN, no explicit grammar rules are provided during training. Nevertheless, the model is capable of producing coherent translations, which demonstrates one of the major strengths of deep learning: the ability to learn complex patterns directly from data. In contrast, traditional approaches—such as early versions of rule-based translation systems (e.g., Google Translate prior to the 2000s)—relied heavily on grammar rules and dictionaries. While precise, such systems often lacked flexibility and failed when encountering words with multiple meanings or when handling context-dependent structures.\nInspired by this, our goal is to combine the strengths of deep learning with human-defined formulas in order to achieve better performance. Specificly in this geography field, we will try to add the benefit from Haversine formula into training for distance and beering calculation between two location on a sphere. These provide the model with additional structure and inductive bias, guiding learning beyond purely statistical correlations.\nHaversine Formula\nFor distance calculation\nThe Haversine formula is used to calculate the great-circle distance between two points on the surface of a sphere — that is, the shortest path over the Earth’s surface.\n$$ d = 2r , \\arcsin!\\left( \\sqrt{ \\sin^2!\\left(\\frac{\\Delta \\varphi}{2}\\right) + \\cos(\\varphi_1)\\cos(\\varphi_2) \\sin^2!\\left(\\frac{\\Delta \\lambda}{2}\\right) } \\right) $$\nWhere:\n$\\varphi_1, \\lambda_1$ and $\\varphi_2, \\lambda_2$ are the latitudes and longitudes of the two points (in radians). $\\Delta \\varphi = \\varphi_2 - \\varphi_1$ $\\Delta \\lambda = \\lambda_2 - \\lambda_1$ $r$ is the Earth’s radius (≈ 6,371 km). In our framework, instead of relying solely on standard loss functions such as MSE, RMSE, or MAPE, we propose using the Haversine formula for distance calculation as the primary loss function. As the model outputs latitude and longitude coordinates for the next hurricane location, the Haversine formula directly measures the distance between predicted and ground-truth points. A distance close to 0 indicates a highly accurate prediction, while a large distance signals a significant error.\nFor beering calculation\nThe Bearing calculation is dereived from Haversine Formula gives the direction from one geographic point to another along the great-circle path:\n$$\\theta = \\text{atan2}!\\left(\\sin(\\Delta \\lambda)\\cos(\\varphi_2),, \\cos(\\varphi_1)\\sin(\\varphi_2) - \\sin(\\varphi_1)\\cos(\\varphi_2)\\cos(\\Delta \\lambda)\\right)$$\nWhere:\n$(\\varphi_1, \\lambda_1)$ is the start point. $(\\varphi_2, \\lambda_2)$ is the end point. $\\Delta \\lambda$ is the difference in longitude. The result $\\theta$ represents the initial bearing (azimuth) measured clockwise from true north.\nIn our implementation, we fully exploit the use of Haversine Formula to compute two additional features — “distance” and “bearing” — which are appended to the dataset.\nThese features provide the model with richer information about hurricane trajectories while maintaining the core objective of predicting the next geographic location.\nD - Short overview of misconceptions in Common Approaches to Sequence Modeling In the field of sequence modeling within deep learning, recurrent architectures such as RNNs, LSTMs, and GRUs are often considered the default solutions. This perception has led many practitioners and researchers to overlook alternative architectures, particularly convolutional neural networks (CNNs), which are traditionally associated with image processing tasks. Textbooks and courses frequently categorize tasks such as language modeling, translation, or other sequential predictions as the domain of recurrent networks, while convolutional networks are presented primarily in the context of spatial data like images. As a result, the potential of CNNs for sequence modeling is frequently underestimated or ignored.\nConvolutional networks offer several intrinsic advantages that make them well-suited for sequential data. Their inherent parallelism allows for significantly faster training compared to strictly sequential models. Additionally, CNNs are highly effective at capturing local spatial and temporal dependencies, a property that can be leveraged in time-series forecasting and other sequential tasks. Despite these benefits, CNNs are often misunderstood as being unsuitable for sequences due to their lack of explicit memory mechanisms and the absence of intrinsic temporal ordering.\nIn our study, we focus on hurricane trajectory prediction, where the data consists of time-series records of latitude and longitude coordinates. We demonstrate that this type of sequential data can be effectively modeled using CNNs, leveraging their computational efficiency and ability to capture local spatiotemporal patterns. To facilitate this, we encode the locations into a 2D matrix representation, enabling the convolutional network to more effectively extract and learn patterns from the data. Each entry in the matrix corresponds to a “pixel” of an image, an approach we refer to as Trajectory-as-Image. Our methodology employs a standard CNN as a baseline, which is then compared with more specialized sequence models, including TCNs, LSTMs, and RNNs, while incorporating various data augmentation techniques, including two novel methods proposed in this work.\nThrough systematic experimentation and evaluation, we aim to challenge the prevailing notion that convolutional architectures are ill-suited for sequential data. By highlighting the effectiveness of CNNs in sequence modeling, we hope to broaden the perspective of researchers and practitioners, encouraging them to explore convolutional computing as a viable and competitive approach in time-series forecasting and other sequential prediction tasks.\nBenefits and Return on Investment Performance Boost: STFA + PGBA generates structured synthetic sequences that enhance model robustness, reduce overfitting, and improve generalization on unseen storm trajectories.\nPhysics Awareness: Incorporating geographical principles such as distance and bearing increases interpretability and ensures physically consistent predictions.\nNew Research Direction: Establishes two novel paradigms for time-series augmentation based on temporal relevance fading, expanding the methodological toolkit for sequence learning.\nScalability and Reusability: The combined STFA + PGBA + PIML framework can be extended to other sequential forecasting domains such as energy demand, traffic flow, and financial trends.\nOverall Impact: By improving predictive stability and interpretability while maintaining scalability, the proposed approach delivers both scientific value and practical return on computational investment.\n3. Solution Architecture The online platform provides users with up-to-date information on recent storms and a powerful tool for hurricane trajectory predictions. Visitors can either view recent storm data or run predictions using the ML models. The results are displayed interactively on a map, showing storm location, time, and predicted path.\nThe platform is built using a serverless AWS architecture to reduce operational costs while maintaining scalability and reliability. Frontend content is hosted on Amazon S3 and delivered globally via CloudFront, ensuring low-latency access. Users’ requests are routed through API Gateway to Lambda functions, which handle prediction computations and data retrieval. Pre-trained ML models and recent storm datasets are securely stored in S3, with weekly updates managed automatically by EventBridge-triggered crawler Lambdas. Sensitive API keys are stored in Secrets Manager, and system performance is monitored through CloudWatch logs and metrics. IAM enforces least-privilege access for all services.\nThe predictive models leverage the proposed STFA (Stepwise Temporal Fading Augmentation) and PGBA (Plausible Geodesic Bearing Augmentation) techniques. These methods generate realistic synthetic time-series trajectories, preserving temporal relevance and spatial consistency, which significantly improves the robustness and accuracy of the models. By integrating STFA and PGBA, the platform delivers more precise hurricane path predictions, helping users better understand storm behavior and make informed decisions.\nThis architecture enables a responsive, cost-efficient, and secure platform where users can visualize real-time storm information and explore hurricane trajectory forecasts with interactive maps, supported by advanced augmentation methods to boost model performance.\nFigure 1 : Model pipeline Figure 2 : Platform Architecture AWS Services Used Amazon S3: Stores static frontend files, pre-trained ML models, and recent storm data. AWS Lambda: Runs prediction models, fetches storm data, and executes web crawling automation. Amazon API Gateway: Handles frontend requests for predictions and storm data. Amazon CloudFront: Delivers static content globally with low latency. Amazon Route 53: Routes user traffic to CloudFront. Amazon EventBridge: Schedules weekly data crawling. AWS Secrets Manager: Stores external API keys securely. Amazon CloudWatch: Monitors Lambda logs, performance metrics, and system health. AWS IAM: Assigns least-privilege access to all services. Component Design Frontend Layer: Hosted on S3 and delivered through CloudFront. Backend Layer: Handles prediction and data retrieval using API Gateway and Lambda. Data Storage: ML models stored in S3, recent storm data updated weekly by the crawler. Automation: EventBridge triggers a weekly Crawler Lambda to fetch external storm data. Security \u0026amp; Monitoring: Secrets stored in Secrets Manager, metrics and logs collected via CloudWatch. 4. Technical Implementation Implementation Phases This project has three main parts: building the prediction pipeline, setting up data crawling, and deploying the web platform. Each part follows four phases:\nDesign Architecture: Plan AWS serverless stack, Lambda functions, S3 structure. (Weak 1) Estimate Costs: Use AWS Pricing Calculator to assess feasibility and adjust design (Weak 1-2). Optimize Architecture: Fine-tune Lambda memory, S3 usage, and caching to reduce cost (Weak 2-4). Develop, Test, Deploy: Implement Lambda functions, event scheduling, ML model integration, and web frontend with Next.js (Weak 4-8). Technical Requirements\nML Models: Pre-trained trajectory models stored in S3 (.h5/.pth), loaded by prediction Lambda. Storm Data: Weekly updated JSON files, stored in S3, used for frontend display and prediction validation. Serverless Infrastructure: Lambda for prediction, fetching, and crawling; API Gateway for frontend requests; CloudFront/S3 for content delivery. Security: Secrets Manager for API keys, IAM for least-privilege access. Monitoring: CloudWatch for logging and Lambda Insights metrics. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Weak 1): Planning, research on external weather APIs, and ML model preparation.\nInternship (Weak 1-8):\nWeak 1-2: AWS study, architecture design, and cost estimation. Weak 2-4: Optimize architecture, configure serverless workflow, and integrate ML models. Weak 4-8: Implement Lambda functions, set up frontend, test system, and deploy to production. Post-Launch: Continuous data collection and monitoring for up to 1 year.\n6. Budget Estimation Region: ap-southeast-1 (Singapore)\nThe estimated monthly costs for running the hurricane prediction platform on AWS are as follows:\nA. Frontend \u0026amp; Content Delivery\nAmazon S3 (Static Files): Hosts 5 GB of frontend files (HTML, CSS, JS) and handles 10 GB of data transfer per month. Cost ≈ $0.54/month. Amazon CloudFront: Handles 50 GB of data transfer and up to 1 million requests (within free tier). Cost ≈ $6.00/month. Amazon Route 53: 1 hosted zone and 1 million DNS queries per month. Cost ≈ $0.90/month. AWS Certificate Manager (ACM): Provides TLS certificate for secure HTTPS access. Free of charge. Subtotal for Frontend \u0026amp; CDN: ≈ $7.4/month\nB. Backend (API \u0026amp; ML Processing)\nAmazon API Gateway: Handles 1 million HTTP API requests per month, each request approximately 1 MB in size. Cost ≈ $2.5/month. Lambda (Storm Prediction): Predicts hurricane trajectories using ML models. Runs ~1,000 times per day with 512 MB memory allocated and 1 GB ephemeral storage. Each execution lasts ~5 seconds. Cost ≈ $2.54/month. Lambda (Fetch Recent Storm Data): Fetches storm data from S3 for the frontend. Runs ~20,000 times per day with 512 MB memory and 512 MB ephemeral storage for 1 second per execution. Cost ≈ $0.00/month (covered by free tier). Subtotal for Backend: ≈ $4.54/month\nC. Automation \u0026amp; Data Crawling\nAmazon EventBridge: Schedules weekly crawling of storm data (1 cron trigger per day). Free under AWS free tier. Lambda (Web Crawler): Fetches data from external APIs weekly. Uses 128 MB memory and 512 MB ephemeral storage, ~30 seconds per execution. Cost ≈ $0.00/month (free tier). AWS Secrets Manager: Stores 5 API keys for secure access to external weather services. Cost ≈ $2.00/month. Subtotal for Automation \u0026amp; Data Crawling: ≈ $2.0/month\nD. Monitoring \u0026amp; Logging\nAmazon CloudWatch Logs: Collects logs from all Lambda functions and delivers to S3 with 1-month retention. Approximately 2 GB of logs per month. Cost ≈ $0.57/month. CloudWatch Metrics (Lambda Insights): Monitors 8 metrics across Lambda functions. First 10 metrics are free, and only record for predictions and crawl data. Cost ≈ $0.00/month. Subtotal for Monitoring \u0026amp; Logging: ≈ $0.57/month\nE. Storage \u0026amp; Data Transfer\nS3 (Model Bucket): Stores ML models (~1 GB) and handles ~60,000 GET requests per month. Cost ≈ $0.05/month. S3 (Recent Storms Bucket): Stores recent storm data (~1 GB) with ~60,000 GET requests and 30 PUT requests per month. Cost ≈ $0.27/month. F. Tooling \u0026amp; Migration\nAWS CodePipeline and CodeBuild for 10 minutes fixing ≈ $0.90/month. Subtotal for Storage \u0026amp; Data Transfer: ≈ $0.32/month\nTotal Estimated Monthly Cost\nFrontend \u0026amp; CDN: $7.4 Backend (API + ML): $4.54 Automation (Crawler + Secrets): $2.0 Monitoring \u0026amp; Logging: $0.57 Storage \u0026amp; Data Transfer: $0.32 Tooling \u0026amp; Migration: $0.9 TOTAL ≈ $15.73/month\n7. Risk Assessment Risk Matrix\nNetwork Outages: Medium impact, medium probability. Data Source Unavailability: Medium impact, low probability. ML Model Errors: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies\nNetwork: Cache recent storm data in S3 to allow frontend display during outages. Data Sources: Store historical storm data for fallback. ML Model: Regular model validation and testing. Cost: Monitor AWS usage and set budget alerts. Contingency Plans\nSwitch to manual updates if external API fails. Rollback to previous ML model using S3 versioning if new model fails. 8. Expected Outcomes Technical Improvements:\nReal-time hurricane trajectory predictions with visualized paths. Scalable serverless system capable of handling thousands of requests/day. Long-term Value:\nCentralized hurricane data for research and analysis. Framework reusable for other geospatial prediction tasks. Low monthly operational cost (\u0026lt; $20/month). "},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.3-event3/","title":"Reinventing DevSecOps with AWS Generative AI","tags":[],"description":"","content":"Summary Report: “Reinventing DevSecOps with AWS Generative AI” Event Objectives To share how DevSecOps has evolved with the automation of system operations through AI. To provide detailed knowledge about the DevSecOps process. To answer how DevSecOps engineers are applying AI to reshape workflows and drive transformation. Speakers Lê Thanh Đức – Cloud Delivery Manager, CMC Global Dư Quốc Thành – Technical Leader, CMC Global Văn Hoàng Kha – Cloud Engineer, AWS Community Builder Key Highlights Overview of DevSecOps The DevSecOps framework is more than a traditional DevOps cycle. It integrates security into multiple parts and throughout the entire lifecycle. DevSecOps Lifecycle The DevSecOps lifecycle includes seven main components, starting from planning to operations. The detailed steps are outlined below: Below are all stages in the DEVSECOPS LIFECYCLE, formatted as requested:\n1. PLAN:\nIdentify security requirements and risks from the start. Align goals between Dev, Sec, and Ops teams. Create a security roadmap linked to project objectives. 2. CODE:\nApply secure coding standards and conduct code reviews. Use SAST tools to detect early vulnerabilities. Build a “security-first” mindset among developers. 3. BUILD:\nAutomate security checks within CI/CD pipelines. Perform dependency and binary scans. Ensure consistent and secure builds. 4. TEST:\nConduct vulnerability scans, DAST, and penetration tests. Validate that applications meet security requirements. Update tests to address newly discovered vulnerabilities. 5. DEPLOY:\nVerify configurations and IaC before deployment. Automate runtime configuration monitoring. Minimize manual errors and ensure secure deployment. 6. OPERATE:\nAutomate patching and continuous security updates. Implement incident response and performance monitoring. Maintain stability and safety after release. 7. MONITOR:\nContinuously monitor activities and potential threats. Use real-time analytics and alerting tools. Detect risks early and respond quickly. DevSecOps Toolchain Overview Pre-commit \u0026amp; Code Quality:\nSonarQube, Codacy, Semgrep (SAST), Gitleaks – detect code issues and secrets before commit. Dependency \u0026amp; SBOM Scanning:\nSyft, Grype, Dependency-Track – manage packages and detect library vulnerabilities. IaC \u0026amp; Policy-as-Code:\nCheckov, TFsec – scan Terraform/Kubernetes configurations. OPA Gatekeeper, Kyverno – automatically enforce policies and compliance. SAST / DAST \u0026amp; Security Tests:\nTrivy, Checkmarx, Semgrep, Codacy – identify code and runtime vulnerabilities. CI/CD Integration:\nJenkins, GitHub Actions, GitLab CI, ArgoCD – automate build, test, and secure deployment. Monitoring \u0026amp; Logging:\nPrometheus, Grafana, Loki, Promtail – real-time monitoring and observability. Alerting \u0026amp; Governance:\nSlack webhooks, Email alerts, AI anomaly detection – provide alerts and quick responses. Centralized risk reports – aggregate and analyze security risks. Benefits of GenAI in DevSecOps 1. Automation \u0026amp; Acceleration of DevSecOps Processes\nAutomated code review and security scanning. Pipeline generation. IaC generation. 2. Enhanced Proactive Security\nNatural language-based threat modeling. Policy-as-code generation. Dynamic scanning augmentation. 3. Optimized Observability \u0026amp; Incident Response\nIncident summaries and root cause analysis. ChatOps integration. Anomaly detection. Key Takeaways Strategy for Integrating AI into Workflow AI improves incident detection and vulnerability analysis. It accelerates response and remediation, saving time and effort. Helps reduce manual workload while maintaining high-quality outcomes. Enables continuous learning and improvement within the DevSecOps cycle. Applying Amazon Q in Coding Use Amazon Q to assist in secure coding. Evaluate carefully whether to approve AI-suggested actions. Utilize it for documentation search and recommendation across multiple sources. Event Experience Attending the “Reinventing DevSecOps with AWS Generative AI” workshop provided me with practical insights into how AI is reshaping the entire DevSecOps workflow — from planning, coding, and testing to operations and monitoring. Below are the key experiences:\nHands-on Technical Experience Gained in-depth understanding of the DevSecOps Lifecycle, from Plan – Code – Build – Test – Deploy – Operate – Monitor, and how to integrate security across all stages of software development. Observed real-world examples of CI/CD pipelines with integrated automated security checks, emphasizing the importance of early vulnerability detection. Modern Tool Application Learned how to use Amazon Q in the DevSecOps workflow — from secure code generation to IaC creation and automated issue fixing. Explored DevSecOps toolchain components such as SonarQube, Checkov, Trivy, Prometheus, and Grafana, understanding how they work together for end-to-end security. Recognized the critical role of AI in safe code creation, testing, and deployment, especially through policy-as-code and natural language-based threat modeling. Networking and Discussion Had opportunities to discuss directly with DevSecOps experts from AWS and CMC Global, gaining valuable insights from real-world implementation. Engaged with the developer community to explore how AI can be integrated into DevSecOps pipelines, inspiring new ideas for my own projects. Lessons Learned DevSecOps is not just about CI/CD automation but about embedding security as a fundamental part of the entire software lifecycle. Implementing AI and Generative AI enhances vulnerability detection, test automation, and incident response efficiency. Amazon Q is a promising tool that helps developers and security engineers optimize their workflow, minimize manual effort, and boost productivity. Effective DevSecOps requires tight collaboration between people, processes, and technology, with AI serving as an intelligent assistant for better decision-making and faster actions. Some event photos Figure 1 Figure 2 Figure 3 Overall, the event not only deepened my understanding of DevSecOps but also reshaped my perspective on how AI can enhance security integration, streamline development workflows, and strengthen collaboration across engineering teams.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn key concepts about Amazon Lightsail. Understand the scaling process for EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the origin and characteristics of time-series data prepared for model training. - Propose algorithms to diversify data and enrich patterns during model training. 22/09/2025 22/09/2025 3 - Learn about Amazon Lightsail Workshop and related information. - Practice using service features. 23/09/2025 23/09/2025 https://000045.awsstudygroup.com/ 4 - Explore the functionalities of Amazon Lightsail Containers. - Create and test container instances. 24/09/2025 24/09/2025 https://000046.awsstudygroup.com/ 5 - Study EC2 Auto Scaling concepts: + Manual Scaling + Dynamic Scaling + Scheduled Scaling + Predictive Scaling 25/09/2025 25/09/2025 https://000006.awsstudygroup.com/ 6 - Translate the technical blog titled \u0026ldquo;Introducing Universal Installers for AWS CLI v2 on macOS\u0026rdquo; 26/09/2025 26/09/2025 https://aws.amazon.com/blogs/devops/introducing-universal-installers-for-aws-cli-v2-on-macos/ Week 3 Achievements: A – Amazon Lightsail Workshop\nExplored 3 applications available on Lightsail:\nWordPress PrestaShop Akaunting Deployed a database on Lightsail:\nCreated specific login credentials Selected the standard $15 database plan Set up a name, added key–value tags, and completed database creation Tested WordPress instance creation:\nChose and deployed a WordPress server Configured Ubuntu and networking settings Followed setup instructions to configure and finalize WordPress installation Created a PrestaShop E-Commerce instance:\nLearned that PrestaShop allows users to create an online store and integrate desired payment gateways Deployed a $5 PrestaShop instance following provided guidelines Configured network connections and achieved successful deployment Experimented with an Akaunting instance:\nUnderstood that Akaunting is designed for financial management and can integrate with WordPress and PrestaShop Created a $5 instance from the available plan Configured connections and completed deployment Security setup: Configured security for all three instances, reviewed larger plan options, and added alarms.\nB – Amazon Lightsail Container\nUnderstood the basic concepts of Lightsail Containers used to run applications.\nCompleted necessary preparation steps.\nSuccessfully created a container service.\nDeployed a container from a Public Image.\nTested deployment from a local system:\nCreated a Lightsail instance and configured AWS CLI as instructed Installed Docker on Ubuntu and built a container image Pushed the created image and deployed it successfully Cleaned up resources after practice C – EC2 Auto Scaling\nStudied EC2 Auto Scaling and its different strategies:\nManual Scaling Dynamic Scaling Scheduled Scaling Predictive Scaling Preparation steps:\nCreated VPC environment, EC2 instance, and RDS database instance Set up data for the database and deployed a web server Prepared data for predictive scaling, uploaded to CloudWatch, and validated Learned about AMI and Launch Templates:\nCreated an Amazon Machine Image (AMI) from an EC2 instance Built a Launch Template Performed all required configurations Load Balancer setup:\nStudied Elastic Load Balancing (ELB) Created an Application Load Balancer Target Group Configured and launched the load balancer from EC2 Auto Scaling Group:\nSuccessfully tested and created an Auto Scaling Group to manage EC2 instances Scaling solution testing:\nTested manual, scheduled, and dynamic scaling Interpreted results from predictive scaling solutions "},{"uri":"https://giaphazzz.github.io/aws/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Introducing universal installers for AWS CLI v2 on macOS by Andrew Asseily | on 11 SEP 2025 | in Announcements, AWS CLI |\nAmazon Web Services (AWS) is announcing the availability of universal macOS installers for the AWS Command Line Interface (AWS CLI) v2.\nWhat’s new Starting with AWS CLI v2 version 2.30.0, the AWS CLI installers will provide universal binary support for macOS that works natively on both Apple silicon and Intel processors with a single download. This eliminates the need for Rosetta translation, a compatibility layer that enables Intel-based applications to run on Apple silicon Macs.\nUpdating existing AWS CLI installations If you’re using AWS CLI v2 on an Apple-silicon Mac, we recommend you upgrade to the latest version to install native binaries.\nThese changes only affect the official AWS CLI installers—building the AWS CLI from source will continue to natively support the host architecture.\nHave questions or feedback? Contact us on GitHub.\nAndrew Asseily\nAndrew is a Software Development Engineer on the AWS CLI team. Outside of work, he’s an avid Brazilian Jiu-Jitsu practitioner.\n"},{"uri":"https://giaphazzz.github.io/aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Exploring Quantum Measurements, Observables and Operators: Practical insights with Amazon Braket This blog explores the mathematical foundations and practical implementation of quantum measurement techniques using Amazon Braket. You will learn how basis transformations, projectors, and observables work together to convert hidden quantum phase information into measurable outcomes — a key concept behind many quantum algorithms. The post also includes hands-on Python examples using Amazon Braket to simulate measurements in arbitrary bases, visualize results on the Bloch sphere, and connect theoretical insights with real quantum hardware experiments.\nBlog 2 - Implement network connectivity patterns for Oracle Database@AWS This blog explains how to design and implement network connectivity patterns for Oracle Database@AWS (ODB@AWS). You will learn how to connect Oracle Exadata infrastructure running on AWS with your applications, on-premises environments, and multi-VPC architectures using AWS Transit Gateway and AWS Cloud WAN. The article details multiple IP routing-based architectures, including direct VPC peering, single-Region scalability, and multi-Region global connectivity. It also covers DNS configuration, security considerations, and best practices for integrating ODB@AWS into complex AWS network topologies while maintaining performance, security, and flexibility.\nBlog 3 - Introducing universal installers for AWS CLI v2 on macOS This blog announces the release of universal macOS installers for the AWS Command Line Interface (AWS CLI) v2, starting from version 2.30.0. With this update, a single installer now runs natively on both Apple silicon and Intel processors, removing the need for Rosetta translation and simplifying installation across macOS devices.\n"},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.3-ml-model/","title":"Machine Learning Model","tags":[],"description":"","content":"MODEL TRAINING PROCESS This work presents the development of a predictive model designed to estimate a storm’s next geographical position using historical observational data from its previous path. To put it simply, we use a sequence of past latitude and longitude values to predict the next latitude and longitude in the future.\nFeature Engineering After completing the data preprocessing stage, we split the dataset into 70% training, 10% validation, and 20% testing. This is done by storm ID, ensuring that no storm appearing in the validation or test set is included in the training set. This prevents data leakage and ensures the reliability of evaluation results.\nFor model training, each input consists of a sequence of 4 consecutive time steps, where each step represents an observation interval of 3 hours. Thus, one input sequence covers a total of 9 hours of storm movement.\nFigure 1 : Dataset Distribution Applying Stepwise Temporal Fading Augmentation (STFA) To enhance the diversity of the training data, we apply our proposed method—Stepwise Temporal Fading Augmentation (STFA)—to 50% of the training set, selected based on unique storm IDs. The original sequences from these storms are replaced by their augmented counterparts, ensuring the final size of the training set remains unchanged (approximately 100% of the original size).\nAs introduced earlier in the proposal section, STFA modifies the older points in a sequence while keeping the most recent observations unchanged. For each 4-step sequence:\nThe latest 2 time steps remain the same The older 2 time steps are scaled using fading coefficients: [0.98,; 0.99] Although these values appear small, latitude and longitude are extremely sensitive features. Even a tiny change—such as from 6.7 to 6.8—can correspond to tens of kilometers of displacement in the real world. Therefore, using modest fading values is both logical and physically meaningful, ensuring the augmented data remains realistic.\nExample of STFA on a 4-Step Sequence Here is a simple example showing how a 4-step time-series sequence is transformed after applying STFA:\nRow Original (lat, lon) Augmented (lat, lon) Operation 1 [-6.8 , 107.5] [-6.66 , 105.35] multiplied by 0.98 2 [-7.0 , 107.1] [-6.93 , 106.03] multiplied by 0.99 3 [-7.3 , 106.7] [-7.3 , 106.7] unchanged 4 [-7.5 , 106.4] [-7.5 , 106.4] unchanged This process reduces the magnitude of older observations while keeping the recent ones intact. The augmentation introduces controlled variability, helping the model generalize better for trajectory forecasting.\nEarlier, we used physics-informed machine learning to compute distance and bearing between time steps using the Haversine formula. After STFA is applied, these values are recomputed based on the augmented coordinates. This guarantees that all physical features remain accurate and consistent with the updated trajectory.\nFigure 2 : Comparison of Augmentation Techniques on Storm Trajectories Model Setting 1.Physics-Informed Loss Our use of the Haversine formula does not end at feature engineering. In addition to generating distance and bearing values, we also incorporate the Haversine distance as a custom loss function, alongside traditional losses such as MSE, RMSE, and MAPE.\nBecause the Haversine formula computes the true geodesic distance between two geographical points, it serves as a natural metric for evaluating the error in the model’s predicted storm location. A larger Haversine distance indicates that the prediction is far from the true position, whereas a smaller value means the model is performing well. The optimal Haversine loss, ideally, is 0 km.\nThe formula was previously introduced in Section 2: Proposal, so we do not rewrite it here.\nExample:\nModel prediction: [-6.72, 107.1] True location: [-6.8, 107.5] Haversine loss: 45.06 km The value 45.06 km directly reflects the real-world positional error, making the loss physically interpretable.\nThis is what we refer to as physics-informed machine learning loss, where physical laws and domain knowledge guide the model’s optimization.\n2.Model Architect Sequence modeling tasks are traditionally handled by recurrent architectures such as RNNs, LSTMs, or GRUs. However, recent research has shown that convolution-based approaches can outperform these methods in many time-series applications.\nWhile CNNs do not inherently model sequence order, they excel at:\nextracting local spatial/temporal features parallel processing faster training stable gradients efficient scaling For our project, we adopt a convolution-based architecture as the foundation. Specifically, our main model is a Temporal Convolutional Network (TCN).\nTemporal Convolutional Network (TCN) A TCN uses 1D dilated convolutions, enabling the model to have a receptive field that expands over time, allowing it to “see” far into the past without requiring recurrence.\nExample (sequence = [a, b, c, d]):\nLayer 1 (dilation = 1): model sees [d] Layer 2 (dilation = 2): model sees [b, d] Layer 3 (dilation = 4): model can see [a, b, c, d] By stacking dilated convolutions, the network learns long-range dependencies while retaining all the benefits of fast convolution operations. Thus, TCNs combine memory of the past with computational efficiency, making them an ideal choice for storm trajectory forecasting.\n3. Model Hyperparameters Below are the key hyperparameters used in our training configuration:\nInput dimensions: 4 (latitude, longitude, distance, bearing) Hidden units: 1024 Number of TCN layers: 2 Learning rate: 1e-4 Epochs: 80 Optimizer: Adam Early stopping: patience = 6 (based on overall loss) 4. Composite Loss Function Our main training loss is a weighted combination of multiple components:\nMSE on latitude and longitude MSE on distance and bearing (auxiliary features) Haversine loss (physics-informed component) The contribution of each auxiliary loss is controlled by weighting factors:\nλ_aux = 0.5 (for distance and bearing MSE) λ_hav = 0.3 (for the Haversine loss) This design ensures that the model:\nlearns to minimize coordinate errors, respects physical displacement, and does not overfit to any single feature. By integrating physics-informed loss terms with data-driven learning, the model becomes more stable, consistent, and aligned with real-world storm dynamics.\nFigure 3 : Training Process Evaluation After the training process concludes and the model triggers early stopping, performing an evaluation on the test set is necessary to determine whether the model generalizes well and is ready for real-world deployment. We evaluate the model using Overall Loss, MSE, RMSE, MAPE, and Haversine distance to gain deeper insight into its performance:\nTotal Loss: 74.3849 MSE: 0.0832 RMSE: 0.2772 MAPE: 0.60% Haversine (km): 30.75 From these results, we observe that the average positional error is approximately 30 km from the true location. This level of error is acceptable because hurricanes are extremely large systems, often spanning hundreds to thousands of kilometers. The MSE of only 0.08 for latitude and longitude also indicates strong predictive accuracy, suggesting that the model can effectively estimate realistic future storm trajectories with minimal error.\nThese results further demonstrate that convolutional computations can perform very well on sequence-modeling tasks, and should be considered a strong alternative to traditional sequential architectures such as RNNs, LSTMs, or GRUs—not only for images but also for structured spatiotemporal data.\nWith the model validated, the next step is to upload the trained model to an Amazon S3 bucket and use an AWS Lambda function to load and execute it in response to user inference requests.\nThe following sections will focus on how we design and deploy our online prediction platform, making the hurricane trajectory model accessible for public use.\nFigure 4 : Evaluation Metrics "},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.4-event4/","title":"AWS Cloud Mastery Series #1","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS Workshop” Event Objectives Provide an overview of AWS’s AI/ML ecosystem Introduce Generative AI with Amazon Bedrock Explore real-world applications of Prompt Engineering and RAG Demonstrate how AI services integrate into cloud-based products Present AgentCore for scalable, production-ready AI agents Speakers Lam Tuan Kiet – Senior DevOps Engineer, FPT Software Dang Hoang Hieu Nghi – AI Engineer, Reonova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, FCJ Key Highlights Transformation from Traditional ML to Foundation Models Traditional ML models\nTask-specific Require pre-labeled datasets Limited generalization Generative AI (Foundation Models)\nTrained on massive unlabeled datasets Self-supervised learning Capable of multiple tasks using prompts Bedrock supports major FM providers: OpenAI, DeepSeek, Anthropic Claude, Meta Llama, Amazon Titan Prompt Engineering Essentials What is a prompt? Instructions that guide a model’s behavior and output\nTechniques\nZero-shot prompting: Minimal instruction → concise but sometimes shallow output Few-shot prompting: Provide examples to guide patterns Chain-of-Thought prompting: Add reasoning guidance → higher-quality, step-by-step outputs Retrieval-Augmented Generation (RAG) Enhances model outputs by adding relevant, external knowledge\nProcess:\nUser gives a prompt System retrieves related documents Combined context fed into the model Model generates an informed answer Embeddings\nConvert text → vectors representing semantic meaning Enable clustering of similar concepts Titan Text Embeddings support 100+ languages RAG improves accuracy, reduces hallucinations, and grounds responses in real data\nAWS AI Services Overview A suite of ready-to-use APIs:\nRekognition – Image/video object detection Translate – Automatic translation \u0026amp; language detection Textract – Extract text and layout from documents Transcribe – Speech-to-text, voice identification Polly – Text-to-speech Comprehend – NLP insights, sentiment analysis Kendra – Intelligent enterprise search (e.g., auto-grading student submissions) Lookout Family – Anomaly detection for metrics, equipment, images Personalize – Recommendation systems Pipecat – AI agent pipeline framework Amazon Bedrock AgentCore A platform to streamline development and deployment of AI agents:\nAutomates challenging aspects of GenAI production:\nScaling Context memory Identity \u0026amp; access control Tool and API integration Workflow orchestration Observability and tracing Key Components\nRuntime – Executes multi-step agent tasks Memory – Stores past interactions Identity – Defines access rules Gateway – Unified access to tools \u0026amp; services Code Interpreter – Safe code execution environment Browser tool – Retrieve external web information Observability – Logs, metrics, debugging Key Takeaways AI \u0026amp; Cloud Development Mindset Companies are shifting from simple ML models to AI-driven cloud-native products Building real projects is crucial — not just assignments for grades Foundation models + cloud services = faster product delivery Technical Knowledge Understanding how prompt engineering affects output quality RAG improves factual correctness and supports structured knowledge integration Embeddings enable powerful search and clustering capabilities AWS AI services cover end-to-end workflow: speech, text, vision, anomaly detection Agent Development AgentCore simplifies workflows that used to require multiple services and heavy DevOps Essential for building: Chatbots Automation agents Research assistants Knowledge-based applications Applying to Work Develop real product prototypes and include them in the CV Use RAG to build domain-specific AI systems Integrate AWS AI APIs to accelerate feature development Leverage Bedrock Agents for multi-step workflows Explore embeddings for semantic search, recommendation, and classification Practice prompt engineering techniques to improve model performance Event Experience Attending the “AI/ML/GenAI on AWS” workshop was highly insightful, providing a comprehensive look into modern AI development on the AWS cloud. Key experiences include:\nKnowledge from Industry Practitioners Speakers from FPT, Reonova Cloud, and AWS shared practical insights on how companies build cost-efficient AI solutions on cloud platforms. Learned how modern businesses deploy AI across vision, text, speech, and automation workflows. Hands-on Understanding of GenAI Concepts Deepened understanding of how Foundation Models differ from traditional ML. Gained clarity on prompting styles, why they matter, and how they change model behavior. Saw practical examples of RAG pipelines improving accuracy and reliability. Exposure to AWS Tools Understood the roles of Rekognition, Textract, Transcribe, Translate, and Comprehend across real use cases. Explored how Bedrock AgentCore simplifies building scalable GenAI applications with memory, access control, and tool integration. Networking \u0026amp; Practical Advice Discussed with engineers how to build real, production-quality projects instead of school-level assignments. Learned about product-focused mindsets and industry expectations for modern AI skillsets. Lessons Learned Combining AI + cloud services accelerates real-world product development. Prompt engineering and RAG are essential for enterprise-grade GenAI. AgentCore represents the next step in scalable AI application design. Some event photos Figure 1 Figure 2 Figure 3 Overall, the workshop provided both strategic understanding and hands-on technical knowledge, helping shape a stronger foundation for AI/ML development on AWS.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master basic concepts of cloud computing Learn AWS CloudWatch, CloudFront, CLI, … Tasks to be carried out this week: Day Task Start Date Completion Date Reference Materials 2 - Learn about AWS CloudWatch and related concepts 29/09/2025 29/09/2025 https://000008.awsstudygroup.com/ 3 - Get familiar with AWS Command Line Interface (CLI), test operations with services + S3 Bucket + SNS + IAM + VPC + … 30/09/2025 30/09/2025 https://000011.awsstudygroup.com/ 4 - Research information about CloudFront and how to use it 1/10/2025 1/10/2025 https://000094.awsstudygroup.com/ 5 - Learn about Amazon DynamoDB and its features + Core components + Primary Keys + Secondary Index + Naming rules and query data 2/10/2025 2/10/2025 https://000060.awsstudygroup.com/ 6 - Standardize and complete data for the project - Study the Temporal Convolutional Network model to be used during training 3/10/2025 3/10/2025 Week 4 Achievements: A – AWS CloudWatch\nUnderstand that AWS CloudWatch is a monitoring and management service that provides actionable insights for AWS resources.\nWork with Metrics, view application Logs, and create Alarms from collected Metrics.\nPrepare to work with CloudWatch:\nCreate a stack in CloudFormation Configure basic stack settings Complete stack creation and deployment Work with CloudWatch metrics:\nView EC2 metrics Analyze CPU utilization Customize chart visualization Learn about search, math expressions, and dynamic labels and experiment with them\nExamine CloudWatch Logs in detail:\nView logs from EC2 instances Practice creating a log Create metric filters to collect and transform data into CloudWatch metrics Explore CloudWatch Alarms and CloudWatch Dashboards features\nB – Getting familiar with AWS Command Line Interface (CLI)\nUnderstand that AWS CLI is an open-source tool allowing users to interact with AWS services via command-line shell.\nDownload AWS CLI and perform setup to start using it\nWork with Amazon S3 via AWS CLI:\nCreate S3 Buckets List buckets and objects Delete objects and buckets Experiment with SNS and IAM via CLI:\nCreate SNS topic Create IAM groups, users, etc. Check group information, add/remove access keys Use CLI to interact with VPC:\nCreate VPC and its components Test creating an Internet Gateway Review troubleshooting scenarios when working with AWS CLI\nC – CloudFront\nStudy CloudFront with S3 Bucket as Origin\nCreate an S3 Bucket to prepare for later steps\nUpload index.html to the newly created S3 Bucket\nConfigure settings for Amazon CloudFront\nTest removing CloudFront from S3 after usage\nD – Amazon DynamoDB\nUnderstand that Amazon DynamoDB is a NoSQL database service supporting fast and predictable performance.\nLearn key properties of DynamoDB:\nCore components Keys Secondary Index Naming rules and data types Additional notes on consistency and read/write capacity modes Perform preparation steps:\nCreate access key, table, read/write, and update data Create a global secondary index and query it Practice with AWS SDK:\nSet up AWS CLI first Create table, perform read-write-update-delete operations Query and scan data, delete tables after use "},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Sharing sessions about the Industry 4.0 technology transformation process and the usefulness of AI in technology development.\nLesson learned: Applying AI-DLC in project development strategies helps increase work efficiency. Generative AI should be used for research and learning new knowledge.\nEvent 2 Event Name: Data Science on AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: Hall A, 5th floor, FPT University, High-Tech Park\nRole: Attendee\nMain activities: Introduction to AWS services specialized for AI and explanations of machine learning concepts.\nLesson learned: The application of AWS services in building and deploying ML models has become indispensable today. It not only saves time but also delivers high performance and good accuracy.\nEvent 3 Event Name: Reinventing DevSecOps with AWS Generative AI\nDate \u0026amp; Time: 19:30, October 16, 2025\nLocation: Online via Teams Meeting\nRole: Attendee\nMain activities: Sharing about how DevSecOps has evolved in automating system operations using AI.\nLesson learned: DevSecOps is not only about automating the CI/CD pipeline but also about integrating security as a core part of the entire software development lifecycle. The adoption of AI and Generative AI accelerates vulnerability detection, automates testing processes, and enhances incident response capabilities.\nEvent 4 Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Introduce the practical applications of AI and prompt engineering in work and daily life. Explore several AWS machine learning services that assist with a variety of tasks, including image recognition, text-to-speech, search, personalization, and more.\nLesson learned: AI and prompt engineering are not just tools for automation, but powerful enablers to enhance productivity and decision-making in daily work and personal tasks. Leveraging AWS ML services—such as image recognition, text-to-speech, search, and personalization—demonstrates how AI can simplify repetitive tasks, provide insights faster, and improve user experience. Integrating these services thoughtfully into workflows ensures efficiency while maintaining control and reliability.\nEvent 5 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 8:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Learning DevOps principles, building automated CI/CD pipelines, exploring Infrastructure as Code (IaC) with CloudFormation and CDK, comparing AWS container services (ECS, EKS, App Runner), and understanding observability practices using CloudWatch.\nLesson learned: Effective DevOps is not only about implementing automation in CI/CD pipelines but also about fostering collaboration between development and operations teams, integrating continuous feedback, and leveraging IaC for repeatable, reliable infrastructure. Using AWS services to containerize applications and enhance observability ensures faster delivery, reduced human error, and scalable, resilient systems.\nEvent 6 Event Name: AWS Edge Services Workshop - Amazon CloudFront as Your Foundation\nDate \u0026amp; Time: 9:00, November 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Exploring CloudFront’s edge capabilities, including security features (TLS, mutual TLS, origin cloaking), cost optimization techniques, caching strategies, edge logic implementation with CloudFront Functions/Lambda@Edge, and best practices for high availability, performance, and reliability.\nLesson learned: Implementing edge services goes beyond deploying a CDN; it requires integrating security, caching, and failover strategies to optimize performance and reduce origin load. Leveraging CloudFront’s advanced features and following best practices ensures resilient, cost-effective, and globally performant content delivery while maintaining full operational visibility.\nEvent 7 Event Name: Master 3 — Security (AWS Well-Architected Security Pillar)\nDate \u0026amp; Time: 9:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Exploring the core principles of the AWS Well-Architected Security Pillar, including identity and access management, continuous monitoring, multi-layer network protection, incident response automation, and multi-account governance. The session also covered hands-on insights into services such as IAM Access Analyzer, GuardDuty, Security Hub, CloudTrail, and EventBridge to help build secure and scalable AWS architectures.\nLesson learned: Security is an ongoing process rather than a one-time configuration. Applying least privilege, enforcing strong authentication, maintaining comprehensive logging, and automating detection and response workflows are essential to reducing risks. Combining GuardDuty, Security Hub, and EventBridge enhances visibility and consistency across accounts and regions. A defense-in-depth approach greatly improves reliability, availability, and overall system resilience in the cloud.\nEvent 8 Event Name: CloudThinker – Building Agentic AI \u0026amp; Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 9:00, December 05, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Attended a knowledge-sharing session led by CloudThinker and AWS, focusing on how agentic AI systems are designed and optimized using Amazon Bedrock. The discussion explored the role of context management, multi-agent orchestration, and how modern AI can support dynamic, automated operations in cloud environments.\nLesson learned: The event emphasized that DevSecOps has expanded far beyond traditional CI/CD automation. Today, incorporating security at every stage of the development lifecycle is essential—and AI plays a major role. With the help of advanced models and Generative AI, teams can detect vulnerabilities earlier, streamline testing, and respond to incidents more efficiently. Integrating agentic AI principles further enhances system reliability and creates more proactive, intelligent DevSecOps workflows.\n"},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/","title":"AI-Model-Integration","tags":[],"description":"","content":"Actions to Integrate Hurricane Prediction Model from Lambda Overview Below here we will represent how did we integrate our model from lambda to use step-by-step.\nFiles Currently Using Mock Data 1. WeatherOverlay.tsx (IMPORTANT) Location: frontend/src/components/WeatherOverlay.tsx Mock data: Temperature and Wind overlay data Function: generateWeatherData() Required change: Replace with an API call to Lambda 2. WeeklyForecast.tsx Location: frontend/src/components/WeeklyForecast.tsx Mock data: mockForecast array Required change: Fetch from the backend API 3. windData.ts Location: frontend/src/lib/windData.ts Mock data: mockWindData Required change: Fetch from OpenWeatherMap or Lambda 4. WindFieldManager.ts Location: frontend/src/components/wind/WindFieldManager.ts Mock data: Fallback when there is no API key Status: OK — it already fetches from OpenWeatherMap; you just need to configure the API key How to Integrate the AI Model from Lambda Step 1: Add API endpoints for Storm Prediction In frontend/src/api/weatherApi.ts, add:\nexport interface StormPrediction { stormId: string; name: string; nameVi: string; currentPosition: { lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }; historicalTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }\u0026gt;; forecastTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; confidence?: number; // Confidence score from the AI model }\u0026gt;; } export const weatherApi = { // ... existing methods ... // Get storm predictions from the Lambda AI model getStormPredictions: async (): Promise\u0026lt;StormPrediction[]\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction[]\u0026gt;(\u0026#39;/storms/predictions\u0026#39;); return response.data; }, // Get details for a specific storm getStormById: async (stormId: string): Promise\u0026lt;StormPrediction\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction\u0026gt;(`/storms/${stormId}`); return response.data; }, }; Step 2: Update the Backend to call Lambda In the C# backend (backend/Controllers/WeatherController.cs), add an endpoint:\n[HttpGet(\u0026#34;storms/predictions\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetStormPredictions() { try { // Call the Lambda function var lambdaClient = new AmazonLambdaClient(); var request = new InvokeRequest { FunctionName = \u0026#34;storm-prediction-function\u0026#34;, InvocationType = InvocationType.RequestResponse, Payload = \u0026#34;{}\u0026#34; // Or parameters if needed }; var response = await lambdaClient.InvokeAsync(request); using var reader = new StreamReader(response.Payload); var result = await reader.ReadToEndAsync(); return Ok(JsonSerializer.Deserialize\u0026lt;List\u0026lt;StormPrediction\u0026gt;\u0026gt;(result)); } catch (Exception ex) { return StatusCode(500, new { error = ex.Message }); } } Step 3: Update the Frontend to use the real API In frontend/src/pages/Index.tsx (or wherever storm data is fetched):\nimport { weatherApi } from \u0026#39;../api/weatherApi\u0026#39;; import { useQuery } from \u0026#39;@tanstack/react-query\u0026#39;; // Instead of using mock data const { data: storms, isLoading } = useQuery({ queryKey: [\u0026#39;storms\u0026#39;], queryFn: () =\u0026gt; weatherApi.getStormPredictions(), refetchInterval: 5 * 60 * 1000, // Refresh every 5 minutes }); Step 4: Configure Environment Variables Frontend (.env.production):\nVITE_API_BASE_URL=https://your-backend-api.com/api/weather Backend (appsettings.json):\n{ \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;LambdaFunctionName\u0026#34;: \u0026#34;storm-prediction-function\u0026#34; } } Deployment Checklist Deploy the AI model to Lambda Test the Lambda function with sample input Add the API endpoint in the C# backend Test the backend endpoint Update weatherApi.ts with the new endpoints Replace mock data with real API calls Test the frontend with real data Update .env.production with the production URL Build and deploy the frontend Monitor logs and errors Notes Caching: You should cache results from Lambda to reduce cost Error handling: Handle Lambda timeouts or errors gracefully Loading states: Show loading indicators while fetching Fallback: You can keep mock data as a fallback when the API fails Files You Don’t Need to Change (Examples Only) These files are demos/examples and do not affect production:\n*.example.tsx */__tests__/* */GUIDE.md "},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/","title":"Fix-Unicode-Error","tags":[],"description":"","content":"Fixing UnicodeDecodeError in Lambda There is a series error we want to talk about during the process of development, and this section will talk about it.\nIssue UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64: invalid start byte Root Cause The PyTorch model uses the .pth extension (a binary file). The Python runtime also uses .pth files for path configuration (text files). If the model .pth is placed directly in LAMBDA_TASK_ROOT, Python may try to read it as text → causing the error. Applied Fix 1. Update Dockerfile Move the model into a subdirectory models/:\n# Copy model file to subdirectory to avoid Python .pth file confusion RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. Update app.py Update the model search paths:\npossible_paths = [ \u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;, # Primary location in Lambda \u0026#39;models/cropping_storm_7304_2l.pth\u0026#39;, tcn_path ] Rebuild \u0026amp; Deploy Steps Step 1: Build the Docker image cd storm_prediction docker build --provenance=false --platform linux/amd64 -t storm-prediction-model . Note: --provenance=false helps reduce image size for pushing to ECR.\nStep 2: Tag the image docker tag storm-prediction-model:latest 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 5: Update Lambda AWS Console → Lambda → storm-prediction Open the Image tab Click Deploy new image Select the latest image Click Save Step 6: Test curl -X POST \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 14.5, \u0026#34;lng\u0026#34;: 121.0}, {\u0026#34;lat\u0026#34;: 14.6, \u0026#34;lng\u0026#34;: 121.1}, {\u0026#34;lat\u0026#34;: 14.7, \u0026#34;lng\u0026#34;: 121.2}, {\u0026#34;lat\u0026#34;: 14.8, \u0026#34;lng\u0026#34;: 121.3}, {\u0026#34;lat\u0026#34;: 14.9, \u0026#34;lng\u0026#34;: 121.4}, {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 121.5}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 121.6}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 121.7}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 121.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Check Logs After Deploy aws logs tail /aws/lambda/storm-prediction --region ap-southeast-1 --follow Summary Before: The .pth model file was in the root → Python mistook it for a config .pth file → UnicodeDecodeError After: The .pth model file is inside models/ → Python ignores it → Lambda works "},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/","title":"Frontend Architecture","tags":[],"description":"","content":"Frontend Architecture - Storm Prediction Web Application Overview Below is the detailed documentation of our front-end development: a React + TypeScript web application for tracking and predicting typhoon trajectories.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐ │ USER BROWSER │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ - Distribution: d3lj47ilp0fgxy.cloudfront.net │ │ - SSL/TLS: HTTPS │ │ - Cache: Static assets + JSON data │ │ - Origin Access: OAI/OAC (Secure) │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ S3 Bucket (Private) │ │ - Bucket: storm-frontend-hosting-duc-2025 │ │ - Static Website Hosting: DISABLED │ │ - Access: CloudFront only via REST API │ │ - Content: HTML, CSS, JS, Images, recent_storms.json │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Functions │ │ ┌─────────────────────────────────────────────────────┐ │ │ │ Lambda #1: Storm Prediction │ │ │ │ - URL: vill3povlzqxdyxm7ubldizobu0kdgbi... │ │ │ │ - Method: POST /predict │ │ │ │ - Auth: NONE (public) │ │ │ │ - Container: ECR (Docker) │ │ │ │ - Models: LSTM + TCN │ │ │ └─────────────────────────────────────────────────────┘ │ │ ┌─────────────────────────────────────────────────────┐ │ │ │ Lambda #2: Storm Data Crawler (New) │ │ │ │ - Trigger: EventBridge (Weekly) │ │ │ │ - Function: Crawl IBTrACS data │ │ │ │ - Output: recent_storms.json → S3 │ │ │ └─────────────────────────────────────────────────────┘ │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ EventBridge │ │ - Rule: storm-data-crawler-weekly-trigger │ │ - Schedule: Every Sunday 00:00 UTC (7AM Vietnam) │ │ - Target: Lambda #2 (Storm Data Crawler) │ └─────────────────────────────────────────────────────────────┘ Frontend Directory Structure frontend/ ├── src/ │ ├── components/ # React components │ │ ├── ui/ # shadcn/ui components (button, card, input, etc.) │ │ ├── storm/ # Storm-specific components │ │ ├── timeline/ # Timeline controls │ │ ├── wind/ # Wind visualization │ │ ├── StormPredictionForm.tsx # Storm coordinate input form │ │ ├── WeatherMap.tsx # Main Leaflet map │ │ ├── StormTracker.tsx # Storm list │ │ ├── StormInfo.tsx # Storm details │ │ ├── StormAnimation.tsx # Animated markers │ │ ├── WeatherOverlay.tsx # Temperature/Wind overlay │ │ ├── WeatherLayerControl.tsx # Satellite/Radar layers │ │ ├── WeatherLayerControlPanel.tsx # Control panel UI │ │ ├── WeatherValueTooltip.tsx # Hover tooltip │ │ ├── WindyLayer.tsx # Windy.com integration │ │ ├── ProvinceLayer.tsx # Vietnam provinces │ │ ├── OptimizedTemperatureLayer.tsx │ │ ├── TemperatureHeatMapLayer.tsx │ │ ├── ThemeToggle.tsx # Dark/Light mode │ │ ├── PreferencesModal.tsx # User preferences │ │ ├── RightSidebar.tsx # Right panel │ │ └── WeeklyForecast.tsx # 7-day forecast │ │ │ ├── pages/ │ │ ├── Index.tsx # Main page │ │ └── NotFound.tsx # 404 page │ │ │ ├── lib/ # Business logic \u0026amp; utilities │ │ ├── api/ # API clients │ │ ├── __tests__/ # Unit tests │ │ ├── stormData.ts # Types \u0026amp; interfaces │ │ ├── stormAnimations.ts # Animation logic │ │ ├── stormIntensityChanges.ts │ │ ├── stormPerformance.ts │ │ ├── stormValidation.ts │ │ ├── windData.ts │ │ ├── windStrengthCalculations.ts │ │ ├── windyStatePersistence.ts │ │ ├── windyUrlState.ts │ │ ├── mapUtils.ts # Map helpers │ │ ├── openWeatherMapClient.ts │ │ ├── dataWorker.ts # Web Worker │ │ ├── utils.ts │ │ └── colorInterpolation.ts │ │ │ ├── hooks/ # Custom React hooks │ │ ├── use-toast.ts │ │ ├── use-theme.tsx │ │ ├── use-mobile.tsx │ │ ├── useTimelineState.ts │ │ ├── useWindyStateSync.ts │ │ └── useSimplifiedTooltip.ts │ │ │ ├── contexts/ # React Context │ │ └── WindyStateContext.tsx │ │ │ ├── api/ │ │ └── weatherApi.ts # API calls │ │ │ ├── utils/ │ │ └── colorInterpolation.ts │ │ │ ├── styles/ │ │ └── accessibility.css # WCAG compliance styles │ │ │ ├── test/ # Test suite │ │ ├── accessibility.test.ts │ │ ├── accessibility-audit.test.ts │ │ ├── wcag-compliance.test.ts │ │ ├── performance.test.ts │ │ ├── cross-browser.test.ts │ │ └── setup.ts │ │ │ ├── assets/ # Images, icons │ ├── App.tsx │ ├── main.tsx │ └── index.css │ ├── public/ # Static assets ├── dist/ # Build output (after npm run build) ├── .env.production # Production config ├── .env.example ├── package.json ├── vite.config.ts ├── vitest.config.ts # Test config ├── tailwind.config.ts ├── tsconfig.json └── components.json # shadcn/ui config Environment Variables .env.production # OpenWeather API VITE_OPENWEATHER_API_KEY=8ff7f009d2bd420c86845c6bcf6de4a9 # CloudFront URL - Fetch storm data VITE_CLOUDFRONT_URL=https://d3lj47ilp0fgxy.cloudfront.net # Lambda Function URL - Storm prediction API VITE_PREDICTION_API_URL=https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws ** Screenshots needed:**\nAWS CloudFront → Distributions → Distribution domain name AWS Lambda → storm-prediction → Function URL Build \u0026amp; Deploy Process 1. Build Production cd frontend npm run build Output: dist/ folder contains:\nindex.html assets/index-[hash].js assets/index-[hash].css 2. Upload to S3 aws s3 sync dist/ s3://storm-frontend-hosting-duc-2025/ --delete Important Notes:\nS3 bucket is PRIVATE (no public access) CloudFront uses REST API endpoint, not website endpoint Origin: storm-frontend-hosting-duc-2025.s3.ap-southeast-1.amazonaws.com\n3. Invalidate CloudFront Cache aws cloudfront create-invalidation \\ --distribution-id E1234567890ABC \\ --paths \u0026#34;/*\u0026#34; Data Flow A. Load Storm Data (Startup) Browser → CloudFront → S3 ↓ GET /recent_storms.json ↓ Parse JSON → Display on map File: src/pages/Index.tsx (line ~40)\nconst CLOUDFRONT_URL = import.meta.env.VITE_CLOUDFRONT_URL; const FETCH_URL = `${CLOUDFRONT_URL}/recent_storms.json?t=${Date.now()}`; B. Storm Prediction (User Action) User fills form → Click \u0026#34;Run Prediction\u0026#34; ↓ POST /predict to Lambda Function URL ↓ { \u0026#34;history\u0026#34;: [{lat, lng}, ...], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } ↓ Lambda processes → Returns forecast ↓ Display predicted path on map File: src/components/StormPredictionForm.tsx (line ~80)\nconst API_URL = `${import.meta.env.VITE_PREDICTION_API_URL}/predict`; const response = await fetch(API_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ history, storm_name }) }); Key Components 1. Core Components StormPredictionForm File: src/components/StormPredictionForm.tsx\nFeatures:\nForm for inputting storm coordinates (min 9 points) Input validation (valid lat/lng) Calls Lambda API for prediction Displays results on map Scrollable list with Add/Remove positions Props:\ninterface StormPredictionFormProps { onPredictionResult: (result: PredictionResult) =\u0026gt; void; setIsLoading: (isLoading: boolean) =\u0026gt; void; } WeatherMap File: src/components/WeatherMap.tsx\nFeatures:\nDisplays Leaflet map Renders storm tracks (historical + forecast) Renders prediction path (purple, dashed) Weather overlays (temperature, wind, radar) Multiple storm rendering Auto-zoom to selected storm Custom panes for z-index layering Props:\ninterface WeatherMapProps { storms: Storm[]; selectedStorm?: Storm; customPrediction?: PredictionResult | null; mapFocusBounds?: LatLngBounds | null; onMapFocusComplete?: () =\u0026gt; void; } Index (Main Page) File: src/pages/Index.tsx\nFeatures:\nMain layout with header/footer State management (storms, selectedStorm, customPrediction) Sidebar with tabs (Current Storms / Predict Storm) Timeline state synchronization Loading \u0026amp; error handling Skip links for accessibility 2. Storm Components StormTracker File: src/components/StormTracker.tsx\nList of current storms Filter by status (active/developing/dissipated) Click to select storm StormInfo File: src/components/StormInfo.tsx\nDetailed storm information Wind speed, pressure, category Historical data Forecast timeline StormAnimation File: src/components/StormAnimation.tsx\nAnimated markers for storm positions Pulsing effect Category-based colors 3. Weather Layer Components WeatherOverlay File: src/components/WeatherOverlay.tsx\nTemperature heatmap overlay Wind speed visualization Real-time data from OpenWeather API Hover to view values WeatherLayerControl File: src/components/WeatherLayerControl.tsx\nSatellite imagery layer Radar layer Temperature layer Tile layer management WeatherLayerControlPanel File: src/components/WeatherLayerControlPanel.tsx\nUI controls for weather layers Opacity slider Layer toggle buttons Temperature animation toggle OptimizedTemperatureLayer \u0026amp; TemperatureHeatMapLayer Files: src/components/OptimizedTemperatureLayer.tsx, TemperatureHeatMapLayer.tsx\nPerformance-optimized temperature rendering Color interpolation Grid-based heatmap 4. Wind Components WindyLayer File: src/components/WindyLayer.tsx\nWindy.com iframe integration Wind animation overlay Synchronized state with main map Context: src/contexts/WindyStateContext.tsx\nGlobal state for Windy layer URL state persistence Sync across components 5. Map Enhancement Components ProvinceLayer File: src/components/ProvinceLayer.tsx\nVietnam provinces boundaries GeoJSON rendering Province labels WeatherValueTooltip File: src/components/WeatherValueTooltip.tsx\nTooltip displaying weather values on hover Temperature, wind speed, pressure Positioned tooltip 6. UI Components ThemeToggle File: src/components/ThemeToggle.tsx\nDark/Light mode switch Persisted preference System theme detection PreferencesModal File: src/components/PreferencesModal.tsx\nUser preferences settings Map options Display preferences RightSidebar File: src/components/RightSidebar.tsx\nAdditional info panel Collapsible sidebar WeeklyForecast File: src/components/WeeklyForecast.tsx\n7-day weather forecast Temperature trends Weather icons 7. Timeline Components Folder: src/components/timeline/\nTimeline controls for storm animation Play/Pause functionality Time scrubbing Speed controls Data Types PredictionResult File: src/lib/stormData.ts\nexport interface PredictionResult { storm_id: string; storm_name: string; prediction_time: string; totalDistance: number; // km actualDistance: number; // km lifespan: number; // hours forecastHours: number; // hours forecast: StormPoint[]; // Predicted positions path?: StormPoint[]; // Legacy support } StormPoint export interface StormPoint { timestamp: number; // Unix timestamp (ms) lat: number; lng: number; windSpeed: number; // km/h pressure: number; // hPa category: string; // \u0026#34;Typhoon\u0026#34;, \u0026#34;Super Typhoon\u0026#34;, etc. } AWS Permissions Required S3 Bucket Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34; } ] } CloudFront Origin Access Origin: S3 bucket Origin Access: Public (or OAI if used) Testing Local Development npm run dev # Open http://localhost:5173 Production Build Test npm run build npm run preview # Open http://localhost:4173 📸 Screenshots needed:\nBrowser DevTools → Network tab → API calls Browser DevTools → Console → No errors Common Issues 1. CORS Error when calling Lambda Symptom: Access-Control-Allow-Origin error\nSolution: Lambda must return CORS headers:\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } 2. CloudFront stale cache Symptom: New code not showing\nSolution: Invalidate cache\naws cloudfront create-invalidation --distribution-id E... --paths \u0026#34;/*\u0026#34; 3. Environment variables not loading Symptom: undefined when accessing import.meta.env.VITE_*\nSolution:\nEnsure .env.production file exists Rebuild: npm run build Variables must start with VITE_ Deployment Checklist Update .env.production with correct URLs npm run build succeeds Upload dist/ to S3 Invalidate CloudFront cache Test on production URL Verify Lambda API works Verify storm data loads Test prediction form with 9+ positions API Endpoints 1. Get Storm Data GET https://d3lj47ilp0fgxy.cloudfront.net/recent_storms.json Response: Array of Storm objects\n2. Predict Storm Path POST https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict Body: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Response: { \u0026#34;storm_id\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;totalDistance\u0026#34;: 500.5, \u0026#34;lifespan\u0026#34;: 72, \u0026#34;forecast\u0026#34;: [...] } Performance Optimization 1. Code Optimization Code Splitting: Vite automatically splits chunks by routes Tree Shaking: Remove unused code Minification: Production build auto-minifies JS/CSS Lazy Loading: Components load on demand 2. Data Optimization Web Workers: Heavy computations run in worker (dataWorker.ts) Memoization: React.memo for expensive components Debouncing: Input handlers are debounced Caching: LocalStorage cache for preferences 3. Rendering Optimization Virtual Scrolling: Large lists use virtual scrolling Optimized Layers: OptimizedTemperatureLayer for performance Canvas Rendering: Heatmap uses canvas instead of DOM Pane Management: Custom Leaflet panes for z-index optimization 4. Network Optimization CDN Caching: CloudFront caches static assets Image Optimization: WebP format, lazy loading API Caching: Cache storm data with timestamp Compression: Gzip/Brotli compression 5. Accessibility Performance Skip Links: Keyboard navigation shortcuts ARIA Labels: Proper semantic HTML Focus Management: Logical tab order Screen Reader: Optimized for screen readers Libraries \u0026amp; Utilities Business Logic (lib/) Storm Management stormData.ts: Types, interfaces, Storm/StormPoint definitions stormAnimations.ts: Animation logic for storm markers stormIntensityChanges.ts: Storm intensity change calculations stormPerformance.ts: Performance optimization for rendering stormValidation.ts: Storm data validation Wind System windData.ts: Wind data structures windStrengthCalculations.ts: Wind strength calculations windyStatePersistence.ts: Windy layer state persistence windyUrlState.ts: URL state management for Windy Map \u0026amp; Weather mapUtils.ts: Map helpers (center, zoom, bounds calculations) openWeatherMapClient.ts: OpenWeather API client colorInterpolation.ts: Color gradient calculations Performance dataWorker.ts: Web Worker for heavy computations utils.ts: General utilities Custom Hooks (hooks/) use-toast.ts: Toast notification system use-theme.tsx: Dark/Light theme management use-mobile.tsx: Mobile device detection useTimelineState.ts: Timeline state synchronization useWindyStateSync.ts: Windy layer state sync useSimplifiedTooltip.ts: Simplified tooltip logic Context (contexts/) WindyStateContext.tsx: Global state for Windy layer integration Testing (test/) accessibility.test.ts: Accessibility testing accessibility-audit.test.ts: WCAG audit wcag-compliance.test.ts: WCAG 2.1 compliance performance.test.ts: Performance benchmarks cross-browser.test.ts: Cross-browser compatibility setup.ts: Test environment setup Dependencies Core React 18 TypeScript Vite (build tool) Vitest (testing) UI Framework Tailwind CSS shadcn/ui (component library) Lucide Icons Radix UI (primitives) Map \u0026amp; Visualization Leaflet React-Leaflet GeoJSON support API \u0026amp; Data Fetch API (native) OpenWeather API AWS Lambda Function URL State Management React Context API URL state (query params) LocalStorage persistence Performance Web Workers Code splitting (Vite) Lazy loading Screenshots Cloudfront Distribution Figure 1 Origin Settings Figure 1 Invalidations Figure 2 storm-frontend-hosting-duc-2025 Figure 3 Permissions Figure 4 storm-ai-models-2025 Figure 5 storm-data-store-2025 Figure 6 Main Page Figure 7 Storm Tracking Features Figure 8 Storm Details Figure 9 Predict Feature Figure 10 Figure 11 Figure 12 "},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/","title":"Lambda Architecture","tags":[],"description":"","content":"Lambda Architecture - Storm Prediction AI Service Overview Lambda functions are an important component of a serverless architecture. They are especially useful due to their cost-effectiveness and ease of deployment—both of which are valuable for our hurricane prediction platform.\nThis section presents the details of how we designed and built our Lambda architecture.\nOur Lambda functions run PyTorch models for typhoon trajectory prediction and are deployed using a Docker container image.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐ │ Frontend (Browser) │ └────────────────────────┬────────────────────────────────────┘ │ POST /predict ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function URL (Public) │ │ URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi... │ │ Auth: NONE │ │ Method: POST │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function │ │ Name: storm-prediction │ │ Runtime: Python 3.10 (Container) │ │ Memory: 3008 MB │ │ Timeout: 120 seconds │ │ Architecture: x86_64 │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ ECR Repository │ │ Account: 339570693867 │ │ Region: ap-southeast-1 │ │ Repo: storm-prediction │ │ Image: latest │ │ Size: ~2 GB │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ S3 Buckets │ │ 1. storm-frontend-hosting-duc-2025 │ │ - models/lstm_totald_256_4.pt (optional) │ │ - predictions/[storm_id]_[timestamp].json │ │ │ │ 2. storm-ai-models (recommended) │ │ - models/lstm_totald_256_4.pt │ │ - models/tcn_model.pth (backup) │ └─────────────────────────────────────────────────────────────┘ storm_prediction/ Directory Structure storm_prediction/ ├── app.py # Lambda handler (main code) ├── Dockerfile # Container definition ├── requirements.txt # Python dependencies ├── cropping_storm_7304_2l.pth # TCN model (included in image) │ ├── DEPLOY_NOW.md # Quick deploy guide ├── DEPLOY_CONSOLE_STEP_BY_STEP.md # AWS Console guide ├── LAMBDA_DEPLOYMENT_GUIDE.md # Detailed deployment ├── AWS_CONSOLE_DEPLOYMENT_GUIDE.md ├── FIX_ECR_PUSH_ERROR.md # Troubleshooting ├── FIX_UNICODE_ERROR.md # UnicodeDecodeError fix ├── FIX_UNICODE_ERROR_SOLUTION.md # Solution details └── REBUILD_AND_DEPLOY.sh # Automated script Docker Image Structure Dockerfile FROM public.ecr.aws/lambda/python:3.10 # Install dependencies COPY requirements.txt . RUN pip3 install -r requirements.txt \\ --target \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; \\ --extra-index-url https://download.pytorch.org/whl/cpu # Copy Lambda handler COPY app.py ${LAMBDA_TASK_ROOT} # Copy TCN model to subdirectory (avoid .pth confusion) RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ # Set handler CMD [ \u0026#34;app.handler\u0026#34; ] Image Layers Layer 1: AWS Lambda Python 3.10 base (~500 MB) Layer 2: PyTorch CPU + dependencies (~1.2 GB) Layer 3: app.py + TCN model (~300 MB) ───────────────────────────────────────────── Total: ~2 GB AI Models 1. TCN Model (Trajectory Prediction) File: cropping_storm_7304_2l.pth Location: Inside Docker image at /var/task/models/ Size: ~300 MB Purpose: Predict next step (lat, lng) of typhoon trajectory\nArchitecture:\nclass StormTCN(nn.Module): def __init__(self, input_dim=4, hidden_units=1024, num_layers=2): self.tcn = TCN(...) self.head_latlon = nn.Linear(hidden_units, 2) # Predict lat, lng self.head_aux = nn.Linear(hidden_units, 2) # Predict aux features Input: [batch, sequence, 4] - (lat, lng, distance, bearing) Output:\npred_latlon: Next (lat, lng) pred_aux: Auxiliary features 2. LSTM Model (Total Distance Prediction) File: lstm_totald_256_4.pt Location: S3 bucket (downloaded on first use) Size: ~50 MB Purpose: Predict total distance typhoon will travel\nArchitecture:\nclass StormLSTM(nn.Module): def __init__(self, input_size=4, hidden_size=256, num_layers=2): self.lstm = nn.LSTM(...) self.fc = nn.Sequential( nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Linear(hidden_size // 2, 1) # Predict total distance ) Input: Daily summary [batch, days, 4] - (day, daily_dist, avg_speed, motion_type) Output: Total distance (km)\nRequest Flow 1. Receive Request POST /predict Content-Type: application/json { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... // Min 9 points ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;storm_id\u0026#34;: \u0026#34;TEST001\u0026#34; // Optional } 2. Load Models (First Invocation Only) def load_models(): global LSTM_MODEL, TCN_MODEL # Load LSTM from S3 (if available) if not os.path.exists(\u0026#39;/tmp/lstm_model.pt\u0026#39;): s3_client.download_file( MODEL_BUCKET, \u0026#39;models/lstm_totald_256_4.pt\u0026#39;, \u0026#39;/tmp/lstm_model.pt\u0026#39; ) LSTM_MODEL = StormLSTM(...) LSTM_MODEL.load_state_dict(torch.load(\u0026#39;/tmp/lstm_model.pt\u0026#39;)) # Load TCN from local (already in image) TCN_MODEL = StormTCN(...) TCN_MODEL.load_state_dict( torch.load(\u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;) ) 3. Preprocess Input def preprocess_history(history): # Convert to tensor [1, sequence_length, 4] # Features: [lat, lng, distance, bearing] processed = [] for i in range(len(history)): if i == 0: processed.append([lat, lng, 0.0, 0.0]) else: dist = haversine(prev_lat, prev_lng, lat, lng) brng = bearing(prev_lat, prev_lng, lat, lng) processed.append([lat, lng, dist, brng]) return torch.tensor(processed).unsqueeze(0) 4. Predict Total Distance (LSTM) def predict_total_distance(record_tensor): if LSTM_MODEL is None: # Fallback: avg_distance * 24 steps return fallback_distance # Group by day (9 points/day) # Run LSTM prediction with torch.no_grad(): pred = LSTM_MODEL(summary_tensor, lengths) return pred.item() # km 5. Predict Path (TCN) def predict_storm_path(record_tensor, total_distance, history): seq = record_tensor.clone() gone_distance = 0 predicted_points = [] while gone_distance \u0026lt; total_distance: # Predict next position pred_latlon, pred_aux = TCN_MODEL(seq) new_lat = pred_latlon[0, -1, 0].item() new_lng = pred_latlon[0, -1, 1].item() # Calculate distance \u0026amp; bearing step_distance = haversine(last_lat, last_lng, new_lat, new_lng) # Estimate windspeed (decay over time) estimated_wind = max(avg_wind * (0.98 ** step), 30) predicted_points.append({ \u0026#39;lat\u0026#39;: new_lat, \u0026#39;lng\u0026#39;: new_lng, \u0026#39;timestamp\u0026#39;: base_timestamp + (step * 3 * 3600 * 1000), \u0026#39;windSpeed\u0026#39;: estimated_wind, \u0026#39;pressure\u0026#39;: 980.0, \u0026#39;category\u0026#39;: calculate_category(estimated_wind) }) # Update sequence (sliding window) seq = torch.cat([seq[:, 1:, :], next_point.unsqueeze(1)], dim=1) gone_distance += step_distance step += 1 return predicted_points 6. Return Response result = { \u0026#39;storm_id\u0026#39;: storm_id, \u0026#39;storm_name\u0026#39;: storm_name, \u0026#39;prediction_time\u0026#39;: datetime.now().isoformat(), \u0026#39;totalDistance\u0026#39;: 500.5, \u0026#39;actualDistance\u0026#39;: 520.3, \u0026#39;lifespan\u0026#39;: 72, \u0026#39;forecastHours\u0026#39;: 72, \u0026#39;forecast\u0026#39;: [ { \u0026#39;lat\u0026#39;: 15.1, \u0026#39;lng\u0026#39;: 106.99, \u0026#39;timestamp\u0026#39;: 1765015351626, \u0026#39;windSpeed\u0026#39;: 65, \u0026#39;pressure\u0026#39;: 980, \u0026#39;category\u0026#39;: \u0026#39;Typhoon\u0026#39; }, ... ] } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } Build \u0026amp; Deploy Process Step 1: Build Docker Image cd storm_prediction docker build \\ --provenance=false \\ --platform linux/amd64 \\ -t storm-prediction-model . Flags:\n--provenance=false: Reduce image size (no build metadata) --platform linux/amd64: Lambda only supports x86_64 -t storm-prediction-model: Tag name Step 2: Tag for ECR docker tag storm-prediction-model:latest \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Time: ~5-10 minutes (2GB upload)\nStep 5: Update Lambda Function AWS Console:\nLambda → storm-prediction Tab Image → Deploy new image Select latest image Click Save Lambda Configuration Function Settings Name: storm-prediction Runtime: Container image Architecture: x86_64 Memory: 3008 MB Timeout: 120 seconds Ephemeral storage: 512 MB Environment Variables MODEL_BUCKET=storm-frontend-hosting-duc-2025 DATA_BUCKET=storm-frontend-hosting-duc-2025 Function URL URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws Auth type: NONE CORS: Enabled - Allow origins: * - Allow methods: POST, OPTIONS - Allow headers: Content-Type IAM Role Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34;, \u0026#34;arn:aws:s3:::storm-ai-models/*\u0026#34; ] } ] } Monitoring \u0026amp; Logs CloudWatch Logs Log Group: /aws/lambda/storm-prediction\nKey Log Messages:\nLoading LSTM model... Downloaded LSTM from S3 LSTM loaded successfully Loading TCN model... Checking: /var/task/models/cropping_storm_7304_2l.pth Found TCN at /var/task/models/cropping_storm_7304_2l.pth TCN loaded successfully Processing: Test Storm (TEST001) Input points: 9 Predicted total distance: 500.50 km Generated 24 predictions (72 hours) Saved to S3: predictions/TEST001_1733486400.json Screenshots needed:\nCloudWatch → Log groups → /aws/lambda/storm-prediction Log stream → Recent logs with emojis Logs → Duration, Memory used Metrics CloudWatch Metrics:\nInvocations Duration (avg ~5-10 seconds) Errors Throttles Memory used (~500-800 MB) Screenshots needed:\nLambda → Monitor → Metrics CloudWatch → Metrics → Lambda → Function metrics Common Issues \u0026amp; Solutions 1. UnicodeDecodeError: \u0026lsquo;utf-8\u0026rsquo; codec can\u0026rsquo;t decode byte 0x80 Symptom:\nUnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64 Cause: Model .pth file at root mistaken as Python config file\nSolution: Move model to subdirectory\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. 502 Bad Gateway Symptom: Frontend receives 502 error\nCauses:\nLambda timeout (exceeds 120s) Lambda crash (out of memory) Model failed to load Solutions:\nCheck CloudWatch Logs Increase memory if needed Increase timeout if needed 3. LSTM Fallback Symptom: Log shows \u0026ldquo;⚠️ Using fallback distance\u0026rdquo;\nCause: LSTM model not on S3\nSolution: Upload lstm_totald_256_4.pt to S3:\naws s3 cp lstm_totald_256_4.pt \\ s3://storm-frontend-hosting-duc-2025/models/ 4. ECR Push 403 Forbidden Symptom: 403 Forbidden when pushing image\nCauses:\nECR login expired Wrong account ID Repository doesn\u0026rsquo;t exist Solutions:\n# Re-login aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com # Create repository if needed aws ecr create-repository \\ --repository-name storm-prediction \\ --region ap-southeast-1 Testing Local Test (if possible) # Run locally python app.py # Test event test_event = { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } result = handler(test_event, None) print(result) Lambda Test AWS Console:\nLambda → Test tab Create test event: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Click Test Check response cURL Test curl -X POST \\ \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Deployment Checklist Model file cropping_storm_7304_2l.pth exists (Optional) Upload LSTM model to S3 Build Docker image successfully Tag image with correct account ID (339570693867) Login to ECR successfully Push image to ECR Update Lambda function with new image Check Lambda configuration (memory, timeout) Test Lambda with test event Test via Function URL with cURL Test from frontend Check CloudWatch Logs Verify prediction results on map Screenshots Function Figure 1 Configuration Figure 2 Environment Variables Figure 3 ECR Repository Figure 4 Figure 5 "},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/","title":"Lamda-Deployment","tags":[],"description":"","content":"Step to deploy our PyTorch Model on storm prediction on AWS Lambda AWS Lambda deployment plays a critical role in our website development pipeline. In this section, we document the process we followed to successfully complete the deployment.\nStep 1: Prepare the Code 1.1. Update app.py import json import torch import numpy as np from typing import List, Dict MODEL_PATH = \u0026#34;model.pth\u0026#34; device = torch.device(\u0026#34;cpu\u0026#34;) model = None def load_model(): global model if model is None: print(f\u0026#34;Loading model from {MODEL_PATH}...\u0026#34;) model = torch.load(MODEL_PATH, map_location=device) model.eval() print(\u0026#34;Model loaded successfully!\u0026#34;) return model def prepare_features(history: List[Dict]) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Convert history into a tensor for the model history: [{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 107.0}, ...] \u0026#34;\u0026#34;\u0026#34; # TODO: Implement feature engineering based on your model lats = [p[\u0026#34;lat\u0026#34;] for p in history] lngs = [p[\u0026#34;lng\u0026#34;] for p in history] features = np.array([lats + lngs]) # Shape: (1, 18) return torch.tensor(features, dtype=torch.float32) def format_predictions(predictions: torch.Tensor, storm_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Format output to match what the frontend expects \u0026#34;\u0026#34;\u0026#34; pred_array = predictions.detach().cpu().numpy()[0] forecast = [] base_timestamp = int(time.time() * 1000) for i in range(0, len(pred_array), 2): if i + 1 \u0026lt; len(pred_array): forecast.append({ \u0026#34;lat\u0026#34;: float(pred_array[i]), \u0026#34;lng\u0026#34;: float(pred_array[i + 1]), \u0026#34;timestamp\u0026#34;: base_timestamp + (i // 2) * 3600000, # +1 hour each \u0026#34;windSpeed\u0026#34;: 120.0, # TODO: Predict from model \u0026#34;pressure\u0026#34;: 980.0, # TODO: Predict from model \u0026#34;category\u0026#34;: \u0026#34;Category 3\u0026#34;, # TODO: Classify from windSpeed \u0026#34;confidence\u0026#34;: 0.85 }) return { \u0026#34;storm_name\u0026#34;: storm_name, \u0026#34;forecast\u0026#34;: forecast } def handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda handler function \u0026#34;\u0026#34;\u0026#34; try: # Parse input body = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) history = body.get(\u0026#39;history\u0026#39;, []) storm_name = body.get(\u0026#39;storm_name\u0026#39;, \u0026#39;Unknown Storm\u0026#39;) # Validate input if len(history) \u0026lt; 9: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: f\u0026#39;Need at least 9 positions, got {len(history)}\u0026#39; }) } # Load model model = load_model() # Prepare features X = prepare_features(history) # Predict with torch.no_grad(): predictions = model(X) # Format output result = format_predictions(predictions, storm_name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 1.2. Update Dockerfile FROM public.ecr.aws/lambda/python:3.11 # Copy requirements and install COPY requirements.txt ${LAMBDA_TASK_ROOT} RUN pip install --no-cache-dir -r requirements.txt # Copy model (rename to model.pth) COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/model.pth # Copy code COPY app.py ${LAMBDA_TASK_ROOT} # Set handler CMD [\u0026#34;app.handler\u0026#34;] 1.3. Verify requirements.txt torch==2.1.0 numpy==1.24.3 Step 2: Build the Docker Image cd storm_prediction # Build image docker build -t storm-prediction-model . # Test locally (optional) docker run -p 9000:8080 storm-prediction-model # Test with curl curl -X POST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \\ -d \u0026#39;{ \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;history\\\u0026#34;: [{\\\u0026#34;lat\\\u0026#34;: 15.0, \\\u0026#34;lng\\\u0026#34;: 107.0}, {\\\u0026#34;lat\\\u0026#34;: 15.1, \\\u0026#34;lng\\\u0026#34;: 107.1}, {\\\u0026#34;lat\\\u0026#34;: 15.2, \\\u0026#34;lng\\\u0026#34;: 107.2}, {\\\u0026#34;lat\\\u0026#34;: 15.3, \\\u0026#34;lng\\\u0026#34;: 107.3}, {\\\u0026#34;lat\\\u0026#34;: 15.4, \\\u0026#34;lng\\\u0026#34;: 107.4}, {\\\u0026#34;lat\\\u0026#34;: 15.5, \\\u0026#34;lng\\\u0026#34;: 107.5}, {\\\u0026#34;lat\\\u0026#34;: 15.6, \\\u0026#34;lng\\\u0026#34;: 107.6}, {\\\u0026#34;lat\\\u0026#34;: 15.7, \\\u0026#34;lng\\\u0026#34;: 107.7}, {\\\u0026#34;lat\\\u0026#34;: 15.8, \\\u0026#34;lng\\\u0026#34;: 107.8}], \\\u0026#34;storm_name\\\u0026#34;: \\\u0026#34;Test Storm\\\u0026#34;}\u0026#34; }\u0026#39; Step 3: Upload to AWS ECR # 1. Create ECR repository aws ecr create-repository \\ --repository-name storm-prediction-model \\ --region ap-southeast-1 # 2. Login Docker to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com # 3. Tag image docker tag storm-prediction-model:latest \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest # 4. Push image docker push \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest Step 4: Create the Lambda Function 4.1. Create Lambda from Console Open AWS Lambda Console Click “Create function” Choose “Container image” Function name: storm-prediction Container image URI: select the image you pushed to ECR Architecture: x86_64 Click “Create function” 4.2. Configure Lambda # Or use AWS CLI aws lambda create-function \\ --function-name storm-prediction \\ --package-type Image \\ --code ImageUri=\u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-execution-role \\ --timeout 60 \\ --memory-size 3008 \\ --region ap-southeast-1 Important configuration:\nMemory: 3008 MB (PyTorch models need RAM) Timeout: 60 seconds (inference can take 10–30s) Ephemeral storage: 512 MB (default; increase if needed) 4.3. Create IAM Role Lambda needs a role with permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 5: Create API Gateway # 1. Create REST API aws apigateway create-rest-api \\ --name storm-prediction-api \\ --region ap-southeast-1 # 2. Get API ID and Root Resource ID API_ID=\u0026lt;your-api-id\u0026gt; ROOT_ID=\u0026lt;your-root-resource-id\u0026gt; # 3. Create resource /predict aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part predict # 4. Create POST method RESOURCE_ID=\u0026lt;predict-resource-id\u0026gt; aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --authorization-type NONE # 5. Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:\u0026lt;account-id\u0026gt;:function:storm-prediction/invocations # 6. Deploy API aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name prod API URL: https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod/predict\nStep 6: Update the Frontend 6.1. Update .env.production VITE_PREDICTION_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod 6.2. Build \u0026amp; deploy frontend cd frontend npm run build # Deploy dist/ to S3/CloudFront Optimizations 1. Reduce Cold Start Provisioned Concurrency:\naws lambda put-provisioned-concurrency-config \\ --function-name storm-prediction \\ --provisioned-concurrent-executions 1 \\ --qualifier $LATEST 2. Reduce Image Size Use PyTorch CPU-only:\n# requirements.txt torch==2.1.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu numpy==1.24.3 Multi-stage build:\n# Stage 1: Build FROM python:3.11-slim as builder COPY requirements.txt . RUN pip install --target /packages -r requirements.txt # Stage 2: Runtime FROM public.ecr.aws/lambda/python:3.11 COPY --from=builder /packages ${LAMBDA_RUNTIME_DIR} COPY model.pth ${LAMBDA_TASK_ROOT}/ COPY app.py ${LAMBDA_TASK_ROOT}/ CMD [\u0026#34;app.handler\u0026#34;] 3. Cache the Model in /tmp import os MODEL_PATH = \u0026#34;/tmp/model.pth\u0026#34; if os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;) else \u0026#34;model.pth\u0026#34; def load_model(): global model if model is None: # Copy to /tmp for faster access if not os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;): import shutil shutil.copy(\u0026#34;model.pth\u0026#34;, \u0026#34;/tmp/model.pth\u0026#34;) model = torch.load(\u0026#34;/tmp/model.pth\u0026#34;, map_location=device) model.eval() return model Monitoring CloudWatch Logs `bash\nView logs aws logs tail /aws/lambda/storm-prediction \u0026ndash;follow `\nCloudWatch Metrics Invocations: number of calls Duration: runtime Errors: error count Throttles: throttled invocations Alerts # Create an alarm for errors aws cloudwatch put-metric-alarm \\ --alarm-name storm-prediction-errors \\ --alarm-description \u0026#34;Alert when Lambda has errors\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=storm-prediction Troubleshooting Error: \u0026ldquo;Task timed out after 3.00 seconds\u0026rdquo;\nFix: Increase timeout to 60s\nError: \u0026ldquo;Runtime exited with error: signal: killed\u0026rdquo;\nFix: Increase memory to 3008 MB\nError: \u0026ldquo;No module named \u0026rsquo;torch\u0026rsquo;\u0026rdquo;\nFix: Check requirements.txt and rebuild the image\nError: Model cannot be loaded\nFix: Verify the model filename in Dockerfile and app.py match\nEstimated Cost Lambda: Free tier: 1M requests/month, 400,000 GB-seconds After that: $0.20 per 1M requests + $0.0000166667 per GB-second Example: 10,000 requests/month, each request 10s, 3GB RAM Compute: 10,000 × 10s × 3GB × $0.0000166667 = ~$5/month Requests: 10,000 × $0.20/1M = ~$0.002/month Total: ~$5/month API Gateway: $3.50 per million requests 10,000 requests = ~$0.035/month ECR: $0.10 per GB/month storage Image ~2GB = ~$0.20/month Estimated total: ~$5.25/month for 10,000 predictions\nFinal Checklist Fix the model filename mismatch in app.py or Dockerfile Test the Docker image locally Push the image to ECR Create Lambda with 3008MB memory, 60s timeout Create API Gateway and integrate with Lambda Test the API with Postman/curl Update VITE_PREDICTION_API_URL in the frontend Build and deploy the frontend Test the prediction form on the web UI Set up CloudWatch alerts Monitor logs and performance "},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.5-event5/","title":"AWS Cloud Mastery Series #2","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Objectives Introduce DevOps principles and the cultural mindset behind modern software delivery Demonstrate how to build automated CI/CD pipelines on AWS Provide guidance on implementing Infrastructure as Code (IaC) Compare AWS container services for modern application deployment Present best practices for observability and monitoring Speakers Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Key Highlights Understanding the DevOps Mindset Collaboration between development and operations → Faster delivery Automation reduces manual work and increases consistency Continuous feedback loops → More stable, reliable systems CI/CD Pipeline on AWS A fully automated pipeline built across four stages:\nSource Control: CodeCommit for managing and versioning code Build \u0026amp; Test: CodeBuild to compile, test, and package applications Deployment: CodeDeploy implementing rolling, canary, and blue/green methods Orchestration: CodePipeline connecting all stages into a continuous workflow Live demos showed how code commits automatically triggered builds, tests, deployments, and rollbacks.\nInfrastructure as Code (IaC) A shift from manual setups to automated, version-controlled environments.\nAWS CloudFormation\nDeclarative YAML/JSON templates Resources, parameters, conditions, outputs Drift detection to maintain state consistency AWS CDK (Cloud Development Kit)\nDefine infrastructure in TypeScript, Python, Java, etc. L1/L2/L3 constructs for reusable patterns CLI to synthesize, diff, and deploy stacks Examples demonstrated identical architectures built using IaC instead of “ClickOps.”\nContainers on AWS Overview of Docker basics and AWS container compute choices:\nAmazon ECR: Secure container registry with vulnerability scanning Amazon ECS: AWS-native orchestration with EC2 or Fargate launch types Amazon EKS: Managed Kubernetes for standards-based workloads AWS App Runner: Simplified container deployment with minimal operations Comparisons highlighted when to choose App Runner vs ECS vs EKS based on team experience and workload patterns.\nObservability and Monitoring Building visibility for modern systems:\nAmazon CloudWatch Metrics, logs, alarms, dashboards AWS X-Ray Distributed tracing to identify bottlenecks and slow microservices Emphasis on actionable alerts, meaningful dashboards, and proactive monitoring practices.\nKey Takeaways DevOps Practices Automation → speed + reliability Cultural alignment between dev and ops Use DORA metrics to guide improvement Integrate feedback at every development stage Infrastructure as Code Minimize manual configuration in production CloudFormation for AWS-native declarative workflows CDK for flexible, programmatic infrastructure definitions Treat infrastructure like software: test, version, automate Application Delivery CI/CD pipelines reduce human error Choose deployment strategies based on risk (rolling, canary, blue/green) Integrate automated testing in every step Container Strategy Containers offer portability, consistency, modular architecture ECS → simple operational model EKS → Kubernetes flexibility App Runner → low-ops deployment ECR as the foundation for image management Observability Combine metrics, logs, traces for a 360° view CloudWatch dashboards + X-Ray service maps for troubleshooting Build proactive alerts, not reactive firefighting Applying to Work Automate CI/CD using CodePipeline, CodeBuild, and CodeDeploy to streamline releases Adopt IaC with CloudFormation or CDK to replace manual setups and ensure consistency Containerize services and select ECS, EKS, or App Runner based on workload needs Improve observability using CloudWatch metrics, logs, alarms, and custom dashboards Enable distributed tracing with AWS X-Ray for microservice troubleshooting Use DORA metrics to measure delivery performance and guide DevOps improvements Event Experience Attending the “Cloud Mastery Series #2 – DevOps on AWS” workshop provided both strategic direction and practical knowledge for implementing DevOps at scale.\nLearning from highly skilled speakers The speakers offered clear explanations of CI/CD, containers, IaC, and monitoring Real-world case studies illustrated how DevOps practices work in production environments Hands-on technical exposure Observed CI/CD pipelines running from commit → build → deployment Understood how CloudFormation and CDK help enforce repeatable infrastructure Clarified trade-offs between ECS, EKS, and App Runner for container orchestration Leveraging modern tools IaC tools enforce consistency and reduce configuration drift CloudWatch and X-Ray provide the foundation for operational excellence Networking and discussions Opportunities to connect with experts and peers Discussions reinforced the importance of culture, automation, and measurable improvement Lessons learned Automation is key to scaling safely Observability is required for reliability Choosing the right compute/container service leads to reduced operational burden Some event photos Figure 1 Figure 2 Figure 3 Overall, the workshop delivered a comprehensive and practical understanding of DevOps principles, CI/CD pipelines, IaC, container orchestration, and monitoring — all essential components of modern cloud-native development.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about the topics in the \u0026ldquo;Migrate to AWS\u0026rdquo; module Understand cost optimization with AWS Lambda Tasks to be carried out this week: Day Task Start Date Completion Date Reference Materials 2 - Continue researching for the project, draw a preliminary architecture diagram 6/10/2025 6/10/2025 3 - Practice VM Import/Export + Try deploying an application server + Upload and import VM to AWS 7/10/2025 7/10/2025 https://000014.awsstudygroup.com/ 4 - Practice Database Schema Conversion \u0026amp; Migration + Configure DMS and SCT + Perform schema conversion and data migration 8/10/2025 8/10/2025 https://000015.awsstudygroup.com/ 5 - Practice AWS Elastic Disaster Recovery Workshop + Configure DRS + Install agent and test failover process 9/10/2025 9/10/2025 https://000016.awsstudygroup.com/ 6 - Practice Optimizing EC2 Costs with Lambda + Create Lambda functions to start/stop instances + Connect Slack for notifications 10/10/2025 10/10/2025 https://000017.awsstudygroup.com/ Week 5 Achievements: A – Learn about VM Import/Export\nUnderstand the VM Import/Export process on AWS, including two-way migration between on-premises environments and AWS Cloud.\nSuccessfully deploy an Ubuntu virtual machine in a VMWare Workstation environment:\nCreate a new VM and install Ubuntu OS. Configure users, disk capacity, and OpenSSH Server for remote access. Export the VM from on-premises:\nUse the Export to OVF feature to generate .vmdk files for AWS import. Create and configure an S3 bucket to store VM files:\nChoose a unique bucket name and select an appropriate region. Enable public access and upload the .vmdk file to S3. Create an IAM Role “vmimport” and attach required policies so the VM Import/Export service can:\nAccess the S3 bucket containing the VM files. Run import-image on AWS CLI to convert the VM into an AMI. Deploy an EC2 Instance from the AMI:\nAccess EC2 Console and select the imported AMI. Create a key pair, launch the instance, and test SSH connectivity. Practice the export process from AWS:\nCreate a new S3 bucket and configure ACL to allow read/write access. Use AWS CLI to export the Instance or AMI into \u0026ldquo;vhd / .vmdk\u0026rdquo; format. Store the files in S3 and redeploy the VM on the on-premises system. Master the two-way workflow:\nImport: On-premises → S3 → AMI → EC2 Export: EC2 / AMI → S3 → On-premises Complete all steps for CLI connection, AMI verification, import/export progress checking, and AWS resource management.\nB – Overview of Database Schema Conversion \u0026amp; Migration\nUnderstand how to prepare AWS environment for database migration, including creating key pairs, configuring EC2 instances, and setting up connectivity.\nLearn how to select and configure different database sources such as Oracle and SQL Server for AWS DMS.\nPractice schema conversion using AWS Schema Conversion Tool (SCT) and learn how to modify procedural code when necessary.\nLearn to set up database migration using AWS Database Migration Service (DMS), including creating replication instances, endpoints, and migration tasks.\nMonitor migration activities via CloudWatch metrics, task logs, and event notifications.\nUnderstand how DMS Serverless works and how it automatically scales under load.\nPractice troubleshooting common migration issues such as memory errors or table errors in DMS tasks.\nComplete environment cleanup after migration, including deleting migration tasks, replication instances, and IAM roles.\nC – AWS Elastic Disaster Recovery Workshop\nUnderstand the overview of AWS Elastic Disaster Recovery (AWS DRS), including its purpose, operation, and benefits in reducing downtime and data loss.\nLearn how to prepare infrastructure for a simulated on-premises environment for DRS practice, including subnet, DNS, and Bastion host setup.\nPractice connecting to Bastion Host via RDP or SSH using the provided credentials.\nConfigure default DRS settings in AWS Management Console, including subnet selection, instance type, and security group.\nCreate and configure an IAM User for the DRS Agent with access rights and save the Access Key for installation.\nPractice installing the DRS Agent on source servers (web and database) via Bastion Host, configuring AWS region, Access Key, Secret Key, and selecting disks to replicate.\nMonitor the initial data sync and verify that DRS source servers are “Ready for recovery” and “Healthy.”\nPerform the Failover process, including starting recovery jobs, selecting the latest data, and checking recovery progress in both DRS console and EC2.\nLearn to check logs, monitor job history, and confirm that instances are successfully launched in AWS environment.\nComplete familiarization with disaster recovery workflow to ensure system availability.\nD – Optimizing EC2 Costs with Lambda\nUnderstand the objectives and principles of EC2 cost optimization using AWS Lambda, including automatically starting/stopping instances and using Savings Plans for continuous operation scenarios.\nPractice creating the initial infrastructure, including VPC, Security Group, and EC2 instances for the lab.\nConfigure Slack Incoming Webhooks to receive instance status notifications directly on a Slack channel.\nTag the EC2 instance in the console (key: environment_auto, value: true) so Lambda can identify and manage it.\nCreate IAM Role for Lambda Function with AmazonEC2FullAccess and CloudWatchFullAccess permissions to allow EC2 management and logging.\nPractice creating two Lambda functions:\nstop instance – automatically stop EC2 when not needed. start instance – automatically start EC2 when required. Test the functions in AWS Management Console:\nRunning the start instance function launches the EC2 instance and Slack shows “Starting instance.” Running the stop instance function stops the EC2 instance and Slack shows “Stopping instance.” Complete the full test workflow and verify Lambda functions operate correctly, optimizing EC2 operational costs.\nPerform resource cleanup after testing.\n"},{"uri":"https://giaphazzz.github.io/aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Overview Hurricanes are powerful natural disasters that cause severe damage to infrastructure and pose significant risks to human life. Early detection and timely warnings are essential so that people in affected areas have enough time to prepare and evacuate safely.\nTo address this need, our project aims to build an online platform that allows users to freely access information about the most recent storms in the Western Pacific, using data sourced from the trusted NOAA (National Oceanic and Atmospheric Administration). In addition, students, meteorologists, or anyone interested in hurricane dynamics can interact with our system by providing their own input trajectories and receiving predictions generated by our machine learning model.\nThis workshop presents the complete process of building such a model for hurricane forecasting, including several novel time-series techniques—Stepwise Temporal Fading and Plausible Geodesic-Aware Augmentation—as well as a step-by-step explanation of how we built and deployed the platform from scratch.\nWith the support of AWS services such as Amazon S3, AWS Lambda, API Gateway, and CloudFront, we construct a fully serverless architecture. This offers simplicity, scalability, and long-term cost efficiency while ensuring reliable and responsive performance.\nPlatform Architecture The final platform delivers two core functionalities:\nStorm Viewing Users can explore up-to-date information on recent Western Pacific storms, including their historical path, wind speed, temperature, and other relevant parameters.\nHurricane Trajectory Prediction Users can input their own partial storm trajectory and receive a predicted future path generated by our trained model.\nContent Workshop overview Data Preparation ML Model Training Front\u0026amp;Back-End Architect API "},{"uri":"https://giaphazzz.github.io/aws/5-workshop/5.5-platform-api/","title":"Platform API","tags":[],"description":"","content":"BACK-END API DETAIL DESCRIPTION 1. Introduction Weather Backend API is a RESTful service that provides weather information by integrating with OpenWeatherMap API. The backend serves as a middleware layer between frontend applications and external weather data sources.\nFigure 1 2. System Architecture High-Level Architecture ┌─────────────────────────────────────────────────────────────┐ │ Frontend Applications │ │ (React, Mobile, Web Clients) │ └─────────────────────────────────────────────────────────────┘ │ │ HTTPS / REST API ▼ ┌─────────────────────────────────────────────────────────────┐ │ Weather Backend API │ │ (.NET 9.0 - ASP.NET Core) │ ├─────────────────────────────────────────────────────────────┤ │ ┌────────────────┐ ┌────────────────┐ ┌────────────────┐ │ │ Controllers │ │ Services │ │ Program.cs │ │ │ │ │ │ │ - App Startup │ │ │ - WeatherCtrl │ │ - WeatherSvc │ │ - Logging │ │ │ - ForecastCtrl │ │ - Cache Layer │ │ - DI Setup │ │ └────────────────┘ └────────────────┘ └────────────────┘ └─────────────────────────────────────────────────────────────┘ │ │ HTTPS / REST API (External) ▼ ┌─────────────────────────────────────────────────────────────┐ │ External Weather Services │ ├─────────────────────────────────────────────────────────────┤ │ • OpenWeatherMap API │ │ • Redis Caching Layer │ │ • Rate Limiting \u0026amp; Monitoring │ └─────────────────────────────────────────────────────────────┘ 3. Core Features Description: Retrieve current weather conditions for any city worldwide.\nFeatures:\nSearch by city name (e.g., \u0026ldquo;Hanoi\u0026rdquo;, \u0026ldquo;Ho Chi Minh City\u0026rdquo;) Optional country code for precise location Multiple unit systems support (metric, imperial, standard) Multi-language weather descriptions Cached responses for performance API Parameters:\ncityName (required): Name of the city countryCode (optional): ISO 3166 country code units (optional): metric, imperial, or standard language (optional): en, vi, fr, etc. Response Example:\nResponse body Download { \u0026#34;localDate\u0026#34;: \u0026#34;2025-12-06 19:57:02\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Hà Nội\u0026#34;, \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 105.8412, \u0026#34;lat\u0026#34;: 21.0245 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 804, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;mây đen u ám\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04n\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 22, \u0026#34;feels_like\u0026#34;: 22.11, \u0026#34;temp_min\u0026#34;: 22, \u0026#34;temp_max\u0026#34;: 22, \u0026#34;pressure\u0026#34;: 1018, \u0026#34;humidity\u0026#34;: 71, \u0026#34;sea_level\u0026#34;: 1018, \u0026#34;grnd_level\u0026#34;: 1017 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 4.14, \u0026#34;deg\u0026#34;: 136, \u0026#34;gust\u0026#34;: 6.84 }, \u0026#34;sys\u0026#34;: { \u0026#34;type\u0026#34;: 1, \u0026#34;id\u0026#34;: 9308, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1764976827, \u0026#34;sunset\u0026#34;: 1765016103 } } Figure 2 4. Technology Stack Backend Framework .NET 9.0 – Latest .NET runtime ASP.NET Core – Web API framework C# 12 – Primary programming language API Integration HttpClientFactory – Managed HTTP client usage Polly – Retry policies \u0026amp; transient fault handling Newtonsoft.Json / System.Text.Json – JSON serialization Caching \u0026amp; Performance MemoryCache – In-memory caching Redis (optional) – Distributed caching ResponseCompression – Gzip / Brotli compression Development Tools Visual Studio 2022 / VS Code – IDE / Code editor Swagger / OpenAPI – API documentation Git – Version control Docker – Containerization 5. Project Structure WeatherBackend/ │ ├── WeatherBackend.csproj # Project file ├── Program.cs # Application entry point ├── WeatherBackend.http # HTTP request testing file │ ├── appsettings.json # Configuration settings │ ├── Controllers/ # API Controllers │ └── WeatherController.cs # Main weather endpoints │ ├── Services/ # Business logic services │ └── WeatherService/ # Service contracts \u0026amp; implementation 6. API Endpoints Base URL https://localhost:7042/swagger/index.html 6.1 GET /api/weather/current Description: Retrieve the current weather by city name.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather?city=hanoi\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather?city=hanoi 6.2 GET /api/weather/forecast Description: Get the 5-day weather forecast for a selected city.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/forecast?city=hochiminh\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/forecast?city=hochiminh 6.3 GET /api/weather/coordinates Description: Retrieve weather data using latitude and longitude.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412 6.4 GET /api/weather/location Description: Retrieve weather data for the user\u0026rsquo;s current location (requires coordinates from client device).\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/global\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/global Figure 3 Last Updated: 2025-12-09\nVersion: 1.0.0\nMaintained by: SKYNET\n"},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.6-event6/","title":"AWS Edge Services Wokrshop","tags":[],"description":"","content":"Summary Report: “AWS Edge Services Workshop - Amazon CloudFront as Your Foundation” Event Objectives Understand how Amazon CloudFront enhances security, performance, and cost efficiency Learn best practices for edge caching, content delivery, and origin protection Explore CloudFront capabilities for static, dynamic, and large-scale content Gain practical insights into implementing edge logic and failover strategies Speakers Hung Nguyen Gia – Head of Solutions Architect, Edge Technical Field Community, Amazon Web Services Viet Nam Key Highlights Customer Challenges and Financial Solutions Unpredictable CDN Costs: Usage-based billing can lead to high bills during traffic spikes or attacks Solution: AWS Fixed-Price CDN + Security plans, providing predictable monthly costs and reducing financial risk Bundles: CloudFront, WAF, Shield AMR, Route 53, CloudWatch Logging, S3 storage credits Fixed monthly pricing regardless of usage CloudFront Capabilities Security and Protection\nHTTPS/TLS Support:\nReduce key size for higher performance and lower latency Use TLS library (s2n) to validate destinations TLS 1.3, ECDSA certificates, and post-quantum-ready options Mutual TLS (2-way): Support for client certificate authentication, upcoming CloudFront setup\nOrigin Cloaking:\nVPC Origin: hides origin from internet, using private ALB Origin Access Control (OAC): signed requests for S3, Lambda, MediaPackage, no API Gateway needed Custom Origins: restrict CloudFront IPs, add custom headers with pre-defined secrets Access Control: Geographic restrictions, Signed URLs with time-frame, URL path, and client IP validation\nCost Optimization\nAutomatic object compression (up to 10 GB, skip already compressed files) Free AWS data transfer from origins to CloudFront Offload origin workload: reduces CPU and load balancer costs Reliability and Resiliency\nCaching with TTL and stale content delivery Built-in failover to healthy secondary origins Graceful failure: deliver cached content or custom error pages Performance Enhancements\nMulti-layer caching: Regional Edge Caches + Origin Shield Request collapsing to reduce duplicate requests Persistent connections to origin to avoid extra TCP handshakes Edge logic execution with CloudFront Functions or Lambda@Edge Use Cases and Best Practices Use Cases:\nStatic web resources Whole website delivery (global performance, security, high availability) API acceleration (connection reuse, reduced latency) Media streaming and large file downloads (resumable, edge caching) Best Practices:\nEnd-to-end visibility: monitor user experience and request paths Maximum caching: normalize cache keys, select appropriate TTL, cache errors to prevent brute-force attacks Block unwanted requests: detect malicious patterns, implement rate limiting Offload business logic: move lightweight processing to edge, e.g., CloudFront Functions Automatic failover: configure Route 53 health checks or CloudFront origin groups Key Takeaways CloudFront is a comprehensive foundation for edge security, performance, and cost efficiency Edge caching and logic reduce origin load while improving response times Security features (TLS, mutual TLS, origin cloaking, signed URLs) protect content at the edge Following best practices ensures high availability, optimized caching, and resilience Applying to Work Enable compression and multi-layer caching for both static and dynamic content Implement Signed URLs and OAC for sensitive content Move lightweight business logic to CloudFront Functions for faster response Monitor edge metrics and logs to maintain visibility and detect unwanted traffic Configure failover to maintain high availability Event Experience Attending the AWS Edge Services Workshop was highly valuable, offering a deep dive into CloudFront’s security, performance, and operational capabilities. Key experiences included:\nLearning from an expert speaker Hung Nguyen Gia shared practical insights and real-world examples on edge optimization Gained an understanding of CloudFront’s full spectrum of features for global content delivery Hands-on technical exposure Learned origin cloaking and origin access control configurations Explored compression, caching strategies, and persistent connections Understood how edge logic reduces latency and improves scalability Leveraging best practices End-to-end visibility and error handling strategies Advanced caching configurations for maximum efficiency Techniques to block malicious requests and offload processing to the edge Some event photos Figure 1 Figure 2 Figure 3 Overall, the workshop reinforced the importance of security, performance, and reliability at the edge, and provided actionable strategies for implementing CloudFront in enterprise environments.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn more about Grafana, Tags, and AWS Systems Manager management Practice with FJC labs Tasks to be carried out this week: Day Task Start Date Completion Date Reference Materials 2 - Train the model for the next coordinate prediction problem, evaluate and comment on new integrations into the model 13/10/2025 13/10/2025 3 - Get familiar with Grafana and monitor AWS resources through a visual dashboard + Learn the concept, functions, and applications of Grafana in system monitoring + Install and configure Grafana on EC2 instance + Connect Grafana with CloudWatch and create a monitoring dashboard for EC2 resources 14/10/2025 14/10/2025 https://000029.awsstudygroup.com/ 4 - Learn about Tags and Resource Groups in AWS + Understand the key–value pair mechanism and the role of tagging in resource organization + Practice creating, deleting, and filtering resources by tag via AWS Console and CLI + Create a Tag-based Resource Group with the criteria Key=BusinessUnit, Value=Marketing 15/10/2025 15/10/2025 https://000027.awsstudygroup.com/ 5 - Manage EC2 access through IAM and Resource Tags + Apply the Least Privilege principle and create IAM Policies with Region and Tag conditions + Create admin group (Admin Group), user (Admin User), and role ec2-admin-team-alpha 16/10/2025 16/10/2025 https://000028.awsstudygroup.com/ 6 - Manage and automate multiple servers using AWS Systems Manager 17/10/2025 17/10/2025 https://000031.awsstudygroup.com/ Week 6 Achievements: A - Get familiar with Grafana and monitor AWS resources through a visual dashboard\nLearn about Grafana:\nUnderstand the concept, functions, and applications of Grafana in data visualization and system monitoring. Grasp key features such as visualization, dynamic dashboards, alerting, and integration with multiple data sources. Prepare the AWS environment:\nCreate VPC, Subnet, Security Group, IAM User, and IAM Role for Grafana installation. Launch an EC2 instance to install Grafana Server. Practice installing Grafana on EC2:\nConnect to the instance via PuTTY, update the system, and add the YUM repository. Install Grafana using the command sudo yum install grafana and start the service with systemctl start grafana-server. Check the service status and set it to start automatically with the system. Log in and perform the initial Grafana configuration:\nAccess the interface via :3000, log in using default credentials admin/admin. Reset the password and verify the basic dashboard interface. Connect Grafana with CloudWatch:\nConfigure CloudWatch as a data source using the Access Key and Secret Key of the IAM User. Verify a successful connection and query data. Create a resource monitoring dashboard:\nAdd a panel showing the CPUUtilization metric of the EC2 instance. Save the dashboard as “Grafana-Monitoring” and share it via link. Try the Explore feature to query and visualize data in real time. B - Learn about Tags and Resource Groups in AWS\nUnderstand the key–value pair concept of tags and the role of tagging in resource management, cost allocation, access control, and automation.\nPractice using Tags through the AWS Management Console:\nCreate an EC2 instance with tags during initialization. Add, delete, and filter resources by tag. Get familiar with using AWS CLI to manage tags:\nAssign tags to existing EC2 resources using the create-tags command. Create new EC2 instances and EBS volumes with tags using run-instances and create-volume. Use describe-instances to view tagged resource information. Create and manage Tag-based Resource Groups:\nBuild a Tag-based resource group with criteria Key=BusinessUnit, Value=Marketing. Check the resource list in the newly created group via the Resource Groups Console. Understand how to organize, group, and access AWS resources efficiently using tags and resource groups.\nC - Learn about EC2 resource access management through IAM and Resource Tags\nUnderstand the Least Privilege principle in IAM and how to apply conditions in IAM Policies to restrict access by region and tag.\nPractice creating an admin group (Admin Group) and admin user (Admin User) in IAM Console.\nCreate specialized IAM Policies for EC2 management, including:\nec2-list-read: allows read-only EC2 access in regions us-east-1 and us-west-1 ec2-create-tags: allows tagging when creating EC2 instances ec2-create-tags-existing: allows tagging if the tag has Key=Team, Value=Alpha ec2-run-instances: allows launching EC2 instances when region and tag conditions are met ec2-manage-instances: allows starting, stopping, and terminating EC2 instances when tag and region conditions are satisfied Create and attach IAM Role ec2-admin-team-alpha with the above policies, and configure the Trust relationship to allow Assume Role.\nPractice the Switch Role mechanism and verify user permissions according to each configured IAM policy.\nUnderstand how to combine IAM and Resource Tags to control access flexibly, securely, and scalably for distributed administration models.\nD - Learn and practice managing multiple servers simultaneously using AWS Systems Manager\nUnderstand the function of Systems Manager in centralizing operational data, automating processes, and synchronizing resource management on AWS.\nPractice deploying an environment with two Windows EC2 virtual machines:\nCreate VPC, subnet, and configure IAM Role to allow Systems Manager to access and manage instances. Attach IAM Role to each instance to connect with Systems Manager. Check Managed Nodes status in Fleet Manager, verify that two nodes (Windows-Lab-SSM-1 and Windows-Lab-SSM-2) are active and can successfully initiate terminal sessions.\nUse Patch Manager to scan and install updates for two Windows EC2 instances:\nSelect “Scan and Install” mode to check and install patches. Configure not to reboot instances after patching. Monitor and verify successful updates. Practice using Run Command on multiple servers simultaneously via AWS Systems Manager:\nExecute “AWS-RunPowerShellScript” to run remote PowerShell commands. Select both Windows EC2 instances as targets. Enable the option to save output results to an S3 Bucket for log and error checking. Complete the output verification process for each instance, confirm successful command execution, and validate correct system response.\nMaster centralized and automated multi-server management to optimize system maintenance, control access, and reduce manual operations in AWS environments.\n"},{"uri":"https://giaphazzz.github.io/aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"My internship journey at AMAZON WEB SERVICES VIETNAM CO., LTD from September 8, 2025 to December 24, 2025 has been a valuable experience. With the opportunity to meet many peers, practice, and use AWS convenient services, as well as expand my knowledge of cloud computing, I was extremely excited about these experiences. This was not only theoretical knowledge but also hands-on practice in a professional working environment.\nIn particular, I participated in the Online Platform for Hurricane Trajectory project with the SKYNET team. The project was created to provide an online platform for internet users to access and track the latest information on hurricanes in the West Pacific region. Moreover, it features a unique capability that allows users to experience hurricane trajectory prediction models simply by entering a few data points about the hurricane’s past movements. Additionally, the project developed algorithms such as Stepwise Temporal Fading Augmentation and Geodesic Beering, offering diversified data augmentation options for training time-series models. Most notably, it addresses bias, emphasizing the most recent records to ensure realistic and accurate predictions**—a technique that, to date, has not been widely explored in time-series data augmentation research.\nThrough this experience, I have improved several skills, including:\nAnalyzing and solving problems Collecting and processing information from reliable sources Researching and experimenting to find new methods by combining existing knowledge Organizing model training and applying new techniques in Machine Learning Integrating AWS cloud services in workflow and deploying the online platform into a functional product Enhancing soft skills Becoming more confident in presenting proposals and improving communication and teamwork with others I have consistently tried to complete assigned tasks and felt very engaged and satisfied when working with new knowledge, especially when I first encountered and explored AWS services.\nTo present a clearer self-assessment of my internship experience, I would evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Learning ability to deeply understand and effectively apply AWS services as well as best practices\nProblem-solving thinking to ensure the process from planning to product completion is smooth and efficient\n"},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.7-event7/","title":"AWS Cloud Mastery Series #3","tags":[],"description":"","content":"Summary Report: “Security - AWS Well-Architected Security Pillar” Event Objectives Introduce the core principles of the AWS Well-Architected Security Pillar Strengthen foundational knowledge in identity, detection, and incident response Guide students to build secure architectures using AWS best practices Promote community growth through learning, collaboration, and content creation Build a pathway for students to earn AWS badges and grow their cloud careers Speakers / Core Team Le Vu Xuan An - FCJ Memeber Tran Duc Anh - FCJ Memeber Tran Doan Cong Ly - FCJ Memeber Danh Hoang Hieu Nghi - FCJ Memeber Key Highlights Overview of AWS Cloud Club — Security Track A community to grow cloud and security skills, develop leadership, and collaborate with peers A structured badging journey recognizes progress and contributions The core team forms the foundation of the club, supporting new members and leading initiatives Participants earn badges by attending workshops and can redeem them for rewards Club members contribute by creating technical content and social media posts A major highlight of the year is the Student Community Day event Benefits to members include: Build Skills Build Community Build Opportunities IAM (Identity \u0026amp; Access Management) Identity controls form the foundation of every AWS security strategy Assign permissions based on least privilege, granting only what each user/service truly needs Remove long-term access keys; prefer IAM roles instead Enable MFA for all users to strengthen authentication Use AWS Single Sign-On (IAM Identity Center) for consistent access management Avoid relying on free credits for experimentation; use AWS Organizations to manage accounts properly Service Control Policies (SCPs) enforce guardrails across multiple accounts (enterprise-level feature) Use Permission Boundaries to prevent privilege escalation Avoid creating IAM users with long-term credentials Support strong authentication methods: TOTP and FIDO2 hardware keys IAM Access Analyzer helps identify unintended public or cross-account access Detection \u0026amp; Continuous Monitoring Implement multi-layer visibility to understand what is happening across the environment Use Amazon EventBridge to automate alerts when unusual activity occurs Adopt Detection-as-Code: manage detection rules and automation through code for consistency Ensure that logs from AWS CloudTrail, CloudWatch, and VPC Flow Logs are centralized for analysis Amazon GuardDuty Addresses key business concerns:\nNot knowing whether the system is under attack Slow response time Compliance risks Unexpected cost spikes due to malicious usage GuardDuty uses machine learning and threat intelligence to identify suspicious behavior\nFully integrates with EventBridge to enable automated workflows\nAdvanced features include S3 Protection, RDS Protection, and attack sequence mapping\nProvides real-time findings, automated recommendations, and multi-account event routing\nAWS Security Hub Managing multiple AWS services makes security findings difficult to track Security Hub aggregates alerts across services into a single, consistent format Helps maintain compliance and implement best practices using AWS and CIS standards Provides step-by-step remediation guidance for findings Supports governance across multiple accounts and multiple regions Network Layer Security A defense-in-depth approach starts with secure VPC design Use Security Groups and Network ACLs to restrict network traffic Monitor network behavior using VPC Flow Logs Design isolated, least-privileged networks and avoid unnecessary public exposure Integrate AWS Network Firewall or PrivateLink where necessary Incident Response Prepare clear incident response playbooks for containment, investigation, remediation, and recovery Automate critical IR steps using EventBridge, Lambda, and Security Hub actions Practice tabletop exercises to validate readiness Document escalation paths, communications, and post-incident reviews Key Takeaways Security Mindset Security is continuous — identity, detection, and incident response must evolve as systems grow Least privilege and strong authentication form the core of AWS identity security Multi-account and multi-region governance ensures consistent protection Technical Practices Use GuardDuty and Security Hub together for end-to-end detection and visibility Automate alerts and workflows with EventBridge Implement Detection-as-Code for version-controlled and repeatable monitoring Apply layered network protection and limit unnecessary access Growth \u0026amp; Community Members develop both technical and leadership skills The club encourages collaboration, teaching, and content creation Badging supports motivation and helps showcase cloud expertise Event Experience Attending the Master 3 — Security workshop provided a comprehensive introduction to modern AWS security practices. Key experiences included:\nLearning from Practitioners The core team offered hands-on insights into identity management, detection tools, and cloud governance Real-world examples helped solidify best practices in IAM and organizational security Hands-on Understanding Discussions about permission boundaries, SCPs, and IAM roles clarified common student difficulties Demonstrations of GuardDuty and Security Hub helped visualize how AWS detects and responds to threats Automation and Visibility EventBridge examples showed how alerts and workflows can be automated Seeing detection and remediation in action highlighted the importance of visibility in security architecture Community Building Sharing knowledge and participating in discussions strengthened connections among members The workshop reinforced the mission of the Cloud Club: learn, grow, and build opportunities together Lessons Learned Identity is the first line of defense Continuous monitoring is essential Automation reduces risk and response time Governance across accounts and regions is critical for real-world environments Some event photos Figure 1 Figure 2 Figure 3 Overall, the event strengthened my understanding of cloud security and deepened my appreciation for AWS best practices. It also highlighted the value of community-driven learning and collaboration.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Continue learning about AWS services Gain more knowledge about AWS CloudFormation and CDK Tasks to be carried out this week: Day Task Start Date Completion Date Reference Materials 2 - Evaluate whether the model meets the requirements of the business problem. - Re-train the model to improve accuracy and ensure other requirements are met. 20/10/2025 20/10/2025 3 - Get familiar with Amazon Systems Manager and the centralized management mechanism for AWS resources. - Understand the function of Session Manager for secure EC2 access without SSH. - Practice creating VPC, subnet, EC2, IAM Role, and successfully connecting via Session Manager. 21/10/2025 21/10/2025 https://000014.awsstudygroup.com/ 4 - Learn about AWS CloudFormation and the mechanism for deploying infrastructure as code. - Understand the structure of CloudFormation Template and Intrinsic Functions. - Practice creating a Stack, granting permissions to Cloud9 Role, and detecting Drift. 22/10/2025 22/10/2025 https://000052.awsstudygroup.com/ 5 - Understand the concept of AWS CDK and its relationship with CloudFormation. - Set up the CDK environment and initialize a project using Cloud9. - Practice creating VPC, subnet, IAM Role, Security Group, and deploying EC2 using CDK. 23/10/2025 23/10/2025 https://000037.awsstudygroup.com/ 6 - Extend CDK infrastructure deployment with API Gateway, ECS, ELB, and Lambda. - Practice creating ECS Cluster, deploying Nginx on Fargate with Load Balancer. - Connect API Gateway and Lambda with S3 to return dynamic data. 24/10/2025 24/10/2025 https://000076.awsstudygroup.com/ Week 7 Achievements: A - Work with Amazon Systems Manager - Session Manager\nGet familiar with and practice Amazon Systems Manager, understanding its mechanism for centralized AWS resource management.\nUnderstand the function of Session Manager for secure EC2 Instance access and management without opening SSH port (22) or using a Bastion Host.\nPractice creating and configuring a complete environment including:\nVPC, Public Subnet, Private Subnet, Security Group EC2 Instances (Linux \u0026amp; Windows) IAM Role allowing EC2 to interact with Systems Manager Successfully connect to the Public Instance via Session Manager, confirming that no SSH traffic occurs and only HTTPS is used, ensuring secure connection.\nConfigure VPC Interface Endpoints (ssm, ssmmessages, ec2messages) to connect to the Private Instance without the Internet, enhancing internal network security.\nDeploy and test Session Logs:\nCreate an S3 Bucket to store session logs and command history. Create an S3 Gateway Endpoint for internal data transfer. Check and verify successful log storage. Practice Port Forwarding via AWS CLI:\nConfigure RDP port forwarding from the local machine to the Windows Instance in the Private Subnet. Successfully log in via Remote Desktop using internal port forwarding. B - AWS CloudFormation\nUnderstand AWS CloudFormation – a service that enables describing and deploying AWS infrastructure as code (IaC), automating resource creation and management safely and repeatedly.\nLearn to use CloudFormation Templates to model infrastructure through key components:\nAWSTemplateFormatVersion Description Parameters Resources Outputs Practice creating and editing CloudFormation Templates in AWS Cloud9, a web-based IDE supporting multiple programming languages (Python, JavaScript, PHP, etc.).\nInstall and configure required tools in Cloud9 Workspace, including:\njq, gettext, bash-completion, moreutils cfn-lint (syntax checker for CloudFormation) taskcat (automated template testing) Understand and apply Intrinsic Functions in CloudFormation:\n!Ref – reference parameter or resource values !Sub – substitute variables in strings !GetAtt – retrieve attributes from created resources Successfully create basic resources using CloudFormation Template:\nSecurity Group allowing port 80 (HTTP) IAM Role \u0026amp; Instance Profile allowing EC2 to interact with SSM and CloudWatch EC2 Instance automatically attached with IAM Role and Network Interface Practice creating a CloudFormation Stack directly from Cloud9 via AWS CLI and grant Cloud9 Role the necessary permissions (AmazonEC2FullAccess, IAMFullAccess) to execute stack creation.\nUnderstand the concept of Drift Detection – identifying discrepancies between actual infrastructure state and CloudFormation configuration.\nGet familiar with advanced topics: Custom Resources, Mapping, StackSet, and Macro – for automating, extending, and managing complex infrastructure deployment.\nC - CDK Basic\nUnderstand AWS CloudFormation and how to deploy infrastructure as code (IaC).\nLearn the relationship between CloudFormation and AWS CDK, where CDK serves as an abstraction layer to define AWS infrastructure using popular programming languages such as Python, TypeScript, JavaScript, Java, and C#.\nInstall and initialize the AWS CDK working environment via Cloud9 IDE.\nPractice initializing a CDK project.\nUnderstand CDK workflow:\nWrite infrastructure code in Python CDK synthesizes code into a CloudFormation Template (using cdk synth) Deploy actual AWS resources via cdk deploy Practice creating a VPC and public subnet using CDK.\nCreate and attach IAM Role for EC2 Instance, configure Security Group.\nDeploy an EC2 Instance on the created VPC using AWS CDK.\nDeploy infrastructure using cdk deploy and verify results in AWS Management Console.\nAccess the created EC2 Instance and check Apache Web Server operation via Public IP.\nUnderstand the full CDK workflow: define – synthesize – validate – deploy – verify.\nD - CDK Basic - 2\nUnderstand how to extend AWS infrastructure deployment with CDK by integrating multiple services such as API Gateway, ECS, Elastic Load Balancer, and Lambda.\nPractice creating an ECS Cluster and deploying a sample Nginx application on Fargate with an Application Load Balancer.\nConfigure API Gateway to route requests to ECS services through the Load Balancer.\nCreate and configure a Lambda function to access an S3 Bucket, read and list objects within it.\nPractice connecting Lambda with API Gateway and test dynamic data responses from S3.\nUnderstand how to manage Python virtual environments and install dependencies when working with CDK.\nLearn about Nested Stack and how to divide CDK architecture into sub-stacks for better scalability and maintainability.\nComplete deployment and validation of the full advanced CDK architecture on AWS.\n"},{"uri":"https://giaphazzz.github.io/aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Gia Phat\nPhone Number: 0939540100\nEmail: phat10102005@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: SE190236\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://giaphazzz.github.io/aws/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe work environment is friendly and professional, with spacious and clean seating. One minor issue I noticed, common among students, is that some groups gather and talk too loudly, which makes it hard for me to focus on my tasks. Additionally, I hope AWS allows the use of the pantry to get water, because if I bring a bottle and finish it during work hours, I have to go all the way down to the ground floor to buy more, which wastes time. Otherwise, everything else was fine, and I have no further comments.\n2. Support from Mentor / Team Admin\nMentors supported students quite well. However, when creating chat groups, there should ideally be two separate groups: one for announcements and one for casual conversation. Sometimes when I miss an announcement, it gets buried under other messages (chats, diagram questions, course sharing, etc.). I also want to thank the mentors for their support and contributions to AWS workshops, which helped us understand the services better.\n3. Relevance of Work to Academic Major\nI study AI, specializing in algorithm design, data processing, and model training. Most AWS services already provide AI models (e.g., Bedrock) that can be used via API without much involvement in this field. The main work mostly involves architecture and services rather than improving model features or accuracy/efficiency.\nHowever, even though this is not exactly my major, I gained practical knowledge that cannot be learned at school. For example, knowing how to integrate an existing model and its API into software for end users, especially combined with cloud technology, is extremely valuable and will likely be useful in my future career.\n4. Learning \u0026amp; Skill Development Opportunities\nSpecifically, I learned things such as using SageMaker to run hurricane trajectory prediction models, exploring other AI-assisted services, using Lambda, S3, EC2, etc. Importantly, I learned more about operational cost calculation for the team. Previously, using RDS, EC2, etc., would result in a cost of around $58 per month. By switching to a serverless architecture using S3 and Lambda, the cost dropped to around $18. This is a noteworthy achievement during this OJT period. It also made me realize that IT work is not purely technical, but also involves cost-saving considerations for the business.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is generally optimistic and positive. Work is not overwhelming, there are many new things to learn, and teammates are friendly and open. However, some students (FPT students, to critique again) have poor communication skills, especially being passive and not contributing ideas for project development. Overall, though, the FCJ group is good, with peers supporting and sharing experiences with each other.\n6. Internship Policies / Benefits\nAllowing students to choose suitable office days was very reasonable. Working hours were standard, from 9 a.m. to 5 p.m. There were also events and workshops that were both useful and enjoyable.\nAdditional Questions Meeting Mr.Eric Yeo in person was impressive. But the most satisfying aspect was learning how to calculate operational costs efficiently and reasonably.\nThe areas for improvement have already been mentioned above. I hope in the future, AWS will allow interns to use the pantry to get drinking water.\nFor students from my school majoring in software engineering or cybersecurity, I would definitely recommend AWS as an internship destination. With cloud infrastructures and the widespread adoption of cloud services, it’s a great place to gain knowledge and expand career opportunities.\nSuggestions \u0026amp; Expectations Some suggestions for future interns: speak more quietly in the office and be proactive and open in both work and life.\nOf course, FCJ with the J stands for a journey. And this journey does not end even when the OJT ends.\n"},{"uri":"https://giaphazzz.github.io/aws/4-eventparticipated/4.8-event8/","title":"CLOUDTHINKER","tags":[],"description":"","content":"Summary Report: “BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock” Event Objectives Provide a structured overview of Agentic AI and the shift toward autonomous AI systems Introduce Amazon Bedrock AgentCore and AWS’s agentic ecosystem Demonstrate real-world Agentic Workflow design on AWS Showcase CloudThinker Agentic Orchestration and context optimization techniques Deliver hands-on experience building agentic applications using AWS Bedrock Create a space for networking with industry experts across the AI and cloud ecosystem Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Kien Nguyen – Solutions Architect, AWS Viet Pham – Founder \u0026amp; CEO, Diaflow Thang Ton – Co-founder \u0026amp; COO, CloudThinker Henry Bui – Head of Engineering, CloudThinker Kha Van – Community Leader, AWS Key Highlights The Evolution of Agentic AI Traditional ML / classical AI\nFixed tasks, narrow capabilities Require structured datasets and extensive preprocessing Limited scalability and adaptability Modern Agentic AI\nDriven by Foundation Models and multi-step reasoning Autonomous task decomposition \u0026amp; tool usage Can integrate APIs, execute workflows, retrieve knowledge Highly flexible and production-ready when paired with AWS services Challenges in Bringing Agentic AI to Production Performance – latency, parallel reasoning, token throughput Scalability – multi-agent workloads, context management Security – data governance, identity flow, access control Governance – observability, audit trails, task boundaries AWS Agentic AI Portfolio Amazon Bedrock AgentCore – identity, memory, runtime, tools, workflows Agent Gateway – unified tool \u0026amp; API integration Model flexibility – choose from Anthropic, Meta Llama, Amazon Titan, and more Production-focused design – scaling, guardrails, observability Amazon Bedrock AgentCore (by AWS) Runtime – Manages multi-step task execution Memory – Long-term and short-term context retention Identity Flow – Enforces permissions, security boundaries Agent Gateway – Connects to tools, APIs, internal systems Code Interpreter – Safe execution environment for dynamic code Browser Tool – Retrieves external or web-based information Observability – Tracing, logs, metrics for debugging \u0026amp; governance Building Agentic Workflow on AWS (Use Case by Diaflow) Multi-agent coordination Context retrieval and function calling Linking agents to enterprise data sources Orchestrating business logic through Bedrock \u0026amp; Diaflow tools Practical implementation patterns for startups \u0026amp; SMEs CloudThinker Agentic Orchestration \u0026amp; Context Optimization High-level agent-to-agent coordination patterns Intelligent context filtering \u0026amp; optimization for model efficiency Dynamic workflow adaptation Evaluating and improving chain-of-thought reliability Integrating CloudThinker with Amazon Bedrock AgentCore CloudThinker Hack: Hands-On Workshop Setting up Bedrock agents Building a simple agentic pipeline Integrating external tools into workflows Debugging and optimizing agent behavior Deploying a working proof-of-concept Key Takeaways Agentic AI Mindset AI is shifting from responding → acting Agents must integrate memory, tools, orchestration, and identity Foundation Models + Orchestration = new generation of AI applications AWS provides the strongest production-ready environment for Agentic AI Technical Understanding The importance of context optimization How tool usage and identity flow affect agent reliability Why AgentCore simplifies multi-step reasoning The value of real-world orchestration patterns How to connect FMs with APIs, business logic, and data repositories Practical Development Skills Designing an end-to-end AI workflow Connecting external tools into an agent pipeline Managing multi-agent interactions Understanding latency, scaling, and security considerations Deploying agents safely with guardrails \u0026amp; monitoring Applying to Work Participants can immediately apply what they learned:\nBuild agent-based assistants, automation pipelines, or internal copilots Integrate AWS Bedrock models into enterprise systems Use AgentCore to create multi-step, governed workflows Build prototypes faster—without DevOps-heavy infrastructure Experiment with CloudThinker orchestration techniques for better performance Apply agentic design patterns in real startup or enterprise contexts Event Experience Attending the “Agentic Build AI – Optimization with Amazon Bedrock” workshop offered strong insights into modern AI development.\nKnowledge from Industry Experts Leaders from AWS, CloudThinker, and Diaflow shared production experience Clear guidance on scaling GenAI products Real-world examples of agentic automation in business workflows Hands-On Experience Built a functioning agent pipeline Understood how context memory, identity, and tools interact Gained practical familiarity with Bedrock’s APIs and orchestration components Networking Opportunities Connected with AWS architects, engineers, founders, and community leaders Discussed career paths, cloud-native AI development, and product roadmaps Lessons Learned Agentic AI is the next step beyond chatbots and LLMs Production readiness requires identity, scalability, and observability Amazon Bedrock + CloudThinker offers an efficient path to production Real projects showcase far more value than academic assignments Some event photos Figure 1 Figure 2 Figure 3 Overall, the “Agentic Build AI – Optimization with Amazon Bedrock” workshop delivered a powerful combination of strategic insights and practical, hands-on learning. Participants gained real-world skills in designing agentic workflows, optimizing context, integrating tools, and deploying scalable, secure AI systems.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Explore knowledge about IoC, VPC Flow Logs, and billing. Complete the hands-on labs provided in the course. Tasks to Implement This Week: Day Task Start Date Completion Date Reference Material 2 - Run a comparison between the proposed STFA method and other augmentation techniques. - Evaluate performance using multiple error metrics, including custom error measures. 27/10/2025 27/10/2025 3 - Learn the concept and benefits of Infrastructure as Code (IaC) for automated infrastructure management. - Differentiate CloudFormation, CDK, Terraform, and Pulumi. - Practice setting up Cloud9, creating an IAM Role, and deploying a Lambda Function using CloudFormation. 28/10/2025 28/10/2025 https://000111.awsstudygroup.com/ 4 - Configure and monitor EC2 resources with Amazon CloudWatch. - Install the CloudWatch Agent to collect CPU, memory, network, and disk metrics. - Apply AWS EC2 Resource Optimization and Compute Optimizer to generate cost optimization recommendations. 29/10/2025 29/10/2025 https://000014.awsstudygroup.com/ 5 - Learn about VPC Flow Logs and how to monitor IP traffic within a VPC. - Deploy network monitoring infrastructure using CloudFormation. - Analyze logs using CloudWatch Log Insights to identify issues and optimize network performance. 30/10/2025 30/10/2025 https://000105.awsstudygroup.com/ 6 - Practice delegating Billing Console access to IAM Users. - Create a custom IAM Policy (BillingViewAccess) and attach it to users or groups. - Verify Billing Console access and clean up resources after practice. 31/10/2025 31/10/2025 https://000106.awsstudygroup.com/ Week 8 Achievements: A - Infrastructure as Code\nUnderstood the concept and benefits of Infrastructure as Code (IaC) in automating infrastructure management through source code.\nDistinguished among common IaC frameworks such as CloudFormation, CDK, Terraform, and understood their deployment differences.\nLearned the role of IaC in DevOps and CI/CD for improving productivity and maintaining consistency in infrastructure deployment.\nPracticed setting up a Cloud9 environment, creating an IAM Role with AdministratorAccess, and attaching it to an EC2 Instance.\nDeployed a Lambda Function using a CloudFormation Template and successfully validated the created resources.\nImplemented a sample VPC and EC2 deployment using CloudFormation and understood template structures including Parameters, Mappings, Resources, and Outputs.\nConnected to the EC2 Instance via SSM, verified the connection, and confirmed a secure connection without opening SSH ports.\nExplored the three-tier architecture deployment model (Bastion Host, Web Tier, App Tier, Database Tier) on AWS CloudFormation.\nPracticed creating stacks, testing cross-tier connectivity, and cleaning up resources after deployment.\nB - Right-sizing with EC2 Resource Optimization\nMastered the setup, configuration, and monitoring of EC2 resources using Amazon CloudWatch to track real-time instance performance.\nSuccessfully created and attached an IAM Role to EC2 for enabling the CloudWatch Agent to collect CPU, memory, network, and disk metrics.\nInstalled and configured CloudWatch Agent to include custom metrics for more accurate analysis.\nApplied AWS EC2 Resource Optimization to identify underutilized instances and recommend cost reduction strategies.\nUnderstood and applied EC2 right-sizing principles in combination with Saving Plans for cost efficiency.\nUsed AWS Compute Optimizer to analyze and receive optimization recommendations for EC2, Auto Scaling Groups, EBS, Lambda, and Fargate based on real performance data.\nUnderstood the conditions and limitations of Compute Optimizer across different resource types.\nC - Network Monitoring with VPC Flow Logs\nGained a clear understanding of how VPC Flow Logs work and how to collect IP traffic data between network interfaces within a VPC.\nDeployed automated network monitoring infrastructure using CloudFormation, including VPC, subnets, IAM Roles, and CloudWatch Log Groups.\nEnabled VPC Flow Logs and configured them to send data to Amazon CloudWatch Logs for real-time network traffic monitoring.\nPracticed log analysis using CloudWatch Log Insights to detect security, configuration, and performance issues.\nCompleted the full network monitoring workflow and understood its application in optimizing, controlling, and troubleshooting AWS environments.\nD - Billing Console Delegation\nLearned the process of delegating Billing Console access in AWS to ensure secure financial management and proper role separation.\nEnabled Billing access for IAM Users via “IAM User and Role Access to Billing Information” in the root account.\nCreated a custom IAM Policy (BillingViewAccess) to restrict permissions to billing and cost management only.\nAttached the policy to IAM groups or users, ensuring precise and controlled access.\nSuccessfully verified Billing Console access using the authorized IAM account.\nCleaned up all user groups and policies after the exercise to maintain a clean environment.\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn about modules related to Security Get familiar with AWS Single Sign-On and Security Hub Tasks to Implement This Week: Day Task Start Date Completion Date Reference Material 2 - Research and prepare to integrate the trained model with AWS services 03/11/2025 03/11/2025 3 - Learn about AWS IAM Identity Center (AWS SSO) + Concept, role, and benefits + Basic setup and configuration 04/11/2025 04/11/2025 https://000121.awsstudygroup.com/ 4 - Practice IAM Identity Center + Create and assign Permission Sets + Configure multi-account access through the portal + Set up Time-based Access Control 05/11/2025 05/11/2025 https://000122.awsstudygroup.com/ 5 - Learn about IAM Permission Boundaries + Mechanism and purpose + Comparison with Identity-based Policy 06/11/2025 06/11/2025 https://000123.awsstudygroup.com/ 6 - Access Control with IAM Policies and Conditions - Enable and configure AWS Security Hub 07/11/2025 15/11/2025 https://000124.awsstudygroup.com/ Week 9 Achievements: A - Identity Federation with AWS Single Sign-On\nUnderstood the concept and role of AWS IAM Identity Center (formerly AWS Single Sign-On) in centralized identity management and access control across multiple AWS accounts. Learned how to set up IAM Identity Center to manage access based on the principle of least privilege, assigning permissions by user groups and project roles. Successfully deployed AWS Organization and created Organizational Units (Security, Shared Services, Logging, Application) for efficient resource management and separation. Practiced creating and assigning Permission Sets for users and configuring multi-account access through AWS IAM Identity Center. Learned and applied Time-based Access Control to restrict permissions within specific time frames, suitable for audit or temporary project roles. Created, assigned, and verified users and groups in IAM Identity Center, confirming access through the portal URL and designated roles. Practiced accessing AWS CLI through IAM Identity Center using automatic authentication (OIDC), ensuring security and Zero Trust compliance. Learned how to use Customer Managed Policies to reuse custom permission sets across multiple AWS accounts for flexible and consistent access management. Deployed and tested Identity Store APIs (IdentityStore API) to automate user, group, and membership creation, updates, and verification. Completed user and group auditing using CLI commands to ensure permissions were properly granted and time-bound, maintaining compliance and security. B - Permission Management with IAM Permission Boundaries\nUnderstood the concept and purpose of IAM Permission Boundaries for defining the maximum allowed permissions for users or groups. Learned the mechanism of Permission Boundaries — where effective permissions are the intersection between the user policy (Identity-based Policy) and the boundary policy. Practiced creating a Restriction Policy that only allows operations on EC2 services within the ap-southeast-1 (Singapore) region. Created an IAM user “ec2-admin” with AmazonEC2FullAccess and applied a Permission Boundary named “ec2-admin-restrict-region.” Verified that the “ec2-admin” user could only create and manage EC2 instances in Singapore and was restricted from other regions such as Sydney. Understood the benefits of Permission Boundaries in preventing privilege escalation and simplifying large-scale permission management. Performed cleanup by deleting users and policies after testing to maintain a clean and secure environment. C - Access Control with IAM Policies and Conditions\nUnderstood and applied the principle of least privilege when granting permissions to ensure users only access what is necessary. Learned the concept and process of Assume Role and how IAM users obtain temporary credentials via AWS STS. Practiced creating IAM Groups with administrative privileges for EC2 and RDS to manage users more securely and centrally. Created IAM Users with different permission levels (EC2-admin-user, RDS-admin-user, Group-user, No-permission-user) to simulate real-world access scenarios. Configured IAM Roles with Admin privileges and established Trust Relationships to allow IAM Users to perform Assume Role operations. Applied Conditions in IAM Roles to restrict access by IP address or time, enhancing security and fine-grained access control. Verified successful role-switching according to defined conditions and cleaned up resources after completion. D - AWS Security Hub\nUnderstood the role of AWS Security Hub in aggregating, organizing, and prioritizing security alerts from multiple AWS services such as GuardDuty, Inspector, and Macie. Learned the operating mechanism of Security Hub in visualizing risks through an intuitive dashboard to easily track security posture and compliance. Learned how to activate Security Hub in the AWS Management Console and select compliance standards such as AWS Foundational Security Best Practices, CIS AWS Foundations Benchmark, and PCI DSS. Monitored and evaluated the AWS account’s Security Score under each standard to identify and address potential risks and vulnerabilities. Understood the relationship between Security Hub and AWS Config and learned how to enable AWS Config to record resources necessary for compliance assessments. Practiced removing irrelevant findings and managing the activation or deactivation of security standards based on actual project needs. Completed the process of monitoring, assessing, and optimizing AWS account security posture based on applicable compliance frameworks. "},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Understand the process of containerizing applications with Docker and deploying them on AWS Cloud. Get familiar with ECS, CDK, and CodePipeline to automate infrastructure and application deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 2 - Learn the concepts of containers and Docker + Docker Engine, Dockerfile, Docker Image, Container 10/11/2025 10/11/2025 https://000201.awsstudygroup.com/ 3 - Learn how to push/pull images to Docker Hub and Amazon ECR - Configure VPC, EC2, and RDS to deploy containerized applications on AWS 11/11/2025 11/11/2025 https://000202.awsstudygroup.com/ 4 - Learn about Amazon ECS and launch types (Fargate, EC2) + Configure ECS Cluster, Task Definition, and Service + Deploy frontend and backend on ECS 12/11/2025 12/11/2025 https://000203.awsstudygroup.com/ 5 - Deploy ECS using AWS CDK (IaC) + Create VPC, NAT Gateway, and ECS Cluster using CDK + Build ECS Service and connect to DynamoDB, API Gateway 13/11/2025 13/11/2025 https://000204.awsstudygroup.com/ 6 - Set up CI/CD pipeline using CodePipeline, CodeBuild, and CodeDeploy + Create repository and build pipeline from GitHub or CodeCommit + Automatically build and deploy the application to ECS Fargate or EC2 14/11/2025 15/11/2025 https://000205.awsstudygroup.com/ Week 10 Achievements : A - Containerization with Docker\nUnderstood the full process of deploying containerized applications with Docker, from local environment to AWS Cloud. Successfully configured and operated AWS services such as VPC, EC2, RDS, and ECR for containerized deployment. Practiced deploying applications using Docker Images, comparing manual deployment vs Docker Compose. Built and managed a container network enabling stable communication between frontend, backend, and database. Pushed and managed images on Docker Hub and Amazon ECR for reuse and automated deployment. Gained a clear understanding of Nginx’s role in routing and load balancing across containers. Optimized development and deployment workflows by separating development and production environments. Improved troubleshooting and application monitoring skills in cloud environments. B - Container Orchestration with Amazon ECS\nMastered the process of deploying containerized applications on Amazon ECS with both Fargate and EC2 launch types. Successfully created and configured ECS Clusters and Task Definitions for frontend and backend services, ensuring stability and isolation. Applied AWS Cloud Map Service Discovery to connect internal containers via DNS names. Set up Application Load Balancer, Target Groups, and Listeners to achieve load balancing and scalability. Practiced backend Blue/Green Deployment to ensure zero-downtime updates. Performed Rolling Deployment for frontend to enable continuous version updates without downtime. Managed and monitored containers via ECS Console and CloudWatch Logs to understand task activity within clusters. Tested and verified smooth application operation across frontend, backend, and database. Optimized CI/CD workflow by integrating ECR, ECS, and CodeDeploy, building a foundation for automated deployment in future projects. C - Infrastructure as Code for ECS with CDK\nUnderstood how to deploy a Spring Boot application on AWS ECS Fargate using AWS CDK. Learned the principles of Infrastructure as Code (IaC) and the benefits of using AWS CDK v2 to automate resource creation. Successfully created and configured an Amazon ECR Repository for Docker image storage. Deployed VPC and NAT Gateway using AWS CDK to establish a secure and isolated network environment. Built ECS Cluster, Task Definitions, and ECS Service for backend microservices. Created API Gateway to manage REST endpoints and connect directly to ECS Service. Designed and deployed a DynamoDB Table for product data storage using AWS SDK v2 for Java. Implemented application logging via CloudWatch Logs and configured AWS X-Ray for tracing and analysis. Enhanced AWS infrastructure management, configuration, and deployment skills with automated and reusable IaC. D - CI/CD Pipeline with AWS CodePipeline\nUnderstood the concepts and workflows of CI/CD (Continuous Integration / Continuous Deployment) for container deployment on AWS ECS. Successfully set up CI/CD pipelines using GitLab, GitHub Actions, and AWS CodeBuild integrated with ECR, ECS, and CodeDeploy. Automated the full process of building, testing, and deploying applications from source code to ECS Fargate. Learned how to create and manage GitLab Runners and define pipelines using .gitlab-ci.yml and .github/workflows. Integrated AWS services including CodeBuild, CodePipeline, and CodeDeploy into a seamless deployment process. Used CloudWatch Container Insights to monitor container performance (CPU, memory, network, and ECS task status). Configured FireLens in ECS to collect and route container logs to Amazon S3 for analysis and tracking. Practiced log management, issue detection, and performance optimization via CloudWatch and FireLens. Completed AWS resource cleanup (ECS, ECR, RDS, EC2, IAM, S3, VPC, NAT Gateway) to maintain a clean environment and reduce costs. Strengthened DevOps knowledge, particularly in implementing modern CI/CD on a cloud-native AWS environment. E - Automated Deployments with AWS CodePipeline\nUnderstood the end-to-end automated CI/CD deployment process for applications on EC2 using AWS CodePipeline. Learned the roles and coordination among services within a pipeline — CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Created an S3 bucket to store build and deployment artifacts. Installed and configured the CodeDeploy Agent on EC2 via Session Manager or User Data. Practiced creating repositories in CodeCommit (or GitHub as an alternative) and performing source code cloning, mirroring, and migration. Set up a build project using AWS CodeBuild, configured GitHub as the source, established a Linux build environment, and stored output artifacts in S3. Created an application and Deployment Group in AWS CodeDeploy, configured in-place deployment, and grouped EC2 instances by tags for automated deployment. Successfully deployed a Node.js application on EC2 using CodeDeploy and verified functionality via instance DNS. Built a complete CodePipeline with Source – Build – Deploy stages to automate the deployment process on code updates. Tested the pipeline by committing code changes and observing the automatic execution of build and deployment steps. Gained a clear understanding of the closed-loop CI/CD process on AWS — from source commit to build, test, deploy, and real-world verification. "},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Get familiar with AWS serverless services Hands-on experience with AWS SAM, Cognito, and related tools Tasks to carry out this week: Day Task Start Date Completion Date Reference Mon - Learn how to deploy the Java SpringBoot Monolith TravelBuddy application on AWS 17/11/2025 17/11/2025 https://000050.awsstudygroup.com/ Tue - Create CloudFormation stack, Key Pair, RDS MySQL - Import TravelBuddy project into Eclipse and run locally 18/11/2025 18/11/2025 https://000052.awsstudygroup.com/ Wed - Learn Lambda and serverless microservices - Create Java Lambda function, test locally, upload to AWS - Connect Lambda with S3 trigger 19/11/2025 19/11/2025 https://000078.awsstudygroup.com/ Thu - Deploy Node.js Lambda function to process S3 and DynamoDB Configure IAM role, memory, timeout, environment 20/11/2025 20/11/2025 https://000080.awsstudygroup.com/ Fri - Deploy SAM project, create API Gateway and Lambda functions - Configure front-end with REST API on S3 - Create Cognito User Pool, authentication flow, and test registration/login 21/11/2025 21/11/2025 https://000081.awsstudygroup.com/ Week 11 Achievements: A - Monolith to Microservices Migration\nLearned how to deploy the Java SpringBoot TravelBuddy application on AWS\nUsed CloudFormation to provision and verify AWS resources\nUtilized AWS Tools for Eclipse and Elastic Beanstalk CLI to deploy and update the application\nUsed AWS SDK to query and manipulate AWS resources programmatically\nPrepared environment before deployment:\nCreated Key Pair to access instances\nCreated CloudFormation stack\nConnected to Windows Instance\nSet up MySQL database on RDS\nDownloaded TravelBuddy project\nTested project on Eclipse IDE:\nImported Maven project into Eclipse\nConfigured environment variables for RDS connection (JDBC_CONNECTION_STRING, JDBC_UID, JDBC_PWD)\nCreated internal Tomcat server to run the application\nRan application and checked data from RDS\nDeployed application on Elastic Beanstalk:\nExported WAR file from Eclipse\nCreated Elastic Beanstalk Application and Environment\nConfigured environment: High availability, VPC, Load Balancer, Subnet, Security Groups, Instance type, EC2 Key Pair\nSet environment variables in Elastic Beanstalk matching RDS\nDeployed WAR file and accessed application via Elastic Beanstalk URL\nApplication update (Update Application):\nEdited index.jsp\nBuilt new WAR file using Maven (mvn package)\nUsed Elastic Beanstalk CLI (eb init+eb deploy) to update application\nVerified changes on the website\nQuery API and manipulate AWS through Eclipse IDE:\nImported EC2Report Maven project Updated EC2Manager.java to query correct region Ran JUnit Test to retrieve EC2 instance information Practiced reading tags, instance info, and creating new instances via AWS SDK Managed entire workflow from local development, build, deployment, to AWS resource management and queries\nB - Building Microservices\nLearned to create and deploy serverless microservices on AWS using Lambda\nPrepared environment and configured Eclipse IDE for Lambda function development\nCreated and tested Lambda microservices locally:\nCreated Java Lambda project in Eclipse\nUpdated pom.xml to use latest Mockito version\nRan JUnit Test with JSON payload simulating S3 events\nHandled URL-encoded keys in LambdaFunctionHandler\nUploaded and tested Lambda on AWS:\nCreated new Lambda function named TestLambda\nUploaded code from Eclipse and selected IAM role LambdaRole\nConnected Lambda to S3 bucket for automatic triggering\nExtended serverless microservices:\nCreated Lambda ImageManager to generate thumbnails for JPG files and delete other files Used AWS SAM and CloudFormation for automated deployment of Lambda, triggers, and S3 bucket Updated Lambda permissions as needed Configured orchestration with CodeStar:\nCreated new branch and managed CI/CD pipeline Redeployed microservices upon code updates Updated target region for API Optional advanced exercises:\nCreated Lambda function to process various file types Configured S3 trigger to invoke Lambda for uploaded images Deployed and configured Lambda function automatically Connected microservice to RDS within VPC Deployed microservices using AWS Developer Tools and CI/CD automation C - Serverless Backend with Lambda, S3, and DynamoDB\nHands-on with serverless architecture and AWS Lambda Created Lambda function to handle S3 events for resizing and managing images Configured source and destination S3 buckets, set up event notifications Set up IAM role and policy for Lambda with least privilege Configured memory, timeout, and environment variables for Lambda function Deployed Node.js Lambda, tested image processing pipeline, logging, and CloudWatch monitoring Created DynamoDB table with appropriate write capacity Created Python Lambda function to write data into DynamoDB (book_create) Configured runtime, function name, and wrote code to handle event data and HTTP responses Deployed and tested Lambda function with correct permissions Applied serverless best practices: error handling, logging, least privilege, throughput management D - Deployment Automation with AWS SAM\nInstalled SAM CLI and configured AWS credentials for sam-admin Created sample SAM project using sam init Created front-end S3 bucket with static web hosting and public access policy via SAM template Built, validated, and deployed SAM application using sam build, sam validate, sam deploy \u0026ndash;guided Created DynamoDB table for book data storage Deployed Python Lambda functions for listing, writing, deleting data, and image resizing Configured API Gateway with GET, POST, DELETE endpoints interacting with Lambda functions Tested APIs using Postman: listing, creating, and deleting items Connected front-end to REST API, updated configuration, uploaded to S3, and tested functionality E - User Authentication with Amazon Cognito\nStudied documentation, installed SAM CLI and AWS CLI, configured AWS credentials\nBuilt front-end from FCJ-Serverless-Workshop repository and uploaded to S3 bucket\nCreated Cognito User Pool:\nSelected traditional web application with email authentication Created App client cognito-fcj-book-shop, saved Client ID and Secret Enabled ALLOW_USER_PASSWORD_AUTH authentication flow Updated template.yaml with Cognito Client ID/Secret, deployed Lambda functions for registration/login\nCreated Registration function in SAM with registerPathPart\nDeployed Lambda and API Gateway:\nBuilt, validated, and deployed SAM project Retrieved Invoke URL to connect front-end Tested front-end:\nUpdated APP_API_URL in config.js Built and uploaded front-end to S3 Registered user and confirmed email Logged in and tested features: Create book, Management, Order access according to permissions Completed authentication flow with Cognito, Lambda, and API Gateway, ensuring users can only access features after logging in\n"},{"uri":"https://giaphazzz.github.io/aws/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Understand the concept of Data Lake and the core components on AWS (S3, Glue, Athena, QuickSight) Practice ingesting, processing, querying, and visualizing data on AWS Get familiar with serverless analytics and machine learning workflow using SageMaker Tasks to carry out this week: Day Task Start Date Completion Date Reference Mon - Read about Data Lake introduction on AWS 24/11/2025 24/11/2025 https://000035.awsstudygroup.com/ Tue - Query data using Athena - Practice statistics and data analysis 25/11/2025 25/11/2025 https://000070.awsstudygroup.com/ Wed - Visualize data using QuickSight - Create Dataset, Visual, Analysis, basic Dashboard - Share Dashboard with other users 26/11/2025 26/11/2025 https://000073.awsstudygroup.com/ Thu - Practice Athena Spark for ETL and data processing - Create federated queries connecting multiple sources Monitor session, DPU, notebook history 27/11/2025 27/11/2025 https://000106.awsstudygroup.com/ Fri - Get familiar with SageMaker Studio and Feature Store - Feature engineering using Data Wrangler 28/11/2025 28/11/2025 https://000200.awsstudygroup.com/ Week 12 Achievements: A - Data Lake Fundamentals on AWS\nUnderstand Data Lake and the main components:\nAmazon S3 for data storage AWS Glue for data management and ETL Amazon Athena for data querying Amazon QuickSight for data visualization Created IAM role for AWS Glue and attached necessary policies\nCreated S3 bucket for storing data and set up Delivery Stream for data collection\nGenerated sample data to test ingest and analysis workflows\nUsed AWS Glue Crawler to ingest data from S3 into Data Catalog:\nCreated Crawler summitcrawler Created database summitdb Ran Crawler and verified raw data tables Used Amazon Athena to query data:\nExecuted SQL queries to inspect data Performed statistical queries, e.g., counting by activity_type Created and used SageMaker Notebook for data processing and transformation:\nInitialized interactive session Ran Python code to clean, transform, and analyze data Used Amazon QuickSight to visualize data:\nCreated Data source and Dataset from S3/Athena Built Visuals, Analysis, and Dashboard Shared Dashboard with other users Capable of implementing a full Data Lake workflow on AWS, from ingestion, processing, querying to visualization\nB - Building a Data Lake with Your Own Data\nExplored serverless Data Lake construction on AWS with core components:\nAmazon S3 for raw and processed data AWS Glue DataBrew for data cleaning, normalization, and transformation AWS Glue Crawler and Jobs to create Data Catalog and convert data to Parquet Amazon Athena for SQL queries Amazon QuickSight for visualization Used Glue DataBrew to clean and transform data:\nData profiling, cleaning, transforming Prepared next dataset Uploaded processed data to another S3 bucket Set up AWS Glue for data ingestion workflow:\nCreated IAM Role with necessary permissions Created Data Catalog with Glue Crawler Ran Glue Jobs to convert CSV to Parquet Created new Data Catalog for Parquet data Verified schema for Athena queries Queried data with Amazon Athena:\nExecuted basic and advanced queries (join, partition, view) Compared columnar vs row-based data Visualized data with Amazon QuickSight:\nRegistered and configured access permissions Connected dataset and modified data Built Visuals, Analysis, and Dashboard Performed cleanup after workshop:\nUnregistered QuickSight and deleted IAM Roles Deleted Workflows, Jobs, Crawlers, and Databases on AWS Glue Emptied and deleted S3 buckets Deleted Cloud9 instance C - Business Intelligence with Amazon QuickSight\nLearned to build dashboards and visualize data in QuickSight:\nKey concepts: Data source, Dataset, Analysis, Visual, Dashboard Connected data from S3, Athena, or other sources Prepared QuickSight environment:\nCreated QuickSight Enterprise account Connected and registered sample dataset (sales.csv) Set permissions and region (Singapore) Built basic dashboard:\nUpdated Dataset Created Line Chart, Pie Chart, Pivot Table, key statistics Completed dashboard to summarize information Enhanced dashboard:\nUsed Sheets to organize pages Applied Themes to customize colors, fonts, and spacing Configured field format and visual format for each chart Added data details and secondary charts for better visualization Created interactive dashboard:\nSaved dashboard backup Set up filter layers and navigation actions Published dashboard and shared with other users D - Serverless Analytics with Amazon Athena\nLearned about Amazon Athena and key features:\nServerless interactive analytics, supports SQL and Python queries directly on S3 or other sources Supports multiple sources: S3, RDS, Redshift, on-premises, and other cloud services Integrated with AWS Glue Data Catalog for metadata and schema management Used Athena for Apache Spark:\nCreated Workgroup using Spark engine Created Notebook, executed calculations, monitored/debugged via CloudWatch Logs Interacted with data from Glue Data Catalog, S3, and other sources Performed ETL, data normalization, and stored results in S3 Visualized data in Athena Spark:\nUsed Matplotlib and Seaborn for statistical plots, data analysis, and reports Integrated additional Python libraries via pip and S3 Managed sessions and Workgroup:\nMonitored session details, compute history, DPU usage, execution status, and notebook results Used Workgroup to manage Notebooks, Sessions, and calculations Athena Federation:\nRan federated queries across multiple sources using familiar SQL Supported connectors for RDS, MySQL, PostgreSQL, Redshift Combined data from multiple sources, saved results to S3 for further analysis Created Lambda function and VPC endpoint for federated data connection E - Machine Learning with Amazon SageMaker\nLearned Amazon SageMaker Immersion Day concepts:\nFull ML workflow: feature engineering, training, tuning, deploying models Transition ML workloads from traditional environment to SageMaker Basics of Model Debugging, Model Monitoring, AutoML, AWS ML Well-Architected assessment Prepared SageMaker environment:\nCreated SageMaker Studio in Singapore region Created user sagemakeruser and default Execution role Initialized first Python 3 Notebook and opened Studio Cloned GitHub repo amazon-sagemaker-immersion-day into Studio Feature Engineering with Data Wrangler:\nImported bank-additional-full.csv from S3 Explored data, identified types, created summary analysis Analyzed correlation between features and target variable y Used SageMaker Feature Store:\nStored standardized features for team use in training/inference Tracked metadata and versioning Exported features offline for training preparation Exported data to S3:\nRetrieved data from Feature Store into Pandas DataFrame Split into train, test, validation sets Converted data to CSV/libSVM for XGBoost Uploaded data to S3 for training Trained, tuned, and deployed XGBoost:\nUsed built-in SageMaker XGBoost algorithm Performed training and automatic hyperparameter tuning Deployed model to endpoint and evaluated performance "},{"uri":"https://giaphazzz.github.io/aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://giaphazzz.github.io/aws/tags/","title":"Tags","tags":[],"description":"","content":""}]